{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import bz2\n",
    "import difflib\n",
    "import logging\n",
    "import time\n",
    "import zlib\n",
    "import itertools\n",
    "import collections\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sn\n",
    "from itertools import chain, combinations, tee, islice, permutations\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from statsmodels.formula.api import ols\n",
    "from pathlib import Path\n",
    "from collections import Counter \n",
    "from IPython.display import display\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "## Content\n",
    "### A. For general use:\n",
    "* [A1. Similarity measures](#A1.-Similarity-measures) \n",
    "    * [A1.1.: Set-theoretic measures (Intersection, Overlap, Jaccard, SMC, LCS, ngram_abs)](#A1.1.:-Set-theoretic-measures-(Intersection,-Overlap,-Jaccard,-SMC,-LCS,-ngram_abs)) \n",
    "    * [A1.2.: ITR measures](#A1.2.:-ITR-measures)\n",
    "        * [A1.2.1. Here we first calculate the observed, expected and maximum items of a specific unit size](#A1.2.1.-Here-we-first-calculate-the-observed,-expected-and-maximum-items-of-a-specific-unit-size)\n",
    "        * [A1.2.2. Then, we summarize them in final metrics (e.g., OmE (Pair Frequency/SOMA), OdE, OmEdM, OdM, OmEdMmE)](#A1.2.2.-Then,-we-summarize-them-in-final-metrics-(e.g.,-OmE-(Pair-Frequency/SOMA),-OdE,-OmEdM,-OdM,-OmEdMmE))\n",
    "    * [A1.3: Edit distance measures](#A1.3:-Edit-distance-measures)\n",
    "    * [A1.4: Graveyard for old similarity measure functions](#1.4:-Gaveyard-or-old-similarity-measure-functions)\n",
    "\n",
    "### B. Project specific: Data wrangling for experiments\n",
    "* [B1. Experiment & data overview](#B1.-Experiment-&-data-overview)\n",
    "* [B2. Pickle file conversion (old --> new)](#B2.-Pickle-file-conversion-(old--->-new))\n",
    "* [B3. Data frame creation (incl. spell check); to be used for subsequent analysis](#B3.-Data-frame-creation-(incl.-spell-check);-to-be-used-for-subsequent-analysis)\n",
    "* [B4. Similarity for dyads by sub-group](#B4.-Similarity-for-dyads-by-sub-group)\n",
    "* [B5. Group means (Results presented in Manuscript)](#B5.-Group-means-(Results-presented-in-Manuscript))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1. Similarity measures\n",
    "#### A1.1.: Set-theoretic measures (Intersection, Overlap, Jaccard, SMC, LCS, ngram_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(str1, str2):\n",
    "    intersection = len(np.intersect1d(str1, str2))\n",
    "    return intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(str1, str2):\n",
    "    intersection = len(np.intersect1d(str1, str2))\n",
    "\n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(str1) >= len(str2):\n",
    "        larger = len(str1) \n",
    "        smaller = len(str2)\n",
    "    else:\n",
    "        smaller = len(str1)\n",
    "        larger = len(str2) \n",
    "    overlap = intersection / smaller\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(str1, str2): \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    union = len(np.union1d(str1, str2))\n",
    "    intersection = len(np.intersect1d(str1, str2))\n",
    "\n",
    "    Jaccard = intersection / union\n",
    "    #print(Jaccard)\n",
    "    return Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMC(str1, str2): # This counts the number of muturally forgotten items as \"similar\". TBD in case of Experiments 1B and 3\n",
    "    encoding = list(range(1,85)) + [157,158,159,160] # Manual entry to get all words from the original study list             \n",
    "    \n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    intersection = len(np.intersect1d(str1, str2))\n",
    "    \n",
    "    Forgotten = 0\n",
    "    for word in encoding:\n",
    "        if word in str1 or word in str2:\n",
    "            continue\n",
    "        else:\n",
    "            Forgotten += 1\n",
    "    \n",
    "    return (((Forgotten+intersection) / len(encoding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_abs(str1, str2, unitSize, y, unidirectional=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    USAGE:\n",
    "    #of bigrams/trigrams/ngrams = ngram_abs(str1, str2, unitSize, y, unidirectional=True)\n",
    "\n",
    "    ARGUMENTS:\n",
    "    str1            The first string\n",
    "    str2            The second string\n",
    "    unitSize        Size of unit of interest (word pair, triple, quartuple etc.)\n",
    "    unidirectional  Order/Direction of unit (only 1-2 valid, or also 2-1)\n",
    "\n",
    "    DESCRIPTION:\n",
    "    Can be used to calculate the joint number of word pairs/bigrams or triplets/trigrams across two participants\n",
    "    \"\"\"\n",
    "     \n",
    "    #str1 = str1.values.tolist()\n",
    "    #str2 = str2.values.tolist()\n",
    "    str1 = None\n",
    "    str2 = None\n",
    "    \n",
    "    if unidirectional == True:\n",
    "        #return [np.array(x) for x in zip(string[0:-1], string[1:])]\n",
    "        iters = tee(str1, unitSize)                                                     \n",
    "        for i, it in enumerate(iters):                                               \n",
    "            next(islice(it, i, i), None)\n",
    "            \n",
    "        iters2 = tee(str2, unitSize)                                                     \n",
    "        for i, it in enumerate(iters2):                                               \n",
    "            next(islice(it, i, i), None)\n",
    "           \n",
    "        str1 = list(zip(*iters))\n",
    "        str2 = list(zip(*iters2))\n",
    "    else:\n",
    "        s = list(str1)\n",
    "        powerset = chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "        s2 = list(str2)\n",
    "        powerset2 = chain.from_iterable(combinations(s2, r) for r in range(len(s2)+1))\n",
    "        str1 = [(x) for x in powerset if len(x)==unitSize]\n",
    "        str2 = [(x) for x in powerset2 if len(x)==unitSize]\n",
    "    #print(str1)\n",
    "    #print(str2)\n",
    "\n",
    "    # This is for assining unique numbers to the different n-grams. At the moment it is executed elsewhere,\\ \n",
    "    # but one could also execute it in here\n",
    "    #y = permu(unitSize)\n",
    "    #all_p = pd.DataFrame()\n",
    "    #all_p['AllPermutations'] = y\n",
    "    \n",
    "    a_list = []\n",
    "    for i in (str1):\n",
    "        if i in y:\n",
    "            x = y.index(i)\n",
    "        else:\n",
    "            continue\n",
    "        a_list.append(x)\n",
    "\n",
    "    b_list = []\n",
    "    for i in (str2):\n",
    "        if i in y:\n",
    "            x = y.index(i)\n",
    "        else:\n",
    "            continue\n",
    "        b_list.append(x)\n",
    "    \n",
    "    count = 0\n",
    "    for i in str1:\n",
    "        if i in str1 and i in str2:\n",
    "            count += 1\n",
    "        else:\n",
    "            count += 0\n",
    "    #print(count)\n",
    "    #a = np.array(a_list)\n",
    "    #b = np.array(b_list)\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from source: https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Longest_common_substring#Python\n",
    "def lcs(str1, str2):\n",
    "    m = [[0] * (1 + len(str2)) for i in range(1 + len(str1))]\n",
    "    longest, x_longest = 0, 0\n",
    "    for x in range(1, 1 + len(str1)):\n",
    "        for y in range(1, 1 + len(str2)):\n",
    "            if str1[x - 1] == str2[y - 1]:\n",
    "                m[x][y] = m[x - 1][y - 1] + 1\n",
    "                if m[x][y] > longest:\n",
    "                    longest = m[x][y]\n",
    "                    x_longest = x\n",
    "            else:\n",
    "                m[x][y] = 0\n",
    "    string = (str1[x_longest - longest: x_longest])\n",
    "    lcs = len(string)\n",
    "    #print(string)\n",
    "    return lcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1.2.: ITR measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1.2.1. Here we first calculate the observed, expected and maximum items of a specific unit size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observed(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    USAGE:\n",
    "    score = observed(str1, str2, unitSize, unidirectional=True)\n",
    "\n",
    "    ARGUMENTS:\n",
    "    str1            The first string\n",
    "    str2            The second string\n",
    "    unitSize        Size of unit of interest (word pair, triple, quartuple etc.)\n",
    "    unidirectional  Direction of unit (only word1-word2 valid, or also word2-word1)\n",
    "\n",
    "    DESCRIPTION:\n",
    "    This calculates the observed words of a specific unit size to be used in subsequent shared organization measures\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(str1) == 0:\n",
    "        return []\n",
    "    elif len(str2) ==0:\n",
    "        return []\n",
    "            \n",
    "    ob_freq = 0\n",
    "    for i in range(len(str1)-1):\n",
    "        p1 = str1[i]\n",
    "        p2 = str1[i+1]\n",
    "        if unitSize ==3:\n",
    "            p3 = str1[i+2]\n",
    "        elif unitSize ==4:\n",
    "            p3 = str1[i+2]\n",
    "            p4 = str1[i+3]\n",
    "\n",
    "        if unitSize == 2:\n",
    "            if p1 in str2 and p2 in str2:\n",
    "                i1 = np.nonzero(np.array(str2) == p1)\n",
    "                i2 = np.nonzero(np.array(str2) == p2)\n",
    "                # Directionality. Difference absolute or not absolute\n",
    "                if unidirectional == True:\n",
    "                    if (i2[0] - i1[0]) == 1:\n",
    "                        ob_freq += 1\n",
    "                else:\n",
    "                    if abs(i2[0] - i1[0]) == 1:\n",
    "                        ob_freq += 1                \n",
    "            \n",
    "        elif unitSize == 3:\n",
    "            if p1 in str2 and p2 in str2 and p3 in str2:\n",
    "                i1 = np.nonzero(np.array(str2) == p1)\n",
    "                i2 = np.nonzero(np.array(str2) == p2)\n",
    "                i3 = np.nonzero(np.array(str2) == p3)\n",
    "                # Directionality. Difference absolute or not absolute\n",
    "                if unidirectional == True:\n",
    "                    if (i2[0] - i1[0]) == 1 and (i3[0] - i1[0]) == 2 and (i3[0] - i2[0]) == 1:\n",
    "                        ob_freq += 1\n",
    "                else:\n",
    "                    if abs(i2[0] - i1[0]) <= 2 and abs(i3[0] - i2[0]) <= 2 and abs(i3[0] - i1[0]) <= 2: # I think I might not need the last and\n",
    "                        ob_freq += 1  \n",
    "            \n",
    "        elif unitSize == 4:\n",
    "            if p1 in str2 and p2 in str2 and p3 in str2:\n",
    "                i1 = np.nonzero(np.array(str2) == p1)\n",
    "                i2 = np.nonzero(np.array(str2) == p2)\n",
    "                i3 = np.nonzero(np.array(str2) == p3)\n",
    "                i4 = np.nonzero(np.array(str2) == p4)\n",
    "                # Directionality. Difference absolute or not absolute\n",
    "                if unidirectional == True:\n",
    "                    if (i4[0] - i1[0]) == 3 and (i3[0] - i1[0]) == 2 and (i2[0] - i1[0]) == 1 and \\\n",
    "                    (i4[0] - i2[0]) == 2 and (i4[0] - i3[0] == 1) and (i3[0]-i2[0] == 1):\n",
    "                        ob_freq += 1\n",
    "                else:\n",
    "                    if \\\n",
    "                    abs(i2[0] - i1[0]) <= 3 and abs(i3[0] - i1[0]) <= 3 and abs(i4[0] - i1[0]) <= 3 and \\\n",
    "                    abs(i3[0] - i2[0]) <= 3 and abs(i4[0] - i2[0]) <= 3 and \\\n",
    "                    abs(i4[0] - i3[0]) <= 3: \n",
    "                        ob_freq += 1     \n",
    "           \n",
    "    return ob_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_BB(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    USAGE:\n",
    "    score = expected_BB(str1, str2, unitSize, unidirectional=True)\n",
    "\n",
    "    ARGUMENTS:\n",
    "    str1            The first string\n",
    "    str2            The second string\n",
    "    unitSize        Size of unit of interest (word pair, triple, quartuple etc.) --> Is always 2 in this case\n",
    "    unidirectional  Direction of unit (only 1-2 valid, or also 2-1)\n",
    "\n",
    "    DESCRIPTION:\n",
    "    This calculates the expected organization of word pairs to be used in subsequent shared organization measures\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # This is only for pairs   \n",
    "    if unitSize != 2:\n",
    "        return \"Use different expected calculation for higher order unit sizes\"\n",
    "    \n",
    "    # Calc expected\n",
    "    num_common_items = len(np.intersect1d(str1, str2))\n",
    "    if unidirectional == True:\n",
    "        x = 1\n",
    "    else:\n",
    "        x = 2\n",
    "        \n",
    "    exp_freq = (x*num_common_items*(num_common_items-1)) / float(len(str1)*len(str2))\n",
    "\n",
    "    if num_common_items == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return exp_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_generalized(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    USAGE:\n",
    "    score = expected_generalized(str1, str2, unitSize, unidirectional=True)\n",
    "\n",
    "    ARGUMENTS:\n",
    "    str1            The first string\n",
    "    str2            The second string\n",
    "    unitSize        Size of unit of interest (word pair, triple, quartuple etc.)\n",
    "    unidirectional  Direction of unit (only 1-2 valid, or also 2-1)\n",
    "\n",
    "    DESCRIPTION:\n",
    "    This calculates the expected organization of word pairs: ((N-X-1)! * A * (M-X + 1-R)) / N!\n",
    "    Not used.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # Calculate expected value according to Boulsfield & Boulsfield\n",
    "    num_common_items = len(np.intersect1d(str1, str2))\n",
    "    if unidirectional == True:\n",
    "        x = 1\n",
    "    else:\n",
    "        x = 2\n",
    "    exp_freq = (x*num_common_items*(num_common_items-1)) / float(len(str1)*len(str2))\n",
    "    #print('x', x)\n",
    "    \n",
    "    return exp_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    USAGE:\n",
    "    score = maximum(str1, str2, unitSize, unidirectional=True)\n",
    "\n",
    "    ARGUMENTS:\n",
    "    str1            The first string\n",
    "    str2            The second string\n",
    "    unitSize        Size of unit of interest (word pair, triple, quartuple etc.)\n",
    "    unidirectional  Direction of unit (only 1-2 valid, or also 2-1)\n",
    "\n",
    "    DESCRIPTION:\n",
    "    This calculates the maximum possible organization of word pairs to be used in subsequent shared organization measures: (c - x + 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_common_items = len(np.intersect1d(str1, str2))\n",
    "\n",
    "    if num_common_items == 0:\n",
    "        return 0\n",
    "    elif num_common_items == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return (num_common_items - unitSize + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A1.2.2. Then, we summarize them in final metrics (e.g., OmE (Pair Frequency/SOMA), OdE, OmEdM, OdM, OmEdMmE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OmE(str1, str2, unitSize, unidirectional=True):   \n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "    Unidirectional & pair = ITR (Bousfield & Bousfield (1966))\n",
    "    Bidirectional & pari = Pair(ed) Frequency (Anderson & Watts (1969); Rosner (1970))\n",
    "    \"\"\"\n",
    "    \n",
    "    if unidirectional==True:\n",
    "        return observed(str1, str2, unitSize, unidirectional=True) - expected_BB(str1, str2, unitSize, unidirectional=True)\n",
    "    else:\n",
    "        return observed(str1, str2, unitSize, unidirectional=False) - expected_BB(str1, str2, unitSize, unidirectional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OdE(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "    Referred to as Sequential Consistency by Sternberg & Tulving (1977), developed by Gorfein, Blair, & Rowland (1968)\n",
    "    \"\"\"\n",
    "        \n",
    "    if expected_BB(str1, str2, unitSize, unidirectional=True)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        if unidirectional==True:\n",
    "            return observed(str1, str2, unitSize, unidirectional=True) / expected_BB(str1, str2, unitSize, unidirectional=True)\n",
    "        else:\n",
    "            return observed(str1, str2, unitSize, unidirectional=False) / expected_BB(str1, str2, unitSize, unidirectional=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OmEdM(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "    Referred to as Sequential Consistency by Sternberg & Tulving (1977)\n",
    "    Unidirectional pairs, Fagan (1968)\n",
    "    Bidirectional pairs, Postman (1970)\n",
    "    \"\"\"\n",
    "    \n",
    "    if maximum(str1, str2, unitSize, unidirectional=True)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        if unidirectional==True:\n",
    "            return ((observed(str1, str2, unitSize, unidirectional=True) - expected_BB(str1, str2, unitSize, unidirectional=True)) / maximum(str1, str2, unitSize, unidirectional=True))\n",
    "        else:\n",
    "            return ((observed(str1, str2, unitSize, unidirectional=False) - expected_BB(str1, str2, unitSize, unidirectional=False)) / maximum(str1, str2, unitSize, unidirectional=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OdM(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "    For bidirectional pairs referred to as ITR(2) (Mandler & Dean (1969))\n",
    "    \"\"\"\n",
    "    \n",
    "    if maximum(str1, str2, unitSize, unidirectional=True)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        if unidirectional==True:\n",
    "            return (observed(str1, str2, unitSize, unidirectional=True) / maximum(str1, str2, unitSize, unidirectional=True))\n",
    "        else:\n",
    "            return (observed(str1, str2, unitSize, unidirectional=False) / maximum(str1, str2, unitSize, unidirectional=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OmEdMmE(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "    Generally referred to as ARC' (Pellegrino (1971); Pellegrino & Battig (1974))\n",
    "    \"\"\"\n",
    "    \n",
    "    if maximum(str1, str2, unitSize, unidirectional=True)==0 and expected_BB(str1, str2, unitSize, unidirectional=True)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        if unidirectional==True:\n",
    "            return ((observed(str1, str2, unitSize, unidirectional=True) - expected_BB(str1, str2, unitSize, unidirectional=True)) / (maximum(str1, str2, unitSize, unidirectional=True) - expected_BB(str1, str2, unitSize, unidirectional=True)))\n",
    "        else:\n",
    "            return ((observed(str1, str2, unitSize, unidirectional=False) - expected_BB(str1, str2, unitSize, unidirectional=False)) / (maximum(str1, str2, unitSize, unidirectional=False) - expected_BB(str1, str2, unitSize, unidirectional=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A1.3: Edit distance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_dists(str1, str2, insert=True, delete=True, substitute=True, transpose=True):\n",
    "    \"\"\"\n",
    "    USAGE:\n",
    "    score = edit_dists(str1, str2, insert=True, delete=True, substitute=True, transpose=True)\n",
    "\n",
    "    ARGUMENTS:\n",
    "    str1            The first string\n",
    "    str2            The second string\n",
    "    insert          Is insertion as an operation allowed?\n",
    "    delete          Is deletion as an operation allowed?\n",
    "    substitute      Is substition as an operation allowed?\n",
    "    transpose       Is transpotion as an operation allowed?\n",
    "\n",
    "    DESCRIPTION:\n",
    "    This calculates different types of edit distances dependent on which operations are allowed\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    if len(str1) > len(str2):\n",
    "        str1, str2 = str2, str1\n",
    "\n",
    "    n1 = len(str1)\n",
    "    n2 = len(str2)\n",
    "    d = np.zeros((n1 + 1, n2 + 1), dtype=int)\n",
    "\n",
    "    for i in range(n1 + 1):\n",
    "        d[i, 0] = i\n",
    "\n",
    "    for j in range(n2 + 1):\n",
    "        d[0, j] = j\n",
    "\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            options = []\n",
    "            # insertion\n",
    "            if insert:\n",
    "                options += [d[i, j+1] + 1]\n",
    "\n",
    "            # deletion\n",
    "            if delete:\n",
    "                options += [d[i+1, j] + 1]\n",
    "\n",
    "            # substitution\n",
    "            if substitute and not(str1[i] == str2[j]):\n",
    "                options += [d[i, j] + 1]\n",
    "\n",
    "            # identical entries are free\n",
    "            elif str1[i] == str2[j]:\n",
    "                options += [d[i, j] + 0]\n",
    "\n",
    "\n",
    "            d[i+1, j+1] = min(options)\n",
    "            #d[i+1, j+1] = min(d[i, j+1] + 1, # insert\n",
    "            #                  d[i+1, j] + 1, # delete\n",
    "            #                  d[i, j] + cost) # replace\n",
    "\n",
    "\n",
    "            if transpose:\n",
    "                if i > 0 and j > 0 and str1[i] == str2[j-1] and str1[i-1] == str2[j]:\n",
    "                    d[i+1, j+1] = min(d[i+1, j+1], d[i-1, j-1] + int(not(str1[i] == str2[j]))) # transpose\n",
    "\n",
    "\n",
    "    # if substitution, max # of edits is max(n1, n2)\n",
    "    if substitute:\n",
    "        return 1 - ( d[n1, n2] / max(n1, n2) )\n",
    "\n",
    "    # otherwise, it's n1 + n2 (delete each of s1, then insert each of s2)\n",
    "    else:\n",
    "        return 1 - ( d[n1, n2] / (n1 + n2) )\n",
    "    #return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A1.4: Graveyard for old similarity measure functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairedFreq(str2, str1):\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(str2) == 0:\n",
    "        return []\n",
    "    ob_freq = 0\n",
    "    for i in range(len(str2)-1):\n",
    "        p1 = str2[i]\n",
    "        p2 = str2[i+1]\n",
    "        if p1 in str1 and p2 in str1:\n",
    "            i1 = np.nonzero(np.array(str1) == p1)\n",
    "            i2 = np.nonzero(np.array(str1) == p2)\n",
    "            if abs(i1[0] - i2[0]) == 1:\n",
    "                ob_freq += 1\n",
    "                #print(ob_freq)\n",
    "    #print(str2)\n",
    "    #print(str1)\n",
    "    #print(\"ob_freq=\", ob_freq)\n",
    "    \n",
    "    if str2 == []:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    # This is the code from Christian\n",
    "    # num_common_items = len(final)\n",
    "    # \n",
    "    # num_common_items = np.intersect1d(final, orig)\n",
    "    # print(\"num_common_items=\", num_common_items)\n",
    "    # \n",
    "    # if type(num_common_items) == np.dtype(int):\n",
    "    #    num_common_items = 1\n",
    "    # else:\n",
    "    #    print(num_common_items)\n",
    "    #    num_common_items = len(num_common_items)\n",
    "\n",
    "    num_common_items = len(np.intersect1d(str1, str2)) #Alex New\n",
    "\n",
    "    exp_freq = (2*num_common_items*(num_common_items-1)) / float(len(str2)*len(str1))\n",
    "    #print(\"exp_freq=\", exp_freq)\n",
    "    PF = ob_freq - exp_freq\n",
    "    #print(\"PF=\", PF)\n",
    "    return PF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized observed bidirectional Inter-Trial-Repetition (O(ITR2) - Max(ITR2)) (by Alex, build on adapted PF from Christian)\n",
    "# \"The maximum ITR value is a function of the number of items common to both sets of events and does not depend on the absolute \n",
    "# number of words recalled or presented. It is equal to the number of items common to both events minus one.\" (Mandler & Dean, 1969)\n",
    "def ITR2(str1, str2, shortest = True):\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(str2) == 0:\n",
    "        return []\n",
    "    ob_freq = 0\n",
    "    for i in range(len(str2)-1):\n",
    "        p1 = str2[i]\n",
    "        p2 = str2[i+1]\n",
    "        if p1 in str1 and p2 in str1:\n",
    "            i1 = np.nonzero(np.array(str1) == p1)\n",
    "            i2 = np.nonzero(np.array(str1) == p2)\n",
    "            if abs(i1[0] - i2[0]) == 1:\n",
    "                ob_freq += 1\n",
    "    #print(\"ob_freq=\", ob_freq)\n",
    "    \n",
    "    if str2 == []:\n",
    "        return 0\n",
    "\n",
    "    num_common_items = len(np.intersect1d(str1, str2))\n",
    "    if num_common_items == 0: #Check with Christian\n",
    "        ITR2 = 0\n",
    "    elif num_common_items == 1:\n",
    "        ITR2 = 0\n",
    "    else:\n",
    "        ITR2 = (ob_freq / (num_common_items - 1)) #M(ITR) = M(ITR2) = c-1\n",
    "    \n",
    "    return ITR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized observed bidirectional Inter-Trial-Repetition (O(ITR2) - Max(ITR2)) (by Alex, build on adapted PF from Christian)\n",
    "# \"The maximum ITR value is a function of the number of items common to both sets of events and does not depend on the absolute \n",
    "# number of words recalled or presented. It is equal to the number of items common to both events minus one.\" (Mandler & Dean, 1969)\n",
    "# According to Pellegrino (1971) M(ITRx) can either be calculated M(ITR) can be calculated either using   \n",
    "# MAX(ITRa/x) = M - X + 1 -R    or   MAX(ITRa/ x) =M- X + 1\n",
    "# where M=number of items recalled on Trial t, x=ngram-size and R=number of units of Size X from Trial t that have one or more items not recalled on t + 1\n",
    "def ITRx(str1, str2, x):\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(str2) == 0:\n",
    "        return []\n",
    "    ob_freq = 0\n",
    "    for i in range(len(str2)-1):\n",
    "        p1 = str2[i]\n",
    "        p2 = str2[i+1]\n",
    "        if p1 in str1 and p2 in str1:\n",
    "            i1 = np.nonzero(np.array(str1) == p1)\n",
    "            i2 = np.nonzero(np.array(str1) == p2)\n",
    "            if abs(i1[0] - i2[0]) == 1:\n",
    "                ob_freq += 1\n",
    "    #print(\"ob_freq=\", ob_freq)\n",
    "    \n",
    "    if str2 == []:\n",
    "        return 0\n",
    "\n",
    "    num_common_items = len(np.intersect1d(str1, str2))\n",
    "    if num_common_items == 0:\n",
    "        ITRx = 0\n",
    "    elif num_common_items == 1:\n",
    "        ITRx = 0\n",
    "    else:\n",
    "        ITRx = (ob_freq / (num_common_items - x + 1))\n",
    "    \n",
    "    return ITRx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different normalization for observed bidirectional Inter-Trial-Repetition \n",
    "# (O(ITR2) - E(ITR2)) / (M(ITR2) - E(ITR2)) (by Alex, build on adapted PF from Christian)\n",
    "def ARC2(str1, str2):\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(str2) == 0:\n",
    "        return []\n",
    "    ob_freq = 0\n",
    "    for i in range(len(str2)-1):\n",
    "        p1 = str2[i]\n",
    "        p2 = str2[i+1]\n",
    "        if p1 in str1 and p2 in str1:\n",
    "            i1 = np.nonzero(np.array(str1) == p1)\n",
    "            i2 = np.nonzero(np.array(str1) == p2)\n",
    "            if abs(i1[0] - i2[0]) == 1:\n",
    "                ob_freq += 1\n",
    "    #print(\"ob_freq=\", ob_freq)\n",
    "    \n",
    "    if str2 == []:\n",
    "        return 0\n",
    "\n",
    "    num_common_items = len(np.intersect1d(str1, str2))\n",
    "    #print(\"num_common_items=\", num_common_items) \n",
    "\n",
    "    exp_freq = (2*num_common_items*(num_common_items-1)) / float(len(str2)*len(str1))\n",
    "    #print(\"exp_freq=\", exp_freq) \n",
    "\n",
    "    max_freq = (num_common_items - 1)\n",
    "    #print(\"max_freq=\", max_freq) \n",
    "    \n",
    "    if (max_freq - exp_freq)== 0 : # Double check with Christian (happens when c=1)\n",
    "        ARC2 = 0\n",
    "        #print('watch out')\n",
    "    else:\n",
    "        ARC2 = (ob_freq - exp_freq) / (max_freq - exp_freq)\n",
    "    #print(ob_freq, exp_freq, max_freq)\n",
    "    return ARC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editdist(str1, str2, min_threshold = None):\n",
    "  #\"\"\"Return approximate string comparator measure (between 0.0 and 1.0)\n",
    "   #  using the edit (or Levenshtein) distance.\n",
    "\n",
    "#  USAGE:\n",
    "#    score = editdist(str1, str2, min_threshold)\n",
    "\n",
    "#  ARGUMENTS:\n",
    "#    str1           The first string\n",
    "#    str2           The second string\n",
    "#    min_threshold  Minimum threshold between 0 and 1\n",
    "#\n",
    "#  DESCRIPTION:\n",
    "#    The edit distance is the minimal number of insertions, deletions and\n",
    "#    substitutions needed to make two strings equal.#\n",
    "#\n",
    "#    For more information on the modified Soundex see:\n",
    "#    - http://www.nist.gov/dads/HTML/editdistance.html\n",
    "#  \"\"\"\n",
    "\n",
    "#  # Quick check if the strings are empty or the same - - - - - - - - - - - - -\n",
    "#  #\n",
    "#  #if (str1 == '') or (str2 == ''):\n",
    "    if (str1.size == 0) or (str2.size == 0):\n",
    "        return 0.0\n",
    "    #if len(str1) == len(str2) and len(str1) > 1 and all(str1 == str2):\n",
    "    #    return 1.0\n",
    "    #elif len(str1) == len(str2) and len(str1) == 1 and (str1 == str2):\n",
    "    #    return 1.0\n",
    "\n",
    "    n = len(str1)\n",
    "    m = len(str2)\n",
    "    max_len = max(n,m)\n",
    "\n",
    "    if (min_threshold != None):\n",
    "        if (isinstance(min_threshold, float)) and (min_threshold > 0.0) and (min_threshold > 0.0):\n",
    "\n",
    "            len_diff = abs(n-m)\n",
    "            w = 1.0 - float(len_diff) / float(max_len)\n",
    "\n",
    "            if (w  < min_threshold):\n",
    "                return 0.0  # Similariy is smaller than minimum threshold\n",
    "\n",
    "        else: # Calculate the maximum distance possible with this threshold\n",
    "            max_dist = (1.0-min_threshold)*max_len\n",
    "\n",
    "    else:\n",
    "        logging.exception('Illegal value for minimum threshold (not between' + \\\n",
    "                        ' 0 and 1): %f' % (min_threshold))\n",
    "        raise Exception\n",
    "\n",
    "    if (n > m):  # Make sure n <= m, to use O(min(n,m)) space\n",
    "        str1, str2 = str2, str1\n",
    "        n, m =       m, n\n",
    "\n",
    "    current = range(n+1)\n",
    "\n",
    "    for i in range(1, m+1):\n",
    "\n",
    "        previous = current\n",
    "        current =  [i]+n*[0]\n",
    "        str2char = str2[i-1]\n",
    "\n",
    "        for j in range(1,n+1):\n",
    "            substitute = previous[j-1]\n",
    "            if (str1[j-1] != str2char):\n",
    "                substitute += 1\n",
    "\n",
    "      # Get minimum of insert, delete and substitute\n",
    "      #\n",
    "            current[j] = min(previous[j]+1, current[j-1]+1, substitute)\n",
    "\n",
    "        if (min_threshold != None) and (min(current) > max_dist):\n",
    "            return 1.0 - float(max_dist+1) / float(max_len)\n",
    "\n",
    "    w = 1.0 - float(current[n]) / float(max_len)\n",
    "\n",
    "    assert (w >= 0.0) and (w <= 1.0), 'Similarity weight outside 0-1: %f' % (w)\n",
    "\n",
    "  # A log message - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "  #\n",
    "    logging.debug('Edit-distance comparator string \"%s\" with \"%s\" value: %.3f' \\\n",
    "                % (str1, str2, w))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_editdist(str1, str2, min_threshold = None):\n",
    "#Return approximate string comparator measure (between 0.0 and 1.0)\n",
    "#     using a modified edit (or Levenshtein) distance that counts transpositions\n",
    "#     as elementary operations as well. This is also called the Damerau-\n",
    "#     Levenshtein distance.\n",
    "\n",
    "#  USAGE:\n",
    "#    score = mod_editdist(str1, str2, min_threshold)\n",
    "\n",
    "#  ARGUMENTS:\n",
    "#    str1           The first string\n",
    "#    str2           The second string\n",
    "#    min_threshold  Minimum threshold between 0 and 1\n",
    "\n",
    "#  DESCRIPTION:\n",
    "#    The edit distance is the minimal number of insertions, deletions,\n",
    "#    substitutions and transpositions needed to make two strings equal.\n",
    "\n",
    "#    Compared to the original editdist function, which handles a transposition\n",
    "#    (like: 'sydney' <-> 'sydeny' as 2 operations (two substitutions or one\n",
    "#    insert and one delet), this modified version handles this as 1 operation.\n",
    "\n",
    "#    Based on code from Justin Zobel's 'vrank'.\n",
    "    \n",
    "# Quick check if the strings are empty or the same - - - - - - - - - - - - -\n",
    "#\n",
    "#if (str1 == '') or (str2 == ''):\n",
    "#print([str1, str2])\n",
    "    #assert(0)\n",
    "    if (str1.size == 0) or (str2.size == 0):\n",
    "        return 0.0\n",
    "    #elif (str1 == str2):\n",
    "    elif np.array_equal(str1, str2):\n",
    "        return 1.0\n",
    "\n",
    "    n = len(str1)\n",
    "    m = len(str2)\n",
    "    max_len = max(n,m)\n",
    "    #print('n', n)\n",
    "    #print('m', m)\n",
    "    #print('max_len',max_len)\n",
    "\n",
    "    if (min_threshold != None):\n",
    "        if (isinstance(min_threshold, float)) and (min_threshold > 0.0) and (min_threshold > 0.0): #I don't get this one\n",
    "        \n",
    "            len_diff = abs(n-m)\n",
    "            w = 1.0 - float(len_diff) / float(max_len)\n",
    "\n",
    "            if (w  < min_threshold):\n",
    "                return 0.0  # Similariy is smaller than minimum threshold\n",
    "\n",
    "        else: # Calculate the maximum distance possible with this threshold\n",
    "            max_dist = (1.0-min_threshold)*max_len\n",
    "\n",
    "    else:\n",
    "        logging.exception('Illegal value for minimum threshold (not between' + ' 0 and 1): %f' % (min_threshold))\n",
    "        raise Exception\n",
    "\n",
    "    if (n > m):  # Make sure n <= m, to use O(min(n,m)) space\n",
    "        str1, str2 = str2, str1\n",
    "        n, m =       m, n\n",
    "\n",
    "    d = []  # Table with the full distance matrix\n",
    "\n",
    "    current = range(n+1)\n",
    "    d.append(current)\n",
    "\n",
    "    for i in range(1,m+1):\n",
    "\n",
    "        previous = current\n",
    "        current =  [i]+n*[0]\n",
    "        str2char = str2[i-1]\n",
    "\n",
    "        for j in range(1,n+1):\n",
    "            substitute = previous[j-1]\n",
    "            if (str1[j-1] != str2char):\n",
    "                substitute += 1\n",
    "\n",
    "            if (i == 1) or (j == 1):  # First characters, no transposition possible\n",
    "\n",
    "            # Get minimum of insert, delete and substitute\n",
    "            #\n",
    "                current[j] = min(previous[j]+1, current[j-1]+1, substitute)\n",
    "\n",
    "            else:\n",
    "                if (str1[j-2] == str2[i-1]) and (str1[j-1] == str2[i-2]):\n",
    "                    transpose = d[i-2][j-2] + 1\n",
    "                else:\n",
    "                    transpose = d[i-2][j-2] + 3\n",
    "\n",
    "                current[j] = min(previous[j]+1, current[j-1]+1, substitute, transpose)\n",
    "\n",
    "        d.append(current)\n",
    "\n",
    "        if (min_threshold != None) and (min(current) > max_dist):\n",
    "            return 1.0 - float(max_dist+1) / float(max_len)\n",
    "\n",
    "    w = 1.0 - float(current[n]) / float(max_len)\n",
    "    \n",
    "    assert (w >= 0.0) and (w <= 1.0), 'Similarity weight outside 0-1: %f' % (w)\n",
    "\n",
    "  # A log message - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "  #\n",
    "    logging.debug('Modified edit-distance comparator string \"%s\" with \"%s\" ' % \\\n",
    "                (str1, str2) + 'value: %.3f' % (w))\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "### B. Project specific: Data wrangling for experiments\n",
    "#### B1. Experiment & Data overview\n",
    "* 4 phases: encoding --> individual recall --> individual or collaborative recall --> individual recall\n",
    "* 2 \"real\" variables: individual/collaborative recall; biased/non-biased (deep/shallow encoding)\n",
    "* 2 \"counter balancing\" variables were considered: word list 1 or 2; order of lists encoded for biased participants\n",
    "* always a biased and unbiased participant collaborated\n",
    "* order is only relevant for biased participants. two orders are possible dependent on subject number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __B2. Pickle file conversion (old --> new)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert files to new pickle files as the files were created with an older pickle version\n",
    "\n",
    "# TO DO\n",
    "# Check whether I can also just use a * \n",
    "\n",
    "\n",
    "# Which participants to include\n",
    "first = 1\n",
    "last = 9192 # Change to max SN number\n",
    "skip = []\n",
    "# Files not usable from SNs: 13, 14, 20, 21, 25, 27, 44, 45, 63, 65 (do not have to be skipped as they are in different folder)\n",
    "\n",
    "# Encoding files\n",
    "for subjectNumber in range(first, last+1):\n",
    "\n",
    "    if subjectNumber in skip:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        original = ('encoding_' + str(subjectNumber) +'_1.pkl')\n",
    "        destination = ('new_encoding_' + str(subjectNumber) +'_1.pkl')\n",
    "\n",
    "        content = ''\n",
    "        outsize = 0\n",
    "        with open(original, 'rb') as infile:\n",
    "            content = infile.read()\n",
    "        with open(destination, 'wb') as output:\n",
    "            for line in content.splitlines():\n",
    "                outsize += len(line) + 1\n",
    "                output.write(line + str.encode('\\n'))\n",
    "\n",
    "        with open(destination, 'rb') as f:\n",
    "            try:\n",
    "                test = pickle.load(f, encoding='latin1')\n",
    "                print(test)\n",
    "            except EOFError:\n",
    "                pass\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "print(\"Done. Saved %s bytes.\" % (len(content)-outsize))\n",
    "\n",
    "for subjectNumber in range(first, last+1):\n",
    "\n",
    "    if subjectNumber in skip:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        original = ('encoding_' + str(subjectNumber) +'_0.pkl')\n",
    "        destination = ('new_encoding_' + str(subjectNumber) +'_0.pkl')\n",
    "\n",
    "        content = ''\n",
    "        outsize = 0\n",
    "        with open(original, 'rb') as infile:\n",
    "            content = infile.read()\n",
    "        with open(destination, 'wb') as output:\n",
    "            for line in content.splitlines():\n",
    "                outsize += len(line) + 1\n",
    "                output.write(line + str.encode('\\n'))\n",
    "\n",
    "        with open(destination, 'rb') as f:\n",
    "            try:\n",
    "                test = pickle.load(f, encoding='latin1')\n",
    "                print(test)\n",
    "            except EOFError:\n",
    "                pass\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "print(\"Done. Saved %s bytes.\" % (len(content)-outsize))\n",
    "\n",
    "\n",
    "# Retrieval 1  files\n",
    "for subjectNumber in range(first, last+1):\n",
    "\n",
    "    if subjectNumber in skip:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        original = ('retrieve_' + str(subjectNumber) +'_0_1.pkl')\n",
    "        destination = ('new_retrieve_' + str(subjectNumber) +'_0_1.pkl')\n",
    "\n",
    "        content = ''\n",
    "        outsize = 0\n",
    "        with open(original, 'rb') as infile:\n",
    "            content = infile.read()\n",
    "        with open(destination, 'wb') as output:\n",
    "            for line in content.splitlines():\n",
    "                outsize += len(line) + 1\n",
    "                output.write(line + str.encode('\\n'))\n",
    "\n",
    "        with open(destination, 'rb') as f:\n",
    "            try:\n",
    "                test = pickle.load(f, encoding='latin1')\n",
    "                print(test)\n",
    "            except EOFError:\n",
    "                pass\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "# Retrieval 2  files\n",
    "for subjectNumber in range(first, last+1):\n",
    "\n",
    "    if subjectNumber in skip:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        original = ('retrieve_' + str(subjectNumber) +'_0_2.pkl')\n",
    "        destination = ('new_retrieve_' + str(subjectNumber) +'_0_2.pkl')\n",
    "\n",
    "        content = ''\n",
    "        outsize = 0\n",
    "        with open(original, 'rb') as infile:\n",
    "            content = infile.read()\n",
    "        with open(destination, 'wb') as output:\n",
    "            for line in content.splitlines():\n",
    "                outsize += len(line) + 1\n",
    "                output.write(line + str.encode('\\n'))\n",
    "\n",
    "        with open(destination, 'rb') as f:\n",
    "            try:\n",
    "                test = pickle.load(f, encoding='latin1')\n",
    "                print(test)\n",
    "            except EOFError:\n",
    "                pass\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "print(\"Done. Saved %s bytes.\" % (len(content)-outsize))\n",
    "\n",
    "\n",
    "\n",
    "# Retrieval 3  files\n",
    "for subjectNumber in range(first, last+1):\n",
    "\n",
    "    if subjectNumber in skip:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        original = ('retrieve_' + str(subjectNumber) +'_0_3.pkl')\n",
    "        destination = ('new_retrieve_' + str(subjectNumber) +'_0_3.pkl')\n",
    "\n",
    "        content = ''\n",
    "        outsize = 0\n",
    "        with open(original, 'rb') as infile:\n",
    "            content = infile.read()\n",
    "        with open(destination, 'wb') as output:\n",
    "            for line in content.splitlines():\n",
    "                outsize += len(line) + 1\n",
    "                output.write(line + str.encode('\\n'))\n",
    "\n",
    "        with open(destination, 'rb') as f:\n",
    "            try:\n",
    "                test = pickle.load(f, encoding='latin1')\n",
    "                print(test)\n",
    "            except EOFError:\n",
    "                pass\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "print(\"Done. Saved %s bytes.\" % (len(content)-outsize))\n",
    "\n",
    "# Collaborative Retrieval 2  files\n",
    "for subjectNumber in range(first, last+1):\n",
    "\n",
    "    if subjectNumber in skip:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        original = ('retrieve_' + str(subjectNumber) +'_1_2.pkl')\n",
    "        destination = ('new_retrieve_' + str(subjectNumber) +'_1_2.pkl')\n",
    "\n",
    "        content = ''\n",
    "        outsize = 0\n",
    "        with open(original, 'rb') as infile:\n",
    "            content = infile.read()\n",
    "        with open(destination, 'wb') as output:\n",
    "            for line in content.splitlines():\n",
    "                outsize += len(line) + 1\n",
    "                output.write(line + str.encode('\\n'))\n",
    "\n",
    "        with open(destination, 'rb') as f:\n",
    "            try:\n",
    "                test = pickle.load(f, encoding='latin1')\n",
    "                print(test)\n",
    "            except EOFError:\n",
    "                pass\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "print(\"Done. Saved %s bytes.\" % (len(content)-outsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __B3. Data frame creation (incl. spell check); to be used for subsequent analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------DESCRIPTION----------------------------------------\n",
    "# Here we set up the dataframe and save it as a csv that will be analyzed\n",
    "\n",
    "#-------------------------------------Notes----------------------------------------------\n",
    "# This is for recalls 1 & 2 only as recall 3 was saved as dlm files up until SN58\n",
    "# Therefore, next step will import all of recall3 as dlm files\n",
    "\n",
    "#-------------------------------------Input----------------------------------------------\n",
    "studyList1 = ['crow','eagle','finch','parrot','pigeon','cardinal','nitrogen','helium','chlorine','calcium','oxygen','mercury','trout',\\\n",
    "              'flounder','halibut','guppy','piranha','shark','carnation','orchid','pansy','daffodil','violet','rose','nectarine','pear',\\\n",
    "              'apple','grape','raspberry','cherry','tuba','drum','trumpet','saxophone','piano','organ','tree','ocean','canyon','mountain',\\\n",
    "              'plateau','cave','cinnamon','mustard','basil','oregano','paprika','cotton','wool', 'velvet','linen','leather','flyer',\\\n",
    "              'newspaper','comic','essay','pamphlet','tornado','hail','blizzard','rain','drought','jacket','dress','blouse','underwear',\\\n",
    "              'shoes','lamp','desk','bookcase','dresser','chair','banker','dentist','secretary','engineer','nurse','hour', 'arms', 'green',\\\n",
    "              'uncle']\n",
    "    \n",
    "studyList2 = ['crow','eagle','finch','parrot','pigeon','nitrogen','helium','chlorine','calcium','oxygen','trout','flounder','halibut',\\\n",
    "              'guppy','piranha','carnation','orchid','pansy','daffodil','violet','nectarine','pear','apple','grape','raspberry','tuba',\\\n",
    "              'drum','trumpet','saxophone','piano','tree','ocean','canyon','mountain','plateau','cinnamon','mustard','basil','oregano',\\\n",
    "              'paprika','salt','cotton','wool','velvet','linen','leather','denim','flyer','newspaper','comic','essay','pamphlet','book',\\\n",
    "              'tornado','hail','blizzard','rain','drought','lightning','jacket','dress','blouse','underwear','shoes','shirt','lamp','desk',\\\n",
    "              'bookcase','dresser','chair','recliner','banker','dentist','secretary','engineer','nurse','doctor','hour', 'arms', 'green', 'uncle']    \n",
    "\n",
    "#-------------------------------------Create Data Frame----------------------------------------------\n",
    "\n",
    "df = pd.DataFrame(columns=['SN', 'biased','order', 'collaboration', 'collaborator', 'word', 'correct', 'buffer', 'phase'])\n",
    "\n",
    "first = 1\n",
    "last = 9192\n",
    "skip = [13, 14, 20, 21, 25, 27, 44, 45, 63, 65] \n",
    "# according to the \"non-usable\"-folder these participants were allocated to wrong [\"because there were some participant condition \\\n",
    "# or subject numbers that were entered incorrectly by the RA. This then made the data unusable because the scripts use these values to make sure that participants in the \\\n",
    "# collaboration condition are assigned properly.\"]\n",
    "\n",
    "# Just to have it written down somewhere, these are the collaborative pairs: \n",
    "# 56,78,1112,1718,2223,3031,3435,4041,4849,5253,5657,6061,6970,7374,7778,8182,8384,8586,8788,8990,9192\n",
    "\n",
    "\n",
    "\n",
    "# A: Any subjects we should exclude?\n",
    "for subjectNumber in range(first, last+1):\n",
    "\n",
    "    if subjectNumber in skip:\n",
    "        continue\n",
    "\n",
    "# A: For pre and post collaboration\n",
    "    j = [1, 2] # This is for the retrieval phase\n",
    "    k = [0, 1] # This is indicating collaboration or not\n",
    "    for z in j:\n",
    "        for y in k:\n",
    "        \n",
    "            # A: Import all words from all participants for recall phase 1 and/or 2\n",
    "            try:\n",
    "                with open('new_retrieve_' + str(subjectNumber) +'_' + str(y) + '_' + str(z) + '.pkl', 'rb') as f:\n",
    "                    try:\n",
    "                        exp_data = pickle.load(f, encoding='latin1')\n",
    "                    except EOFError:\n",
    "                        pass\n",
    "\n",
    "                    for trial in exp_data:\n",
    "                        biased = 0\n",
    "                        order = 0\n",
    "                        collaboration = 0\n",
    "                        phase = z\n",
    "                        correct = 0\n",
    "                        buffer = 0\n",
    "\n",
    "                        # Select the word from the pickle files\n",
    "                        word = trial['word']\n",
    "\n",
    "                        # Spell checking\n",
    "                        for x in word:\n",
    "                            word = word.lower()\n",
    "                            word = word.strip()\n",
    "                            df = df.replace(\"\\\\\", '')\n",
    "                            # From experiment 1\n",
    "                            word = ''.join('canyon' if word == 'canyoan' else word for word in word.split())\n",
    "                            word = ''.join('canyon' if word == 'canyons' else word for word in word.split())\n",
    "                            word = ''.join('cherry' if word == 'cherries' else word for word in word.split())\n",
    "                            word = ''.join('chlorine' if word == 'clorine' else word for word in word.split())      \n",
    "                            word = ''.join('cinnamon' if word == 'cinammon' else word for word in word.split())  \n",
    "                            word = ''.join('cinnamon' if word == 'cinnaman' else word for word in word.split())      \n",
    "                            word = ''.join('cinnamon' if word == 'cinnimon' else word for word in word.split())\n",
    "                            word = ''.join('cinnamon' if word == 'cinomman' else word for word in word.split())\n",
    "                            word = ''.join('cotton' if word == 'cotten' else word for word in word.split())\n",
    "                            word = ''.join('daffodil' if word == 'daffidil' else word for word in word.split())\n",
    "                            word = ''.join('daffodil' if word == 'dafodill' else word for word in word.split())      \n",
    "                            word = ''.join('daffodil' if word == 'daphodile' else word for word in word.split())\n",
    "                            word = ''.join('daffodil' if word == 'doffodil' else word for word in word.split())   \n",
    "                            word = ''.join('denim' if word == 'denin' else word for word in word.split())\n",
    "                            word = ''.join('drum' if word == 'drums' else word for word in word.split())\n",
    "                            word = ''.join('engineer' if word == 'enginner' else word for word in word.split())\n",
    "                            word = ''.join('flounder' if word == 'flunder' else word for word in word.split())\n",
    "                            word = ''.join('flyer' if word == 'flier' else word for word in word.split())      \n",
    "                            word = ''.join('flyer' if word == 'flyers' else word for word in word.split())\n",
    "                            word = ''.join('grape' if word == 'grapes' else word for word in word.split())   \n",
    "                            word = ''.join('guppy' if word == 'gupppy' else word for word in word.split())\n",
    "                            word = ''.join('halibut' if word == 'hailbut' else word for word in word.split())\n",
    "                            word = ''.join('halibut' if word == 'halibet' else word for word in word.split())\n",
    "                            word = ''.join('linen' if word == 'linens' else word for word in word.split())      \n",
    "                            word = ''.join('linen' if word == 'linnen' else word for word in word.split())\n",
    "                            word = ''.join('mercury' if word == 'ercury' else word for word in word.split())   \n",
    "                            word = ''.join('mountain' if word == 'mountains' else word for word in word.split())\n",
    "                            word = ''.join('mountain' if word == 'moutain' else word for word in word.split())\n",
    "                            word = ''.join('nectarine' if word == 'necratine' else word for word in word.split())\n",
    "                            word = ''.join('nectarine' if word == 'necterine' else word for word in word.split())      \n",
    "                            word = ''.join('orchid' if word == 'orchad' else word for word in word.split())\n",
    "                            word = ''.join('oregano' if word == 'aregano' else word for word in word.split())   \n",
    "                            word = ''.join('oregano' if word == 'orageno' else word for word in word.split())\n",
    "                            word = ''.join('oregano' if word == 'regano' else word for word in word.split())\n",
    "                            word = ''.join('pamphlet' if word == 'amphlet' else word for word in word.split())\n",
    "                            word = ''.join('pamphlet' if word == 'pamplet' else word for word in word.split())      \n",
    "                            word = ''.join('pamphlet' if word == 'panplet' else word for word in word.split())\n",
    "                            word = ''.join('pamphlet' if word == 'phamlet' else word for word in word.split())\n",
    "                            word = ''.join('pansy' if word == 'ansy' else word for word in word.split())\n",
    "                            word = ''.join('paprika' if word == 'paparika' else word for word in word.split())\n",
    "                            word = ''.join('paprika' if word == 'papprika' else word for word in word.split())\n",
    "                            word = ''.join('paprika' if word == 'paprica' else word for word in word.split())      \n",
    "                            word = ''.join('paprika' if word == 'peprica' else word for word in word.split())\n",
    "                            word = ''.join('pigeon' if word == 'pegion' else word for word in word.split())\n",
    "                            word = ''.join('pigeon' if word == 'pidgeon' else word for word in word.split())\n",
    "                            word = ''.join('pigeon' if word == 'pidgeons' else word for word in word.split())\n",
    "                            word = ''.join('pigeon' if word == 'pidgieon' else word for word in word.split())      \n",
    "                            word = ''.join('piranha' if word == 'paranha' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'pirahana' else word for word in word.split())      \n",
    "                            word = ''.join('piranha' if word == 'pirahna' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'pirahnna' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'pirannah' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'pirannha' else word for word in word.split())      \n",
    "                            word = ''.join('piranha' if word == 'pirhana' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'pirranha' else word for word in word.split())   \n",
    "                            word = ''.join('plateau' if word == 'plateu' else word for word in word.split())\n",
    "                            word = ''.join('plateau' if word == 'platue' else word for word in word.split())\n",
    "                            word = ''.join('raspberry' if word == 'rasberries' else word for word in word.split())\n",
    "                            word = ''.join('raspberry' if word == 'rasberry' else word for word in word.split())      \n",
    "                            word = ''.join('raspberry' if word == 'raspberries' else word for word in word.split())\n",
    "                            word = ''.join('raspberry' if word == 'rasphberry' else word for word in word.split())   \n",
    "                            word = ''.join('saxophone' if word == 'saxaphone' else word for word in word.split())\n",
    "                            word = ''.join('saxophone' if word == 'saxephone' else word for word in word.split())\n",
    "                            word = ''.join('saxophone' if word == 'saxiphone' else word for word in word.split())\n",
    "                            word = ''.join('secretary' if word == 'secratary' else word for word in word.split())      \n",
    "                            word = ''.join('secretary' if word == 'secrertary' else word for word in word.split())\n",
    "                            word = ''.join('shoes' if word == 'shoe' else word for word in word.split())   \n",
    "                            word = ''.join('trumpet' if word == 'trumphet' else word for word in word.split())\n",
    "                            word = ''.join('trumpet' if word == 'tumpet' else word for word in word.split())\n",
    "                            word = ''.join('underwear' if word == 'nderwear' else word for word in word.split())\n",
    "                            word = ''.join('underwear' if word == 'udnerwear' else word for word in word.split())      \n",
    "                            word = ''.join('bookcase' if word == 'bookshelf' else word for word in word.split())\n",
    "                            word = ''.join('chair' if word == 'chari' else word for word in word.split())   \n",
    "                            word = ''.join('lightning' if word == 'lightening' else word for word in word.split())\n",
    "                            word = ''.join('lightning' if word == 'lightenings' else word for word in word.split())\n",
    "                            word = ''.join('lightning' if word == 'lighting' else word for word in word.split())\n",
    "                            word = ''.join('lightning' if word == 'lightning' else word for word in word.split())\n",
    "                            word = ''.join('lightning' if word == 'lightnening' else word for word in word.split())      \n",
    "                            word = ''.join('tree' if word == 'tress' else word for word in word.split())\n",
    "                            word = ''.join('violet' if word == 'voilet' else word for word in word.split())\n",
    "                            word = ''.join('blizzard' if word == 'blizarrd' else word for word in word.split())            \n",
    "                            word = ''.join('velvet' if word == 'velvey' else word for word in word.split())\n",
    "                            word = ''.join('daffodil' if word == 'dafodil' else word for word in word.split())\n",
    "                            word = ''.join('halibut' if word == 'halibit' else word for word in word.split())\n",
    "                            word = ''.join('book' if word == 'books' else word for word in word.split())\n",
    "                            word = ''.join('hurricane' if word == 'hurricaine' else word for word in word.split())                    \n",
    "\n",
    "                            # From experiment 2\n",
    "                            word = ''.join('' if word == 'ado' else word for word in word.split()) # one row was ado, second tornao, third tornado               \n",
    "                            word = ''.join('dandelion' if word == 'dandalione' else word for word in word.split())                \n",
    "                            word = ''.join('dandelion' if word == 'dandeline' else word for word in word.split())                \n",
    "                            word = ''.join('dandelion' if word == 'dandelion' else word for word in word.split())                \n",
    "                            word = ''.join('dandelion' if word == 'dandelion' else word for word in word.split())                  \n",
    "                            word = ''.join('dandelion' if word == 'dandalione' else word for word in word.split())                \n",
    "                            word = ''.join('dandelion' if word == 'dandilion' else word for word in word.split())                  \n",
    "                            #word = ''.join('finch' if word == 'flinch' else word for word in word.split()) as flinch has a different meaning, we didn't change it                   \n",
    "                            word = ''.join('oregano' if word == 'gano' else word for word in word.split()) \n",
    "                            word = ''.join('halibut' if word == 'halbait' else word for word in word.split())                \n",
    "                            #word = ''.join('hail' if word == 'hale' else word for word in word.split()) as hale has a different meaning, we didn't change it                \n",
    "                            #word = ''.join('linen' if word == 'lenin' else word for word in word.split()) as lenin has a different meaning, we didn't change it                 \n",
    "                            word = ''.join('velvet' if word == 'lvet' else word for word in word.split())                  \n",
    "                            word = ''.join('mountain' if word == 'ntain' else word for word in word.split())                \n",
    "                            word = ''.join('nurse' if word == 'nurde' else word for word in word.split())                    \n",
    "                            word = ''.join('' if word == 'ado' else word for word in word.split()) # one row was ado, second tornao, third tornado               \n",
    "                            word = ''.join('blouse' if word == 'ouse' else word for word in word.split())                \n",
    "                            word = ''.join('pants' if word == 'pant' else word for word in word.split())                \n",
    "                            word = ''.join('piranha' if word == 'parhna' else word for word in word.split())\n",
    "                            #word = ''.join('pear' if word == 'peat' else word for word in word.split()) as peat has a different meaning, we didn't change it\n",
    "                            word = ''.join('oregano' if word == 'pregeno' else word for word in word.split())                  \n",
    "                            word = ''.join('dresser' if word == 'sser' else word for word in word.split()) # difficult, but all other recalled items from that participant \\\n",
    "                            # had a couple of letters missing in the front (see ntain or tton)\n",
    "                            word = ''.join('cotton' if word == 'tton' else word for word in word.split())                  \n",
    "                            word = ''.join('basil' if word == 'brasil' else word for word in word.split())                  \n",
    "                            word = ''.join('tangerine' if word == 'tangarine' else word for word in word.split())                \n",
    "                            word = ''.join('cloth' if word == 'clothes' else word for word in word.split())                \n",
    "                            word = ''.join('cloth' if word == 'clothing' else word for word in word.split())                \n",
    "                            word = ''.join('cloud' if word == 'clouds' else word for word in word.split())                  \n",
    "                            word = ''.join('book' if word == 'books' else word for word in word.split())                \n",
    "                            word = ''.join('lily' if word == 'lilly' else word for word in word.split())\n",
    "                            word = ''.join('oregano' if word == 'oragano' else word for word in word.split())                \n",
    "                            word = ''.join('tornado' if word == 'tronado' else word for word in word.split())                \n",
    "                            word = ''.join('apple' if word == 'apll' else word for word in word.split())                \n",
    "                            word = ''.join('comic' if word == 'comics' else word for word in word.split())                \n",
    "                            word = ''.join('daffodil' if word == 'fodil' else word for word in word.split())                \n",
    "                            word = ''.join('halibut' if word == 'habut' else word for word in word.split())                  \n",
    "                            word = ''.join('lightning' if word == 'lighnting' else word for word in word.split())                \n",
    "                            word = ''.join('lightning' if word == 'lightining' else word for word in word.split())             \n",
    "                            word = ''.join('lightning' if word == 'lightning' else word for word in word.split())      \n",
    "                            word = ''.join('organ' if word == 'organs' else word for word in word.split()) # Organ has multiple meanings by itself, hence potentially ok to change\n",
    "                            word = ''.join('pigeon' if word == 'piogen' else word for word in word.split())      \n",
    "                            word = ''.join('paprika' if word == 'pipraki' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'purrana' else word for word in word.split())\n",
    "                            word = ''.join('drought' if word == 'rought' else word for word in word.split())\n",
    "                            word = ''.join('trumpet' if word == 'trumphent' else word for word in word.split())\n",
    "                            word = ''.join('trumpet' if word == 'trumpht' else word for word in word.split())      \n",
    "                            word = ''.join('trumpet' if word == 'trumpt' else word for word in word.split())\n",
    "                            word = ''.join('bookcase' if word == 'bookself' else word for word in word.split())                                   \n",
    "                            word = ''.join('bookcase' if word == 'bookshlelf' else word for word in word.split())                \n",
    "                            word = ''.join('bookcase' if word == 'boookcase' else word for word in word.split())                \n",
    "                            word = ''.join('chlorine' if word == 'chloride' else word for word in word.split())\n",
    "                            word = ''.join('chlorine' if word == 'chorline' else word for word in word.split())  \n",
    "                            word = ''.join('chlorine' if word == 'cholrine' else word for word in word.split())                  \n",
    "                            word = ''.join('chlorine' if word == 'chroine' else word for word in word.split())                \n",
    "                            word = ''.join('cinnamon' if word == 'cinamon' else word for word in word.split())\n",
    "                            word = ''.join('cinnamon' if word == 'cinnimin' else word for word in word.split())                \n",
    "                            word = ''.join('cinnamon' if word == 'cinnoman' else word for word in word.split())                \n",
    "                            word = ''.join('cinnamon' if word == 'cinnomin' else word for word in word.split())                \n",
    "                            word = ''.join('cotton' if word == 'coten' else word for word in word.split())                  \n",
    "                            word = ''.join('halibut' if word == 'halibat' else word for word in word.split())                \n",
    "                            word = ''.join('halibut' if word == 'halibiut' else word for word in word.split())\n",
    "                            word = ''.join('halibut' if word == 'hallibut' else word for word in word.split())                \n",
    "                            word = ''.join('halibut' if word == 'halubit' else word for word in word.split())                \n",
    "                            word = ''.join('helium' if word == 'heliu,' else word for word in word.split())\n",
    "                            word = ''.join('helium' if word == 'elium' else word for word in word.split())\n",
    "                            word = ''.join('helium' if word == 'heliu' else word for word in word.split())   \n",
    "                            word = ''.join('hour' if word == 'hours' else word for word in word.split())                  \n",
    "                            word = ''.join('paprika' if word == 'prika' else word for word in word.split())                \n",
    "                            word = ''.join('rose' if word == 'roses' else word for word in word.split())\n",
    "                            word = ''.join('oregano' if word == 'aregeno' else word for word in word.split())                \n",
    "                            word = ''.join('blizzard' if word == 'blizzzard' else word for word in word.split())                \n",
    "                            word = ''.join('blouse' if word == 'blousse' else word for word in word.split())                \n",
    "                            word = ''.join('daffodil' if word == 'dafadil' else word for word in word.split())                  \n",
    "                            word = ''.join('daffodil' if word == 'daffadil' else word for word in word.split())                \n",
    "                            word = ''.join('daffodil' if word == 'daffildil' else word for word in word.split())                \n",
    "                            word = ''.join('daffodil' if word == 'daffodils' else word for word in word.split())                \n",
    "                            word = ''.join('daffodil' if word == 'daffoldil' else word for word in word.split())                \n",
    "                            word = ''.join('daffodil' if word == 'dafidil' else word for word in word.split())                \n",
    "                            word = ''.join('daffodil' if word == 'dafidill' else word for word in word.split())                  \n",
    "                            word = ''.join('daffodil' if word == 'dafidill' else word for word in word.split())                \n",
    "                            word = ''.join('daffodil' if word == 'daphadil' else word for word in word.split())\n",
    "                            word = ''.join('daffodil' if word == 'daphadile' else word for word in word.split())                \n",
    "                            word = ''.join('daffodil' if word == 'daphadile' else word for word in word.split())                \n",
    "                            word = ''.join('daffodil' if word == 'daphadill' else word for word in word.split())                \n",
    "                            word = ''.join('daffodil' if word == 'daphidile' else word for word in word.split())                  \n",
    "                            word = ''.join('daffodil' if word == 'daphodil' else word for word in word.split())                \n",
    "                            word = ''.join('daffodil' if word == 'daphodil' else word for word in word.split())                \n",
    "                            word = ''.join('daffodil' if word == 'dapphodil' else word for word in word.split())                \n",
    "                            word = ''.join('dentist' if word == 'dentis' else word for word in word.split())                \n",
    "                            word = ''.join('dresser' if word == 'dreser' else word for word in word.split())                \n",
    "                            word = ''.join('dress' if word == 'dresses' else word for word in word.split()) #very difficult, either dress or dresser, therefore we left it as is                \n",
    "                            word = ''.join('drought' if word == 'drout' else word for word in word.split())                \n",
    "                            word = ''.join('drum' if word == 'dum' else word for word in word.split())                   \n",
    "                            word = ''.join('engineer' if word == 'engeneer' else word for word in word.split())                \n",
    "                            word = ''.join('engineer' if word == 'enigineer' else word for word in word.split())                \n",
    "                            word = ''.join('helium' if word == 'helim' else word for word in word.split())                \n",
    "                            word = ''.join('linen' if word == 'linene' else word for word in word.split())\n",
    "                            word = ''.join('lamp' if word == 'lamo' else word for word in word.split()) \n",
    "                            word = ''.join('mountain' if word == 'mountian' else word for word in word.split())                \n",
    "                            word = ''.join('nectarine' if word == 'necatrina' else word for word in word.split())                   \n",
    "                            word = ''.join('nectarine' if word == 'necatrine' else word for word in word.split())                \n",
    "                            word = ''.join('nectarine' if word == 'nectarine' else word for word in word.split())                \n",
    "                            word = ''.join('nectarine' if word == 'nectatine' else word for word in word.split())                \n",
    "                            word = ''.join('nectarine' if word == 'nectorin' else word for word in word.split())                  \n",
    "                            word = ''.join('nectarine' if word == 'nectorine' else word for word in word.split())                \n",
    "                            word = ''.join('nectarine' if word == 'nectrine' else word for word in word.split())                   \n",
    "                            word = ''.join('nectarine' if word == 'necturine' else word for word in word.split())                \n",
    "                            word = ''.join('nectarine' if word == 'nictarine' else word for word in word.split())                                             \n",
    "                            word = ''.join('nitrogen' if word == 'nigrogen' else word for word in word.split())                  \n",
    "                            word = ''.join('nitrogen' if word == 'nitrogren' else word for word in word.split())                \n",
    "                            word = ''.join('oxygen' if word == 'ocygen' else word for word in word.split())                \n",
    "                            word = ''.join('orchid' if word == 'orchird' else word for word in word.split())                \n",
    "                            word = ''.join('oregano' if word == 'oreganno' else word for word in word.split())                \n",
    "                            word = ''.join('oregano' if word == 'oregeno' else word for word in word.split())                \n",
    "                            word = ''.join('oregano' if word == 'oregno' else word for word in word.split())                  \n",
    "                            word = ''.join('oregano' if word == 'oregno' else word for word in word.split())                \n",
    "                            word = ''.join('oregano' if word == 'oregono' else word for word in word.split())\n",
    "                            word = ''.join('oxygen' if word == 'oxogen' else word for word in word.split())                \n",
    "                            word = ''.join('paprika' if word == 'papricka' else word for word in word.split())                \n",
    "                            word = ''.join('parrot' if word == 'parot' else word for word in word.split())                \n",
    "                            word = ''.join('paprika' if word == 'peperika' else word for word in word.split())                  \n",
    "                            word = ''.join('pamphlet' if word == 'phamplet' else word for word in word.split())                \n",
    "                            word = ''.join('piranha' if word == 'phirana' else word for word in word.split())                \n",
    "                            word = ''.join('piranha' if word == 'pirana' else word for word in word.split())                                \n",
    "                            word = ''.join('piranha' if word == 'piranah' else word for word in word.split())                  \n",
    "                            word = ''.join('piranha' if word == 'piranna' else word for word in word.split())                \n",
    "                            word = ''.join('piranha' if word == 'pirhanna' else word for word in word.split())                \n",
    "                            word = ''.join('piranha' if word == 'pirnaha' else word for word in word.split())                \n",
    "                            word = ''.join('piranha' if word == 'pirrhana' else word for word in word.split())                \n",
    "                            word = ''.join('plateau' if word == 'plaeatu' else word for word in word.split())                \n",
    "                            word = ''.join('plateau' if word == 'plataue' else word for word in word.split())                  \n",
    "                            word = ''.join('plateau' if word == 'plateua' else word for word in word.split())                \n",
    "                            word = ''.join('plateau' if word == 'plateux' else word for word in word.split())\n",
    "                            word = ''.join('plateau' if word == 'plaute' else word for word in word.split())                \n",
    "                            word = ''.join('plateau' if word == 'pleatu' else word for word in word.split())                \n",
    "                            word = ''.join('plateau' if word == 'pleteau' else word for word in word.split())                \n",
    "                            word = ''.join('raspberry' if word == 'rasperry' else word for word in word.split())                  \n",
    "                            word = ''.join('trumpet' if word == 'rumpet' else word for word in word.split())                \n",
    "                            word = ''.join('secretary' if word == 'sactatary' else word for word in word.split())                \n",
    "                            word = ''.join('saxophone' if word == 'saxphone' else word for word in word.split())\n",
    "                            word = ''.join('tornado' if word == 'torando' else word for word in word.split())                  \n",
    "                            word = ''.join('tornado' if word == 'tornadeo' else word for word in word.split())                \n",
    "                            word = ''.join('tornado' if word == 'tornadoe' else word for word in word.split())                \n",
    "                            word = ''.join('tornado' if word == 'tornadoe' else word for word in word.split())                \n",
    "                            word = ''.join('tornado' if word == 'tornador' else word for word in word.split())                \n",
    "                            word = ''.join('tornado' if word == 'tornando' else word for word in word.split())                \n",
    "                            word = ''.join('tornado' if word == 'tornao' else word for word in word.split())                  \n",
    "                            word = ''.join('tree' if word == 'trees' else word for word in word.split())                \n",
    "                            word = ''.join('trumpet' if word == 'trmpet' else word for word in word.split())\n",
    "                            word = ''.join('trumpet' if word == 'trmpet' else word for word in word.split())                \n",
    "                            word = ''.join('trumpet' if word == 'trupmet' else word for word in word.split())                \n",
    "                            word = ''.join('underwear' if word == 'undewear' else word for word in word.split())                \n",
    "                            word = ''.join('velvet' if word == 'velvot' else word for word in word.split())                  \n",
    "                            word = ''.join('velvet' if word == 'vevelt' else word for word in word.split())                \n",
    "                            word = ''.join('velvet' if word == 'volvet' else word for word in word.split())                \n",
    "                            word = ''.join('saxophone' if word == 'xophone' else word for word in word.split())                \n",
    "                            word = ''.join('oxygen' if word == 'xygen' else word for word in word.split())                  \n",
    "                            word = ''.join('daffodil' if word == 'dafodil' else word for word in word.split())\n",
    "                            word = ''.join('halibut' if word == 'halibit' else word for word in word.split())\n",
    "                            word = ''.join('raspberry, grape' if word == 'raspberrygrape' else word for word in word.split())\n",
    "                            word = ''.join('desk, chair, lamp, hour' if word == 'deskchairlamphour' else word for word in word.split())\n",
    "                            word = ''.join('tornado, hail' if word == 'tornadohail' else word for word in word.split())                \n",
    "\n",
    "                            #Buffer\n",
    "                            word = ''.join('arms' if word == 'arm' else word for word in word.split())\n",
    "                            word = ''.join('green' if word == 'grreen' else word for word in word.split())\n",
    "\n",
    "\n",
    "                            # From experiment 3\n",
    "                            word = ''.join('apple' if word == 'appl' else word for word in word.split())\n",
    "                            word = ''.join('oregano' if word == 'arangano' else word for word in word.split())\n",
    "                            word = ''.join('arms' if word == 'arm' else word for word in word.split())\n",
    "                            word = ''.join('oregano' if word == 'arregano' else word for word in word.split())\n",
    "                            word = ''.join('saxophone' if word == 'axophpne' else word for word in word.split())\n",
    "                            word = ''.join('blizzard' if word == 'Bizzard' else word for word in word.split())\n",
    "                            word = ''.join('blizzard' if word == 'blizard' else word for word in word.split())\n",
    "                            word = ''.join('blouse' if word == 'Blouse' else word for word in word.split())\n",
    "                            word = ''.join('blouse' if word == 'blouser' else word for word in word.split())\n",
    "                            word = ''.join('bookcase' if word == 'bookshelf' else word for word in word.split())\n",
    "                            word = ''.join('book' if word == 'boooks' else word for word in word.split())\n",
    "                            word = ''.join('cardinal' if word == 'Cardinal' else word for word in word.split())\n",
    "                            word = ''.join('cardigan' if word == 'cartigan' else word for word in word.split())\n",
    "                            word = ''.join('chlorine' if word == 'chloride' else word for word in word.split())\n",
    "                            word = ''.join('comic' if word == 'Comic' else word for word in word.split())\n",
    "                            word = ''.join('cotton' if word == 'cutton' else word for word in word.split())\n",
    "                            word = ''.join('daffodil' if word == 'daffadil' else word for word in word.split())\n",
    "                            word = ''.join('daffodil' if word == 'daffildol' else word for word in word.split())\n",
    "                            word = ''.join('daffodil' if word == 'daffodile' else word for word in word.split())\n",
    "                            word = ''.join('daffodil' if word == 'daffodils' else word for word in word.split())\n",
    "                            word = ''.join('daffodil' if word == 'dafidil' else word for word in word.split())\n",
    "                            word = ''.join('daffodil' if word == 'dafiodil' else word for word in word.split())\n",
    "                            word = ''.join('dandelion' if word == 'dandelion' else word for word in word.split())\n",
    "                            word = ''.join('dandelion' if word == 'danelion' else word for word in word.split())\n",
    "                            word = ''.join('daffodil' if word == 'daphodil' else word for word in word.split())\n",
    "                            word = ''.join('daffodil' if word == 'daphodyl' else word for word in word.split())\n",
    "                            word = ''.join('doctor' if word == 'doctore' else word for word in word.split())\n",
    "                            word = ''.join('dress' if word == 'Dress' else word for word in word.split())\n",
    "                            word = ''.join('dress' if word == 'dresses' else word for word in word.split())\n",
    "                            word = ''.join('dresser' if word == 'dressor' else word for word in word.split())\n",
    "                            word = ''.join('drought' if word == 'drough' else word for word in word.split())\n",
    "                            word = ''.join('drum' if word == 'drums' else word for word in word.split())\n",
    "                            word = ''.join('eagle' if word == 'Eagle' else word for word in word.split())\n",
    "                            word = ''.join('eagle' if word == 'egale' else word for word in word.split())\n",
    "                            word = ''.join('engineer' if word == 'eingineer' else word for word in word.split())\n",
    "                            word = ''.join('finch' if word == 'finc' else word for word in word.split())\n",
    "                            word = ''.join('finch' if word == 'Finch' else word for word in word.split())\n",
    "                            word = ''.join('grape' if word == 'graoe' else word for word in word.split())\n",
    "                            word = ''.join('grape' if word == 'grap' else word for word in word.split())\n",
    "                            word = ''.join('grape' if word == 'Grape' else word for word in word.split())\n",
    "                            word = ''.join('grape' if word == 'grapes' else word for word in word.split())\n",
    "                            word = ''.join('green' if word == 'Green' else word for word in word.split())\n",
    "                            word = ''.join('guppy' if word == 'gruppy' else word for word in word.split())\n",
    "                            word = ''.join('guppy' if word == 'gubby' else word for word in word.split())\n",
    "                            word = ''.join('guppy' if word == 'Guppy' else word for word in word.split())\n",
    "                            word = ''.join('halibut' if word == 'hailbut' else word for word in word.split())\n",
    "                            word = ''.join('halibut' if word == 'halbit' else word for word in word.split())\n",
    "                            word = ''.join('halibut' if word == 'halibat' else word for word in word.split())\n",
    "                            word = ''.join('helium' if word == 'helium\\'' else word for word in word.split())\n",
    "                            word = ''.join('helium' if word == 'heluim' else word for word in word.split())\n",
    "                            word = ''.join('nitrogen' if word == 'hitrogen' else word for word in word.split())\n",
    "                            word = ''.join('hour' if word == 'Hour' else word for word in word.split())\n",
    "                            word = ''.join('linen' if word == 'inen' else word for word in word.split())\n",
    "                            word = ''.join('jeans' if word == 'jean' else word for word in word.split())\n",
    "                            word = ''.join('jesus' if word == 'Jesus' else word for word in word.split())\n",
    "                            word = ''.join('jupiter' if word == 'jupitor' else word for word in word.split())\n",
    "                            word = ''.join('leather' if word == 'lether' else word for word in word.split())\n",
    "                            word = ''.join('lightning' if word == 'lightining' else word for word in word.split())\n",
    "                            word = ''.join('lightning' if word == 'lightning' else word for word in word.split())\n",
    "                            word = ''.join('linen' if word == 'linnen' else word for word in word.split())\n",
    "                            word = ''.join('mountain' if word == 'mountains' else word for word in word.split())\n",
    "                            word = ''.join('trumpet' if word == 'mpet' else word for word in word.split())\n",
    "                            word = ''.join('nectarine' if word == 'nectorine' else word for word in word.split())\n",
    "                            word = ''.join('nectarine' if word == 'nectraine' else word for word in word.split())\n",
    "                            word = ''.join('nitrogen' if word == 'nitrgoen' else word for word in word.split())\n",
    "                            word = ''.join('nitrogen' if word == 'Nitrogen' else word for word in word.split())\n",
    "                            word = ''.join('nitrogen' if word == 'nitrogen\\'' else word for word in word.split())\n",
    "                            word = ''.join('ocean' if word == 'Ocean' else word for word in word.split())\n",
    "                            word = ''.join('orchid' if word == 'ochid' else word for word in word.split())\n",
    "                            word = ''.join('octopus' if word == 'octupus' else word for word in word.split())\n",
    "                            word = ''.join('orchid' if word == 'orchids' else word for word in word.split())\n",
    "                            word = ''.join('orchid' if word == 'orchird' else word for word in word.split())\n",
    "                            word = ''.join('orchid' if word == 'orcid' else word for word in word.split())\n",
    "                            #word = ''.join('oregano' if word == 'oregon' else word for word in word.split())\n",
    "                            word = ''.join('oregano' if word == 'organo' else word for word in word.split())\n",
    "                            word = ''.join('oxygen' if word == 'Oxygen' else word for word in word.split())\n",
    "                            word = ''.join('piano' if word == 'paino' else word for word in word.split())\n",
    "                            word = ''.join('pamphlet' if word == 'pamphelt' else word for word in word.split())\n",
    "                            word = ''.join('pamphlet' if word == 'pamplet' else word for word in word.split())\n",
    "                            word = ''.join('paprika' if word == 'paprikka' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'parahana' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'paranha' else word for word in word.split())\n",
    "                            word = ''.join('pansy' if word == 'pasny' else word for word in word.split())\n",
    "                            word = ''.join('piano' if word == 'pianp' else word for word in word.split())\n",
    "                            word = ''.join('pigeon' if word == 'pidgeon' else word for word in word.split())\n",
    "                            word = ''.join('pigeon' if word == 'piegon' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'pihrana' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'pirahna' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'pirahnna' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'piranah' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'pirannhea' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'pirhana' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'pirhanna' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'pirranha' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'pirranhea' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'pirrhana' else word for word in word.split())\n",
    "                            word = ''.join('plateau' if word == 'plataeu' else word for word in word.split())\n",
    "                            word = ''.join('plateau' if word == 'plateu' else word for word in word.split())\n",
    "                            word = ''.join('pleasant' if word == 'pleasent' else word for word in word.split())\n",
    "                            word = ''.join('plateau' if word == 'pleatou' else word for word in word.split())\n",
    "                            word = ''.join('oxygen' if word == 'poxygen' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'prihana' else word for word in word.split())\n",
    "                            word = ''.join('raspberry' if word == 'rasberry' else word for word in word.split())\n",
    "                            word = ''.join('raspberry' if word == 'raseberry' else word for word in word.split())\n",
    "                            word = ''.join('robin' if word == 'robbin' else word for word in word.split())\n",
    "                            word = ''.join('saxophone' if word == 'saxaphone' else word for word in word.split())\n",
    "                            word = ''.join('saxophone' if word == 'saxapphone' else word for word in word.split())\n",
    "                            word = ''.join('saxophone' if word == 'saxohpone' else word for word in word.split())\n",
    "                            word = ''.join('saxophone' if word == 'Saxophone' else word for word in word.split())\n",
    "                            word = ''.join('saxophone' if word == 'saxopon' else word for word in word.split())\n",
    "                            word = ''.join('saxophone' if word == 'saxphone' else word for word in word.split())\n",
    "                            word = ''.join('secretary' if word == 'secatary' else word for word in word.split())\n",
    "                            word = ''.join('secretary' if word == 'Secretary' else word for word in word.split())\n",
    "                            word = ''.join('shoes' if word == 'shoe' else word for word in word.split())\n",
    "                            word = ''.join('sulfur' if word == 'sulfure' else word for word in word.split())\n",
    "                            word = ''.join('tangerine' if word == 'tangerene' else word for word in word.split())\n",
    "                            word = ''.join('tornado' if word == 'tornadeo' else word for word in word.split())\n",
    "                            word = ''.join('tree' if word == 'Tree' else word for word in word.split())\n",
    "                            word = ''.join('tree' if word == 'trees' else word for word in word.split())\n",
    "                            word = ''.join('drought' if word == 'trought' else word for word in word.split())\n",
    "                            word = ''.join('trombone' if word == 'trumbone' else word for word in word.split())\n",
    "                            word = ''.join('trumpet' if word == 'Trumpet' else word for word in word.split())\n",
    "                            word = ''.join('trumpet' if word == 'trumphet' else word for word in word.split())\n",
    "                            word = ''.join('turmeric' if word == 'tumeric' else word for word in word.split())\n",
    "                            word = ''.join('trumpet' if word == 'umpet' else word for word in word.split())\n",
    "                            word = ''.join('velvet' if word == 'Velvet' else word for word in word.split())\n",
    "                            word = ''.join('velvet' if word == 'velvet3' else word for word in word.split())\n",
    "                            word = ''.join('velvet' if word == 'velviot' else word for word in word.split())\n",
    "                            word = ''.join('violet' if word == 'Violet' else word for word in word.split())\n",
    "                            word = ''.join('women' if word == 'woman' else word for word in word.split())\n",
    "                            word = ''.join('wool' if word == 'wool\\\\' else word for word in word.split())\n",
    "                            word = ''.join('saxophone' if word == 'xophone' else word for word in word.split())\n",
    "                            word = ''.join('chair' if word == 'chair\\\\' else word for word in word.split())\n",
    "                            word = ''.join('daffodil' if word == 'daffodil\\\\' else word for word in word.split())\n",
    "                            word = ''.join('ocean' if word == 'ocean\\\\' else word for word in word.split())\n",
    "                            word = ''.join('leather' if word == 'eather' else word for word in word.split())\n",
    "                            word = ''.join('choir' if word == 'chior' else word for word in word.split())\n",
    "                            word = ''.join('halibut' if word == 'haliberd' else word for word in word.split())\n",
    "                                                        \n",
    "                            # Experiment 3, recall 2\n",
    "                            word = ''.join('tree' if word == 'dree' else word for word in word.split())\n",
    "                            word = ''.join('piranha' if word == 'piranhea' else word for word in word.split())\n",
    "                            word = ''.join('oxygen' if word == 'oxigen' else word for word in word.split())\n",
    "                            word = ''.join('organ' if word == 'ogran' else word for word in word.split())\n",
    "                            word = ''.join('chair' if word == 'chaor' else word for word in word.split())\n",
    "                            word = ''.join('rain' if word == 'rain\\'' else word for word in word.split())\n",
    "                            word = ''.join('blouse' if word == 'blosue' else word for word in word.split())\n",
    "                            word = ''.join('oxygen' if word == 'oxegen' else word for word in word.split())\n",
    "                            word = ''.join('parrot' if word == 'parrot\\\\' else word for word in word.split())\n",
    "                            word = ''.join('engineer' if word == 'enigneer' else word for word in word.split())\n",
    "                            word = ''.join('halibut' if word == 'halabit' else word for word in word.split())\n",
    "                            word = ''.join('secretary' if word == 'secetary' else word for word in word.split())\n",
    "                            word = ''.join('dentist' if word == 'demtist' else word for word in word.split())\n",
    "                            word = ''.join('dentist' if word == 'denist' else word for word in word.split())                       \n",
    "                            \n",
    "                            \n",
    "                        # Biase comes from file name\n",
    "                        if os.path.exists('new_encoding_' + str(subjectNumber) +'_0.pkl'):\n",
    "                            biased = 0\n",
    "                        else:\n",
    "                            biased = 1\n",
    "\n",
    "                        # Order comes from SN and impacts correct study list reference\n",
    "                        if (subjectNumber%2 == 0) and (biased == 1):\n",
    "                            order = 1\n",
    "                            if word in studyList2:\n",
    "                                correct = 1\n",
    "                            else:\n",
    "                                correct = 0\n",
    "                        elif (subjectNumber%2 == 1) and (biased == 1):\n",
    "                            order = 0     \n",
    "                            if word in studyList1:\n",
    "                                correct = 1\n",
    "                            else:\n",
    "                                correct = 0\n",
    "                        if (subjectNumber%2 == 0) and (biased == 0):\n",
    "                            order = 0\n",
    "                            if word in studyList1:\n",
    "                                correct = 1\n",
    "                            else:\n",
    "                                correct = 0\n",
    "                        elif (subjectNumber%2 == 1) and (biased == 0):\n",
    "                            order = 1     \n",
    "                            if word in studyList2:\n",
    "                                correct = 1\n",
    "                            else:\n",
    "                                correct = 0\n",
    "                      \n",
    "\n",
    "                        # Indicate buffer\n",
    "                        buffer = 0\n",
    "                        for x in word:\n",
    "                            if word in ['arms', 'green', 'uncle', 'hour']:\n",
    "                                buffer = 1\n",
    "                            else:\n",
    "                                buffer = 0\n",
    "\n",
    "                        # Collaboration comes from retrieval_1_2 files (1_2 collaborated, 0_2 did not)\n",
    "                        if os.path.exists('new_retrieve_' + str(subjectNumber) +'_0_2.pkl'):\n",
    "                            collaboration = 0\n",
    "                        else:\n",
    "                            collaboration = 1\n",
    "\n",
    "                        # Collaborator for recall 1 and 3 easy to identify\n",
    "                        # However, at the moment, phase 2 is included with a specific SN that is SN1 and SN2 concatenanted\n",
    "                        # Therefore, SN56 in recall 1 is NOT SN56 in recall 2. All SN in recall 2 are concatenated and have bias=2 & order=2\n",
    "                        collaborator = 0\n",
    "                        if (subjectNumber == 56) and (collaboration == 1) and (phase == 2):\n",
    "                            collaborator = str(0)\n",
    "                        elif (subjectNumber == 78) and (collaboration == 1) and (phase == 2):\n",
    "                            collaborator = str(0)\n",
    "                        elif os.path.exists('new_retrieve_' + str(subjectNumber) + str(subjectNumber+1) + '_1_2.pkl'):\n",
    "                            collaborator = str(subjectNumber+1)\n",
    "                        elif os.path.exists('new_retrieve_' + str(subjectNumber-1) + str(subjectNumber) + '_1_2.pkl'):\n",
    "                            collaborator = str(subjectNumber-1)\n",
    "                        else:\n",
    "                            collaborator = 0\n",
    "\n",
    "                        # all collaborating dyads (SN with numbers > 94 and the two 56 and 78) get a 2 (similar to na) for order and bias\n",
    "                        if (subjectNumber > 94):\n",
    "                            collaboration=1\n",
    "                            order = 2\n",
    "                            biased = 2\n",
    "                        elif (subjectNumber == 56) and (collaboration == 1) and (phase == 2):\n",
    "                            collaboration=1\n",
    "                            order = 2\n",
    "                            biased = 2  \n",
    "                        elif (subjectNumber == 78) and (collaboration == 1) and (phase == 2):\n",
    "                            collaboration=1\n",
    "                            order = 2\n",
    "                            biased = 2\n",
    "                        else:\n",
    "                            pass\n",
    "                            \n",
    "                        trialDict = {'SN':subjectNumber, 'biased':biased, 'order':order, 'phase':phase, 'collaboration':collaboration, 'collaborator':collaborator, \n",
    "                                     'word':word, 'correct':correct, 'buffer':buffer}\n",
    "                        df = df.append(trialDict, ignore_index=True)\n",
    "            except OSError:\n",
    "                logging.info('File missing' + str(f))\n",
    "                pass\n",
    "\n",
    "df = df.drop_duplicates(['SN','word','phase']) # Delete duplicate recalls in the same phase by the same participant\n",
    "df = df.replace('', np.nan) \n",
    "df = df.dropna(axis=0, how='any', subset=['word']) # Delete rows with empty word\n",
    "\n",
    "\n",
    "\n",
    "#df.to_csv('2020-09-17_Similarity_Exp3_AllWords_Clean1_Check.csv', index=False)\n",
    "df['number'] = df['word']\n",
    "\n",
    "# Includes misspellings and buffer words, same as in experiments 1 and 2 plus new intrusions and buffer\n",
    "translator = {'crow':1,'eagle':2,'finch':3,'parrot':4,'pigeon':5,'cardinal':6,'nitrogen':7,'helium':8,'chlorine':9,'calcium':10,'oxygen':11,\n",
    "          'mercury':12,'trout':13,'flounder':14,'halibut':15,'guppy':16,'piranha':17,'shark':18,'carnation':19,'orchid':20,'pansy':21,'daffodil':22,'violet':23,'rose':24,\n",
    "          'nectarine':25,'pear':26,'apple':27,'grape':28,'raspberry':29,'cherry':30,'tuba':31,'drum':32,'trumpet':33,'saxophone':34,'piano':35,'organ':36,'tree':37,'ocean':38,\n",
    "          'canyon':39,'mountain':40,'plateau':41,'cave':42,'cinnamon':43,'mustard':44,'basil':45,'oregano':46,'paprika':47,'salt':48,'cotton':49,'wool':50,'velvet':51,\n",
    "          'linen':52,'leather':53,'denim':54,'flyer':55,'newspaper':56,'comic':57,'essay':58,'pamphlet':59,'book':60,'tornado':61,'hail':62,'blizzard':63,'rain':64,'drought':65,\n",
    "          'lightning':66,'jacket':67,'dress':68,'blouse':69,'underwear':70,'shoes':71,'shirt':72,'lamp':73,'desk':74,'bookcase':75,'dresser':76,'chair':77,'recliner':78,\n",
    "          'banker':79,'dentist':80,'secretary':81,'engineer':82,'nurse':83,'doctor':84,'a':85,'architect':86,'avidafil?':87,'ballet':88,'baseball':89,'bed':90,\n",
    "          'bird':91,'blueberry':92,'cage':93,'canary':94,'carbon':95,'chemical':96,'clarinet':97,'coat':98,'concrete':99,'daisy':100,'dog':101,'experimenter':102,\n",
    "          'fish':103,'flamenco':104,'flower':105,'flute':106,'fruit':107,'give':108,'grass':109,'hydrogen':110,'instrument':111,'lawyer':112,'library':113,'lily':114,\n",
    "          'lithium':115,'lung':116,'melon':117,'music':118,'nickel':119,'nylon':120,'pants':121,'parsley':122,'pepper':123,'project':124,'red':125,'salmon':126,\n",
    "          'satin':127,'sea':128,'sky':129,'slave':130,'sofa':131,'storm':132,'table':133,'tango':134,'teacher':135,'the':136,'thunder':137,'thunderstorm':138,'tissue':139,\n",
    "          'to':140,'trombone':141,'trousers':142,'tulip':143,'tuna':144,'willing':145,'address':146,'cavern':147,'homework':148,'magazine':149,'are':150,\n",
    "          'dove':151,'sodium':152,'hurricaine':153,'hurricane':154,'violent':155,'violin':156,'green':157,'uncle':158,'hour':159,'arms':160,'orchard':161,\n",
    "          'paper':162,'sunflower':163,'dandelion':164,'snow':165,'green':166,'professor':167,'socks':168,'purse':169,'ar':170,'e':171,'mium':172,\n",
    "          't':173,'rk':174, 'hid':175,'breakfast':176,'uncle':177,'hat':178,'hale':179,'speacker':180,'dance':181,'baker':182,'couch':183,'tube':184,\n",
    "          'valley':185,'tangerine':186,'guitar':187,'orange':188,'skirt':189,'robin':190,'peach':191,'lavender':192,'thyme':193,'drawer':194,\n",
    "          'cloud':195,'word':196,'paper':197,'carnelian':198,'article':199,'daydream':200,'tectonicplate':201,'blossom':202,'cloth':203,'tsunami':204,\n",
    "          'legs':205,'keyboard':206,'fiction':207,'water':208,'earth':209,'light':210,'hair':211,'rocks':212,'moon':213,'weather':214,'wind':215,\n",
    "          'door':216,'letter':217,'time':218,'yellow':219,'river':220,'lake':221,'typhoon':222,'gloves':223,'magnesium':224,'ear':225,'camp':226,\n",
    "          'seed':227,'phone':228,'fresh':229,'lenin':230,'in':231,'halogen':232,'wager':233,'research':234,'positive':235,'success':236,'pelican':237,\n",
    "          'computer':238,'plum':239,'although':240,'brazil':241,'fly':242,'brochure':243,'flood':244,'perch':245,'year':246,'lilac':247,'silk':248,\n",
    "          'disaster':249,'bloom':250,'politician':251,'onion':252,'jazz':253,'peat':254,'flinch':255,'weatherphenomenon':256,'natural':257,\n",
    "          'flavoring':258,'food':259,'furniture':260,'formation':261,'papya':262,'snowstorm':263,'basketball':264, 'jeans':265,'park':266,'cactus':267,\n",
    "          'window':268,'bankteller':269,'keys':270,'raven':271,'purple':272,'cat':273,'candy':274,'mom':275,'dad':276,'brother':277,'sister':278,'chicken':279,'bread':280,\n",
    "          'science':281,'english':282,'psy':283,'biology':284,'canopy':285,'run':286,'fast':287,'school':288,'waitress':289,'cow':290,\n",
    "          'turmeric':291,'cinnamone':292,'perple':293,'fear':294,'male':295,'pleasant':296,'nicotine':297,'sparrow':298,'6312414461hmu':299,\n",
    "          'cardigan':300,'bank':301,'clementine':302,'house':303,'squash':304,'basket':305,'cilantro':306,'owl':307,'cinemmon':308,'saffron':309,\n",
    "          'erect':310,'exit':311,'egress':312,'laptop':314,'clemintine':315,'home':316,'ught':317,'blackberry':318, 'seagull':319, 'bear':320,\n",
    "          'bass':321,'god':322,'love':323,'mercy':324,'feel':325,'nephew':326,'sun':327,'jesus':328,'man':329,'women':330,'men':331,'university':332,\n",
    "          'take':333,'get':334,'have':335,'go':336,'meet':337,'someone':338,'somebody':339,'piece':340,'female':341,'song':342,'hiphop':343,'hoody':344,\n",
    "          'but':345,'what':346,'which':347,'when':348,'who':349,'whose':350,'how':351,'mean':352,'iq':353,'friend':354,'boy':355,'girl':356,'all':357,\n",
    "          'guest':358,'same':359,'silmilar':360,'hope':361,'peace':362,'forgive':363,'beat':364,'hello':365,'hi':366,'nice':367,'good':368,'carrot':369,\n",
    "          'banana':370,'strawberry':371,'cranberry':372,'car':373,'truck':374,'sneak':375,'monkey':376,'monster':377,'dark':378,'cyber':379,'space':380,\n",
    "          'teach':381,'learn':382,'me':383,'you':384,'i':385,'puppy':386,'mermaid':387,'bye':388,'se':389,'see':390,'next':391,'goes':392,'breeze':393,\n",
    "          'deer':394,'crayon':395,'mustered':396,'waves':397,'suit':398,'bluejay':399,'pieceoffurniture':400,'typeofinstrument':401,'gout':402,\n",
    "          'box':403,'mouse':404,'rat':405,'whale':406,'week':407,'minute':408,'appointment':409,'schedule':410,'leaf':411,'news':412,'sulfur':413,\n",
    "          'wardrobe':414,'halibit':415,'polyester':416,'choir':417,'octopus':418,'snake':419,'jupiter':420,'afruit':421,'ajoborprofession':422,\n",
    "          'aweathercondition':423,'abird':424,'aflower':425,'apieceoffurniture':426,'afish':427,'ageographicalformation':428,'amusicalinstrument':429,\n",
    "          'nan':430,'nen':431,'ton':432, 'shelf':433,'apply':434,'glove':435,'cabarnet':436,'floud':437,'happy':438,'guinea':439,'torso':440,'parsley':441,\n",
    "          'ran':442,'valor':443,'honor':444,'courage':445,'homework':446,'work':447,'money':448,'envelope':449,'life':450,'rail':451,'office':452,'oregon':453,\n",
    "          'carnage':454, 'birch':455, 'pine':456}\n",
    "            # changed 'haliberd':313 as intrusion to correction\n",
    "df.number = [translator[item] for item in df.number] \n",
    "\n",
    "df.to_csv('2020-12-17_Similarity_Exp3_AllWords_Clean1.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      SN  biased  order  collaboration collaborator      word  correct  \\\n",
      "0      1       0      1              0            0      tree        1   \n",
      "1      1       0      1              0            0     trout        1   \n",
      "2      1       0      1              0            0     ocean        1   \n",
      "3      1       0      1              0            0  mountain        1   \n",
      "4      1       0      1              0            0     green        1   \n",
      "...   ..     ...    ...            ...          ...       ...      ...   \n",
      "2275  93       1      0              0            0     apple        1   \n",
      "2276  93       1      0              0            0      rain        1   \n",
      "2278  93       1      0              0            0    helium        1   \n",
      "2279  93       1      0              0            0   calcium        1   \n",
      "2280  93       1      0              0            0    cotton        1   \n",
      "\n",
      "      buffer  phase  number  \n",
      "0          0      3      37  \n",
      "1          0      3      13  \n",
      "2          0      3      38  \n",
      "3          0      3      40  \n",
      "4          1      3     166  \n",
      "...      ...    ...     ...  \n",
      "2275       0      3      27  \n",
      "2276       0      3      64  \n",
      "2278       0      3       8  \n",
      "2279       0      3      10  \n",
      "2280       0      3      49  \n",
      "\n",
      "[2187 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------DESCRIPTION-----------------------------------------\n",
    "# This is for recall 3, adding the dlm files\n",
    "\n",
    "\n",
    "#-------------------------------------Input----------------------------------------------\n",
    "studyList1 = ['crow','eagle','finch','parrot','pigeon','cardinal','nitrogen','helium','chlorine','calcium','oxygen','mercury','trout',\\\n",
    "              'flounder','halibut','guppy','piranha','shark','carnation','orchid','pansy','daffodil','violet','rose','nectarine','pear',\\\n",
    "              'apple','grape','raspberry','cherry','tuba','drum','trumpet','saxophone','piano','organ','tree','ocean','canyon','mountain',\\\n",
    "              'plateau','cave','cinnamon','mustard','basil','oregano','paprika','cotton','wool', 'velvet','linen','leather','flyer',\\\n",
    "              'newspaper','comic','essay','pamphlet','tornado','hail','blizzard','rain','drought','jacket','dress','blouse','underwear',\\\n",
    "              'shoes','lamp','desk','bookcase','dresser','chair','banker','dentist','secretary','engineer','nurse','hour', 'arms', 'green',\\\n",
    "              'uncle']\n",
    "    \n",
    "studyList2 = ['crow','eagle','finch','parrot','pigeon','nitrogen','helium','chlorine','calcium','oxygen','trout','flounder','halibut',\\\n",
    "              'guppy','piranha','carnation','orchid','pansy','daffodil','violet','nectarine','pear','apple','grape','raspberry','tuba',\\\n",
    "              'drum','trumpet','saxophone','piano','tree','ocean','canyon','mountain','plateau','cinnamon','mustard','basil','oregano',\\\n",
    "              'paprika','salt','cotton','wool','velvet','linen','leather','denim','flyer','newspaper','comic','essay','pamphlet','book',\\\n",
    "              'tornado','hail','blizzard','rain','drought','lightning','jacket','dress','blouse','underwear','shoes','shirt','lamp','desk',\\\n",
    "              'bookcase','dresser','chair','recliner','banker','dentist','secretary','engineer','nurse','doctor','hour', 'arms', 'green', 'uncle']    \n",
    "    \n",
    "#-------------------------------------Create Data Frame----------------------------------------------\n",
    "\n",
    "df_dlm = pd.DataFrame(columns=['SN', 'biased','order', 'collaboration', 'collaborator', 'word', 'correct', 'buffer', 'phase'])\n",
    "\n",
    "#df_dlm = pd.read_table('retrieve_27_0_2.dlm', sep='\\t', names=[\"time-del\", \"time\", \"word-del\", \"word\"])\n",
    "\n",
    "first = 1\n",
    "last = 93\n",
    "skip = [13, 14, 20, 21, 25, 27, 44, 45, 63, 65] \n",
    "# according to the \"non-usable\"-folder these participants were allocated to wrong [\"because there were some participant condition \\\n",
    "# or subject numbers that were entered incorrectly by the RA. This then made the data unusable because the scripts use these values to make sure that participants in the \\\n",
    "# collaboration condition are assigned properly.\"]\n",
    "# 1_0_3 is missing pkl file, only has dlm\n",
    "\n",
    "\n",
    "\n",
    "# A: Any subjects we should exclude?\n",
    "for subjectNumber in range(first, last+1):\n",
    "\n",
    "    if subjectNumber in skip:\n",
    "        continue\n",
    "\n",
    "       \n",
    "    # A: Import all words from all participants for recall phase 3\n",
    "    try:\n",
    "        exp_data = pd.read_table('retrieve_'+str(subjectNumber)+'_0_3.dlm', sep='\\t', names=[\"time-del\", \"time\", \"word-del\", \"word\"])\n",
    "        exp_data = exp_data.drop(['time-del', 'time', 'word-del'], axis=1)\n",
    "        exp_data = exp_data.drop([0])\n",
    "        words = exp_data['word'].to_list()\n",
    "        #type(word)\n",
    "        \n",
    "        #word2 = \" \".join([str(i) for i in word])\n",
    "\n",
    "        #print(word2)\n",
    "\n",
    "        for i in words:\n",
    "            word = str(i)\n",
    "            biased = 0\n",
    "            order = 0\n",
    "            collaboration = 0\n",
    "            phase = 3\n",
    "            correct = 0\n",
    "            buffer = 0\n",
    "            \n",
    "            # From experiment 1\n",
    "            word = word.replace(' ', '')\n",
    "            word = word.replace('\\\\', '')\n",
    "            word = word.lower()\n",
    "            word = re.sub(r'\\bcanyoan\\b', 'canyon', word)\n",
    "            word = re.sub(r'\\bcanyons\\b', 'canyon', word)\n",
    "            word = re.sub(r'\\bcherries\\b', 'cherry', word)\n",
    "            word = re.sub(r'\\bclorine\\b', 'chlorine', word)\n",
    "            word = re.sub(r'\\bcinammon\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinnaman\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinnimon\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinomman\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcotten\\b', 'cotton', word)\n",
    "            word = re.sub(r'\\bdaffidil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdafodill\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphodile\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdoffodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdenin\\b', 'denim', word)\n",
    "            word = re.sub(r'\\bdrums\\b', 'drum', word)\n",
    "            word = re.sub(r'\\benginner\\b', 'engineer', word)\n",
    "            word = re.sub(r'\\bflunder\\b', 'flounder', word)\n",
    "            word = re.sub(r'\\bflier\\b', 'flyer', word)\n",
    "            word = re.sub(r'\\bflyers\\b', 'flyer', word)\n",
    "            word = re.sub(r'\\bgrapes\\b', 'grape', word)\n",
    "            word = re.sub(r'\\bgupppy\\b', 'guppy', word)\n",
    "            word = re.sub(r'\\bhailbut\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhalibet\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\blinens\\b', 'linen', word)\n",
    "            word = re.sub(r'\\blinnen\\b', 'linen', word)\n",
    "            word = re.sub(r'\\bercury\\b', 'mercury', word)\n",
    "            word = re.sub(r'\\bmountains\\b', 'mountain', word)\n",
    "            word = re.sub(r'\\bmoutain\\b', 'mountain', word)\n",
    "            word = re.sub(r'\\bnecratine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnecterine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\borchad\\b', 'orchid', word)\n",
    "            word = re.sub(r'\\baregano\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\borageno\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\bregano\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\bamphlet\\b', 'pamphlet', word)\n",
    "            word = re.sub(r'\\bpamplet\\b', 'pamphlet', word)\n",
    "            word = re.sub(r'\\bpanplet\\b', 'pamphlet', word)\n",
    "            word = re.sub(r'\\bphamlet\\b', 'pamphlet', word)\n",
    "            word = re.sub(r'\\bansy\\b', 'pansy', word)\n",
    "            word = re.sub(r'\\bpaparika\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bpapprika\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bpaprica\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bpeprica\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bpegion\\b', 'pigeon', word)\n",
    "            word = re.sub(r'\\bpidgeon\\b', 'pigeon', word)\n",
    "            word = re.sub(r'\\bpidgeons\\b', 'pigeon', word)\n",
    "            word = re.sub(r'\\bpidgieon\\b', 'pigeon', word)\n",
    "            word = re.sub(r'\\bparanha\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirahana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirahna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirahnna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirannah\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirannha\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirhana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirranha\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bplateu\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bplatue\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\brasberries\\b', 'raspberry', word)\n",
    "            word = re.sub(r'\\brasberry\\b', 'raspberry', word)\n",
    "            word = re.sub(r'\\braspberries\\b', 'raspberry', word)\n",
    "            word = re.sub(r'\\brasphberry\\b', 'raspberry', word)\n",
    "            word = re.sub(r'\\bsaxaphone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsaxephone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsaxiphone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsecratary\\b', 'secretary', word)\n",
    "            word = re.sub(r'\\bsecrertary\\b', 'secretary', word)\n",
    "            word = re.sub(r'\\bshoe\\b', 'shoes', word)\n",
    "            word = re.sub(r'\\btrumphet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\btumpet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\bnderwear\\b', 'underwear', word)\n",
    "            word = re.sub(r'\\budnerwear\\b', 'underwear', word)\n",
    "            word = re.sub(r'\\bbookshelf\\b', 'bookcase', word)\n",
    "            word = re.sub(r'\\bchari\\b', 'chair', word)\n",
    "            word = re.sub(r'\\blightening\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blightenings\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blighting\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blightning\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blightnening\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\btress\\b', 'tree', word)\n",
    "            word = re.sub(r'\\bvoilet\\b', 'violet', word)\n",
    "            word = re.sub(r'\\bblizarrd\\b', 'blizzard', word)\n",
    "            word = re.sub(r'\\bvelvey\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bdafodil\\b', 'daffodil', word)            \n",
    "            word = re.sub(r'\\bhalibit\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bbooks\\b', 'book', word)\n",
    "            word = re.sub(r'\\bhurricaine\\b', 'hurricane', word)          \n",
    "            \n",
    "            #From experiment 2\n",
    "            #word = re.sub(r'\\bado\\b', '', word)\n",
    "            word = re.sub(r'\\bdandalione\\b', 'dandelion', word)\n",
    "            word = re.sub(r'\\bdandeline\\b', 'dandelion', word)\n",
    "            word = re.sub(r'\\bdandelion\\b', 'dandelion', word)\n",
    "            word = re.sub(r'\\bdandelion\\b', 'dandelion', word)\n",
    "            word = re.sub(r'\\bdandalione\\b', 'dandelion', word)\n",
    "            word = re.sub(r'\\bdandilion\\b', 'dandelion', word)\n",
    "            #word = re.sub(r'\\bflinch\\b', 'finch', word)\n",
    "            word = re.sub(r'\\bgano\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\bhalbait\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhale\\b', 'hail', word)\n",
    "            word = re.sub(r'\\blenin\\b', 'linen', word)\n",
    "            word = re.sub(r'\\blvet\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bntain\\b', 'mountain', word)\n",
    "            word = re.sub(r'\\bnurde\\b', 'nurse', word)\n",
    "            word = re.sub(r'\\bado\\b', '', word)\n",
    "            word = re.sub(r'\\bouse\\b', 'blouse', word)\n",
    "            word = re.sub(r'\\bpant\\b', 'pants', word)\n",
    "            word = re.sub(r'\\bparhna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpeat\\b', 'pear', word)\n",
    "            word = re.sub(r'\\bpregeno\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\bsser\\b', 'dresser', word)\n",
    "            word = re.sub(r'\\btton\\b', 'cotton', word)\n",
    "            word = re.sub(r'\\bbrasil\\b', 'basil', word)\n",
    "            word = re.sub(r'\\btangarine\\b', 'tangerine', word)\n",
    "            word = re.sub(r'\\bclothes\\b', 'cloth', word)\n",
    "            word = re.sub(r'\\bclothing\\b', 'cloth', word)\n",
    "            word = re.sub(r'\\bclouds\\b', 'cloud', word)\n",
    "            word = re.sub(r'\\bbooks\\b', 'book', word)\n",
    "            word = re.sub(r'\\blilly\\b', 'lily', word)\n",
    "            word = re.sub(r'\\boragano\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\btronado\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\bapll\\b', 'apple', word)\n",
    "            word = re.sub(r'\\bcomics\\b', 'comic', word)\n",
    "            word = re.sub(r'\\bfodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bhabut\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\blighnting\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blightining\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blightning\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\borgans\\b', 'organ', word)\n",
    "            word = re.sub(r'\\bpiogen\\b', 'pigeon', word)\n",
    "            word = re.sub(r'\\bpipraki\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bpurrana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\brought\\b', 'drought', word)\n",
    "            word = re.sub(r'\\btrumphent\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\btrumpht\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\btrumpt\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\bbookself\\b', 'bookcase', word)\n",
    "            word = re.sub(r'\\bbookshlelf\\b', 'bookcase', word)\n",
    "            word = re.sub(r'\\bboookcase\\b', 'bookcase', word)\n",
    "            word = re.sub(r'\\bchloride\\b', 'chlorine', word)\n",
    "            word = re.sub(r'\\bchorline\\b', 'chlorine', word)\n",
    "            word = re.sub(r'\\bcholrine\\b', 'chlorine', word)\n",
    "            word = re.sub(r'\\bchroine\\b', 'chlorine', word)\n",
    "            word = re.sub(r'\\bcinamon\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinnimin\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinnoman\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinnomin\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcoten\\b', 'cotton', word)\n",
    "            word = re.sub(r'\\bhalibat\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhalibiut\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhallibut\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhalubit\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bheliu,\\b', 'helium', word)\n",
    "            word = re.sub(r'\\belium\\b', 'helium', word)\n",
    "            word = re.sub(r'\\bheliu\\b', 'helium', word)\n",
    "            word = re.sub(r'\\bhours\\b', 'hour', word)\n",
    "            word = re.sub(r'\\bprika\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\broses\\b', 'rose', word)\n",
    "            word = re.sub(r'\\baregeno\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\bblizzzard\\b', 'blizzard', word)\n",
    "            word = re.sub(r'\\bblousse\\b', 'blouse', word)\n",
    "            word = re.sub(r'\\bdafadil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaffadil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaffildil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaffodils\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaffoldil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdafidil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdafidill\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdafidill\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphadil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphadile\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphadile\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphadill\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphidile\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdapphodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdentis\\b', 'dentist', word)\n",
    "            word = re.sub(r'\\bdreser\\b', 'dresser', word)\n",
    "            word = re.sub(r'\\bdresses\\b', 'dress', word)\n",
    "            word = re.sub(r'\\bdrout\\b', 'drought', word)\n",
    "            word = re.sub(r'\\bdum\\b', 'drum', word)\n",
    "            word = re.sub(r'\\bengeneer\\b', 'engineer', word)\n",
    "            word = re.sub(r'\\benigineer\\b', 'engineer', word)\n",
    "            word = re.sub(r'\\bhelim\\b', 'helium', word)\n",
    "            word = re.sub(r'\\blinene\\b', 'linen', word)\n",
    "            word = re.sub(r'\\blamo\\b', 'lamp', word)\n",
    "            word = re.sub(r'\\bmountian\\b', 'mountain', word)\n",
    "            word = re.sub(r'\\bnecatrina\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnecatrine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnectarine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnectatine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnectorin\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnectorine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnectrine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnecturine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnictarine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnigrogen\\b', 'nitrogen', word)\n",
    "            word = re.sub(r'\\bnitrogren\\b', 'nitrogen', word)\n",
    "            word = re.sub(r'\\bocygen\\b', 'oxygen', word)\n",
    "            word = re.sub(r'\\borchird\\b', 'orchid', word)\n",
    "            word = re.sub(r'\\boreganno\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\boregeno\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\boregno\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\boregno\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\boregono\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\boxogen\\b', 'oxygen', word)\n",
    "            word = re.sub(r'\\bpapricka\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bparot\\b', 'parrot', word)\n",
    "            word = re.sub(r'\\bpeperika\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bphamplet\\b', 'pamphlet', word)\n",
    "            word = re.sub(r'\\bphirana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpiranah\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpiranna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirhanna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirnaha\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirrhana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bplaeatu\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bplataue\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bplateua\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bplateux\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bplaute\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bpleatu\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bpleteau\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\brasperry\\b', 'raspberry', word)\n",
    "            word = re.sub(r'\\brumpet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\bsactatary\\b', 'secretary', word)\n",
    "            word = re.sub(r'\\bsaxphone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\btorando\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\btornadeo\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\btornadoe\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\btornadoe\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\btornador\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\btornando\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\btornao\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\btrees\\b', 'tree', word)\n",
    "            word = re.sub(r'\\btrmpet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\btrmpet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\btrupmet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\bundewear\\b', 'underwear', word)\n",
    "            word = re.sub(r'\\bvelvot\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bvevelt\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bvolvet\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bxophone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bxygen\\b', 'oxygen', word)\n",
    "            word = re.sub(r'\\bdafodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bhalibit\\b', 'halibut', word)\n",
    "\n",
    "            # From experiment 3\n",
    "            word = re.sub(r'\\beather\\b', 'leather', word)\n",
    "            word = re.sub(r'\\bappl\\b', 'apple', word)\n",
    "            word = re.sub(r'\\barangano\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\barm\\b', 'arms', word)\n",
    "            word = re.sub(r'\\barregano\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\baxophpne\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bbizzard\\b', 'blizzard', word)\n",
    "            word = re.sub(r'\\bblizard\\b', 'blizzard', word)\n",
    "            #word = re.sub(r'\\bBlouse\\b', 'blouse', word)\n",
    "            word = re.sub(r'\\bblouser\\b', 'blouse', word)\n",
    "            word = re.sub(r'\\bbookshelf\\b', 'bookcase', word)\n",
    "            word = re.sub(r'\\bboooks\\b', 'book', word)\n",
    "            #word = re.sub(r'\\bCardinal\\b', 'cardinal', word)\n",
    "            word = re.sub(r'\\bcartigan\\b', 'cardigan', word)\n",
    "            word = re.sub(r'\\bchloride\\b', 'chlorine', word)\n",
    "            #word = re.sub(r'\\bComic\\b', 'comic', word)\n",
    "            word = re.sub(r'\\bcutton\\b', 'cotton', word)\n",
    "            word = re.sub(r'\\bdaffadil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaffildol\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaffodile\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaffodils\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdafidil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdafiodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdandelion\\b', 'dandelion', word)\n",
    "            word = re.sub(r'\\bdanelion\\b', 'dandelion', word)\n",
    "            word = re.sub(r'\\bdaphodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphodyl\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdoctore\\b', 'doctor', word)\n",
    "            #word = re.sub(r'\\bDress\\b', 'dress', word)\n",
    "            word = re.sub(r'\\bdresses\\b', 'dress', word)\n",
    "            word = re.sub(r'\\bdressor\\b', 'dresser', word)\n",
    "            word = re.sub(r'\\bdrough\\b', 'drought', word)\n",
    "            word = re.sub(r'\\bdrums\\b', 'drum', word)\n",
    "            #word = re.sub(r'\\bEagle\\b', 'eagle', word)\n",
    "            word = re.sub(r'\\begale\\b', 'eagle', word)\n",
    "            word = re.sub(r'\\beingineer\\b', 'engineer', word)\n",
    "            word = re.sub(r'\\bfinc\\b', 'finch', word)\n",
    "            word = re.sub(r'\\bFinch\\b', 'finch', word)\n",
    "            word = re.sub(r'\\bgraoe\\b', 'grape', word)\n",
    "            word = re.sub(r'\\bgrap\\b', 'grape', word)\n",
    "            #word = re.sub(r'\\bGrape\\b', 'grape', word)\n",
    "            word = re.sub(r'\\bgrapes\\b', 'grape', word)\n",
    "            #word = re.sub(r'\\bGreen\\b', 'green', word)\n",
    "            word = re.sub(r'\\bgruppy\\b', 'guppy', word)\n",
    "            word = re.sub(r'\\bgubby\\b', 'guppy', word)\n",
    "            #word = re.sub(r'\\bGuppy\\b', 'guppy', word)\n",
    "            word = re.sub(r'\\bhailbut\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhalbit\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhalibat\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhelium\\'\\b', 'helium', word)\n",
    "            word = re.sub(r'\\bheluim\\b', 'helium', word)\n",
    "            word = re.sub(r'\\bhitrogen\\b', 'nitrogen', word)\n",
    "            #word = re.sub(r'\\bHour\\b', 'hour', word)\n",
    "            word = re.sub(r'\\binen\\b', 'linen', word)\n",
    "            word = re.sub(r'\\bjean\\b', 'jeans', word)\n",
    "            word = re.sub(r'\\bJesus\\b', 'jesus', word)\n",
    "            word = re.sub(r'\\bjupitor\\b', 'jupiter', word)\n",
    "            word = re.sub(r'\\blether\\b', 'leather', word)\n",
    "            word = re.sub(r'\\blightining\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blightning\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blinnen\\b', 'linen', word)\n",
    "            word = re.sub(r'\\bmountains\\b', 'mountain', word)\n",
    "            word = re.sub(r'\\bmpet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\bnectorine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnectraine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnitrgoen\\b', 'nitrogen', word)\n",
    "            #word = re.sub(r'\\bNitrogen\\b', 'nitrogen', word)\n",
    "            word = re.sub(r'\\bnitrogen\\'\\b', 'nitrogen', word)\n",
    "            word = re.sub(r'\\bOcean\\b', 'ocean', word)\n",
    "            word = re.sub(r'\\bochid\\b', 'orchid', word)\n",
    "            word = re.sub(r'\\boctupus\\b', 'octopus', word)\n",
    "            word = re.sub(r'\\borchids\\b', 'orchid', word)\n",
    "            word = re.sub(r'\\borchird\\b', 'orchid', word)\n",
    "            word = re.sub(r'\\borcid\\b', 'orchid', word)\n",
    "            #word = re.sub(r'\\boregon\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\borgano\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\bOxygen\\b', 'oxygen', word)\n",
    "            word = re.sub(r'\\bpaino\\b', 'piano', word)\n",
    "            word = re.sub(r'\\bpamphelt\\b', 'pamphlet', word)\n",
    "            word = re.sub(r'\\bpamplet\\b', 'pamphlet', word)\n",
    "            word = re.sub(r'\\bpaprikka\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bparahana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bparanha\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpasny\\b', 'pansy', word)\n",
    "            word = re.sub(r'\\bpianp\\b', 'piano', word)\n",
    "            word = re.sub(r'\\bpidgeon\\b', 'pigeon', word)\n",
    "            word = re.sub(r'\\bpiegon\\b', 'pigeon', word)\n",
    "            word = re.sub(r'\\bpihrana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirahna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirahnna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpiranah\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirannhea\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirhana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirhanna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirranha\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirranhea\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirrhana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bplataeu\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bplateu\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bpleasent\\b', 'pleasant', word)\n",
    "            word = re.sub(r'\\bpleatou\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bpoxygen\\b', 'oxygen', word)\n",
    "            word = re.sub(r'\\bprihana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\brasberry\\b', 'raspberry', word)\n",
    "            word = re.sub(r'\\braseberry\\b', 'raspberry', word)\n",
    "            word = re.sub(r'\\brobbin\\b', 'robin', word)\n",
    "            word = re.sub(r'\\bsaxaphone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsaxapphone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsaxohpone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bSaxophone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsaxopon\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsaxphone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsecatary\\b', 'secretary', word)\n",
    "            word = re.sub(r'\\bSecretary\\b', 'secretary', word)\n",
    "            word = re.sub(r'\\bshoe\\b', 'shoes', word)\n",
    "            word = re.sub(r'\\bsulfure\\b', 'sulfur', word)\n",
    "            word = re.sub(r'\\btangerene\\b', 'tangerine', word)\n",
    "            word = re.sub(r'\\bTangerine\\b', 'tangerine', word)\n",
    "            word = re.sub(r'\\btornadeo\\b', 'tornado', word)\n",
    "            #word = re.sub(r'\\bTree\\b', 'tree', word)\n",
    "            word = re.sub(r'\\btrees\\b', 'tree', word)\n",
    "            word = re.sub(r'\\btrought\\b', 'drought', word)\n",
    "            word = re.sub(r'\\btrumbone\\b', 'trombone', word)\n",
    "            word = re.sub(r'\\bTrumpet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\btrumphet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\btumeric\\b', 'turmeric', word)\n",
    "            word = re.sub(r'\\bumpet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\bVelvet\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bvelvet3\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bvelviot\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bViolet\\b', 'violet', word)\n",
    "            word = re.sub(r'\\bwoman\\b', 'women', word)\n",
    "            word = re.sub(r'\\bwool\\\\b', 'wool', word)\n",
    "            word = re.sub(r'\\bxophone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bcinammon\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinamon\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinemmon\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinnamone\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinnemon\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bchior\\b', 'choir', word)\n",
    "            word = re.sub(r'\\bhaliberd\\b', 'halibut', word)\n",
    "            \n",
    "            #Buffer\n",
    "            word = re.sub(r'\\barm\\b', 'arms', word)\n",
    "            word = re.sub(r'\\bgrreen\\b', 'green', word)\n",
    "\n",
    "            #Experiment 3, recall 2\n",
    "            word = re.sub(r'\\bdree\\b', 'tree', word)\n",
    "            word = re.sub(r'\\bpiranhea\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\boxigen\\b', 'oxygen', word)\n",
    "            word = re.sub(r'\\bogran\\b', 'organ', word)\n",
    "            word = re.sub(r'\\bchaor\\b', 'chair', word)\n",
    "            word = re.sub(r'\\brain\\'\\b', 'rain', word)\n",
    "            word = re.sub(r'\\bblosue\\b', 'blouse', word)\n",
    "            word = re.sub(r'\\boxegen\\b', 'oxygen', word)\n",
    "            word = re.sub(r'\\bparrot\\\\b', 'parrot', word)\n",
    "            word = re.sub(r'\\benigneer\\b', 'engineer', word)\n",
    "            word = re.sub(r'\\bhalabit\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bsecetary\\b', 'secretary', word)\n",
    "            word = re.sub(r'\\bdemtist\\b', 'dentist', word)\n",
    "            word = re.sub(r'\\bdenist\\b', 'dentist', word)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            # Bias comes from file name\n",
    "            if os.path.exists('new_encoding_' + str(subjectNumber) +'_0.pkl'):\n",
    "                biased = 0\n",
    "            else:\n",
    "                biased = 1\n",
    "\n",
    "            # Order comes from SN and impacts correct study list reference\n",
    "            if (subjectNumber%2 == 0) and (biased == 1):\n",
    "                order = 1\n",
    "                if word in studyList2:\n",
    "                    correct = 1\n",
    "                else:\n",
    "                    correct = 0\n",
    "            elif (subjectNumber%2 == 1) and (biased == 1):\n",
    "                order = 0     \n",
    "                if word in studyList1:\n",
    "                    correct = 1\n",
    "                else:\n",
    "                    correct = 0\n",
    "            if (subjectNumber%2 == 0) and (biased == 0):\n",
    "                order = 0\n",
    "                if word in studyList1:\n",
    "                    correct = 1\n",
    "                else:\n",
    "                    correct = 0\n",
    "            elif (subjectNumber%2 == 1) and (biased == 0):\n",
    "                order = 1     \n",
    "                if word in studyList2:\n",
    "                    correct = 1\n",
    "                else:\n",
    "                    correct = 0\n",
    "\n",
    "            # Indicate buffer\n",
    "            buffer = 0\n",
    "\n",
    "            if i in ['arms', 'green', 'uncle', 'hour']:\n",
    "                buffer = 1\n",
    "            else:\n",
    "                buffer = 0\n",
    "\n",
    "            # Collaboration comes from retrieval_1_2 files (1_2 collaborated, 0_2 did not)\n",
    "            if os.path.exists('new_retrieve_' + str(subjectNumber) +'_0_2.pkl'):\n",
    "                collaboration = 0\n",
    "            else:\n",
    "                collaboration = 1\n",
    "\n",
    "            #Problem: This is only the words from the 3. recall. And the column with who they collaborated\n",
    "            collaborator = 0\n",
    "            if os.path.exists('new_retrieve_' + str(subjectNumber) + str(subjectNumber+1) + '_1_2.pkl'):\n",
    "                collaborator = str(subjectNumber+1)\n",
    "            elif os.path.exists('new_retrieve_' + str(subjectNumber-1) + str(subjectNumber) + '_1_2.pkl'):\n",
    "                collaborator = str(subjectNumber-1)\n",
    "            else:\n",
    "                collaborator = 0         \n",
    "            \n",
    "            trialDict = {'SN':subjectNumber, 'biased':biased, 'order':order, 'phase':phase, 'collaboration':collaboration, 'collaborator':collaborator, \n",
    "                     'word':word, 'correct':correct, 'buffer':buffer}   \n",
    "            df_dlm = df_dlm.append(trialDict, ignore_index=True)\n",
    "            \n",
    "    except EOFError:\n",
    "        pass\n",
    "\n",
    "df_dlm = df_dlm.drop_duplicates(['SN','word', 'phase']) #Delete duplicates\n",
    "df_dlm = df_dlm.replace('', np.nan)\n",
    "df_dlm = df_dlm.dropna(axis=0, how='any', subset=['word'])\n",
    "\n",
    "\n",
    "\n",
    "df_dlm.to_csv('2020-12-17_Similarity_Exp3_AllWords_Clean2.csv', index=False)\n",
    "df_dlm['number'] = df_dlm['word']\n",
    "# Includes misspellings and buffer words, same as in experiment 1 plus new intrusions and buffer\n",
    "\n",
    "translator = {'crow':1,'eagle':2,'finch':3,'parrot':4,'pigeon':5,'cardinal':6,'nitrogen':7,'helium':8,'chlorine':9,'calcium':10,'oxygen':11,\n",
    "          'mercury':12,'trout':13,'flounder':14,'halibut':15,'guppy':16,'piranha':17,'shark':18,'carnation':19,'orchid':20,'pansy':21,'daffodil':22,'violet':23,'rose':24,\n",
    "          'nectarine':25,'pear':26,'apple':27,'grape':28,'raspberry':29,'cherry':30,'tuba':31,'drum':32,'trumpet':33,'saxophone':34,'piano':35,'organ':36,'tree':37,'ocean':38,\n",
    "          'canyon':39,'mountain':40,'plateau':41,'cave':42,'cinnamon':43,'mustard':44,'basil':45,'oregano':46,'paprika':47,'salt':48,'cotton':49,'wool':50,'velvet':51,\n",
    "          'linen':52,'leather':53,'denim':54,'flyer':55,'newspaper':56,'comic':57,'essay':58,'pamphlet':59,'book':60,'tornado':61,'hail':62,'blizzard':63,'rain':64,'drought':65,\n",
    "          'lightning':66,'jacket':67,'dress':68,'blouse':69,'underwear':70,'shoes':71,'shirt':72,'lamp':73,'desk':74,'bookcase':75,'dresser':76,'chair':77,'recliner':78,\n",
    "          'banker':79,'dentist':80,'secretary':81,'engineer':82,'nurse':83,'doctor':84,'a':85,'architect':86,'avidafil?':87,'ballet':88,'baseball':89,'bed':90,\n",
    "          'bird':91,'blueberry':92,'cage':93,'canary':94,'carbon':95,'chemical':96,'clarinet':97,'coat':98,'concrete':99,'daisy':100,'dog':101,'experimenter':102,\n",
    "          'fish':103,'flamenco':104,'flower':105,'flute':106,'fruit':107,'give':108,'grass':109,'hydrogen':110,'instrument':111,'lawyer':112,'library':113,'lily':114,\n",
    "          'lithium':115,'lung':116,'melon':117,'music':118,'nickel':119,'nylon':120,'pants':121,'parsley':122,'pepper':123,'project':124,'red':125,'salmon':126,\n",
    "          'satin':127,'sea':128,'sky':129,'slave':130,'sofa':131,'storm':132,'table':133,'tango':134,'teacher':135,'the':136,'thunder':137,'thunderstorm':138,'tissue':139,\n",
    "          'to':140,'trombone':141,'trousers':142,'tulip':143,'tuna':144,'willing':145,'address':146,'cavern':147,'homework':148,'magazine':149,'are':150,\n",
    "          'dove':151,'sodium':152,'hurricaine':153,'hurricane':154,'violent':155,'violin':156,'green':157,'uncle':158,'hour':159,'arms':160,'orchard':161,\n",
    "          'paper':162,'sunflower':163,'dandelion':164,'snow':165,'green':166,'professor':167,'socks':168,'purse':169,'ar':170,'e':171,'mium':172,\n",
    "          't':173,'rk':174, 'hid':175,'breakfast':176,'uncle':177,'hat':178,'hale':179,'speacker':180,'dance':181,'baker':182,'couch':183,'tube':184,\n",
    "          'valley':185,'tangerine':186,'guitar':187,'orange':188,'skirt':189,'robin':190,'peach':191,'lavender':192,'thyme':193,'drawer':194,\n",
    "          'cloud':195,'word':196,'paper':197,'carnelian':198,'article':199,'daydream':200,'tectonicplate':201,'blossom':202,'cloth':203,'tsunami':204,\n",
    "          'legs':205,'keyboard':206,'fiction':207,'water':208,'earth':209,'light':210,'hair':211,'rocks':212,'moon':213,'weather':214,'wind':215,\n",
    "          'door':216,'letter':217,'time':218,'yellow':219,'river':220,'lake':221,'typhoon':222,'gloves':223,'magnesium':224,'ear':225,'camp':226,\n",
    "          'seed':227,'phone':228,'fresh':229,'lenin':230,'in':231,'halogen':232,'wager':233,'research':234,'positive':235,'success':236,'pelican':237,\n",
    "          'computer':238,'plum':239,'although':240,'brazil':241,'fly':242,'brochure':243,'flood':244,'perch':245,'year':246,'lilac':247,'silk':248,\n",
    "          'disaster':249,'bloom':250,'politician':251,'onion':252,'jazz':253,'peat':254,'flinch':255,'weatherphenomenon':256,'natural':257,\n",
    "          'flavoring':258,'food':259,'furniture':260,'formation':261,'papya':262,'snowstorm':263,'basketball':264, 'jeans':265,'park':266,'cactus':267,\n",
    "          'window':268,'bankteller':269,'keys':270,'raven':271,'purple':272,'cat':273,'candy':274,'mom':275,'dad':276,'brother':277,'sister':278,'chicken':279,'bread':280,\n",
    "          'science':281,'english':282,'psy':283,'biology':284,'canopy':285,'run':286,'fast':287,'school':288,'waitress':289,'cow':290,\n",
    "          'turmeric':291,'cinnamone':292,'perple':293,'fear':294,'male':295,'pleasant':296,'nicotine':297,'sparrow':298,'6312414461hmu':299,\n",
    "          'cardigan':300,'bank':301,'clementine':302,'house':303,'squash':304,'basket':305,'cilantro':306,'owl':307,'cinemmon':308,'saffron':309,\n",
    "          'erect':310,'exit':311,'egress':312,'laptop':314,'clemintine':315,'home':316,'ught':317,'blackberry':318, 'seagull':319, 'bear':320,\n",
    "          'bass':321,'god':322,'love':323,'mercy':324,'feel':325,'nephew':326,'sun':327,'jesus':328,'man':329,'women':330,'men':331,'university':332,\n",
    "          'take':333,'get':334,'have':335,'go':336,'meet':337,'someone':338,'somebody':339,'piece':340,'female':341,'song':342,'hiphop':343,'hoody':344,\n",
    "          'but':345,'what':346,'which':347,'when':348,'who':349,'whose':350,'how':351,'mean':352,'iq':353,'friend':354,'boy':355,'girl':356,'all':357,\n",
    "          'guest':358,'same':359,'silmilar':360,'hope':361,'peace':362,'forgive':363,'beat':364,'hello':365,'hi':366,'nice':367,'good':368,'carrot':369,\n",
    "          'banana':370,'strawberry':371,'cranberry':372,'car':373,'truck':374,'sneak':375,'monkey':376,'monster':377,'dark':378,'cyber':379,'space':380,\n",
    "          'teach':381,'learn':382,'me':383,'you':384,'i':385,'puppy':386,'mermaid':387,'bye':388,'se':389,'see':390,'next':391,'goes':392,'breeze':393,\n",
    "          'deer':394,'crayon':395,'mustered':396,'waves':397,'suit':398,'bluejay':399,'pieceoffurniture':400,'typeofinstrument':401,'gout':402,\n",
    "          'box':403,'mouse':404,'rat':405,'whale':406,'week':407,'minute':408,'appointment':409,'schedule':410,'leaf':411,'news':412,'sulfur':413,\n",
    "          'wardrobe':414,'halibit':415,'polyester':416,'choir':417,'octopus':418,'snake':419,'jupiter':420,'afruit':421,'ajoborprofession':422,\n",
    "          'aweathercondition':423,'abird':424,'aflower':425,'apieceoffurniture':426,'afish':427,'ageographicalformation':428,'amusicalinstrument':429,\n",
    "          'nan':430,'nen':431,'ton':432, 'shelf':433,'apply':434,'glove':435,'cabarnet':436,'floud':437,'happy':438,'guinea':439,'torso':440,'parsley':441,\n",
    "          'ran':442,'valor':443,'honor':444,'courage':445,'homework':446,'work':447,'money':448,'envelope':449,'life':450,'rail':451,'office':452,'oregon':453,\n",
    "          'carnage':454, 'birch':455, 'pine':456}\n",
    "            # Changed 'haliberd':313 from intrusion to correction into halibut\n",
    "            # Carnage, birch added due to intrusion participant from dlm file\n",
    "\n",
    "\n",
    "df_dlm.number = [translator[item] for item in df_dlm.number] \n",
    "                          \n",
    "# Import files with recalls 1 & 2                          \n",
    "df_dlm.to_csv('2020-12-17_Similarity_Exp3_AllWords_Clean2.csv', index=False)\n",
    "print(df_dlm)\n",
    "df = pd.read_csv('2020-12-17_Similarity_Exp3_AllWords_Clean1.csv')                        \n",
    "\n",
    "# Combine recalls (1 & 2) & 3\n",
    "frames = [df, df_dlm]\n",
    "df_both = pd.concat(frames)\n",
    "df_both.to_csv('2020-12-17_Similarity_Exp3_AllWords_Clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION! This needs to be manually adjusted over the experiments to identify the max of permutations, which is specific to the experiment\n",
    "def permu(n):\n",
    "    df_max = pd.read_csv('2020-08-14_Similarity_Availability_AllWords_Clean.csv')\n",
    "    maxi = max(df_max.number) \n",
    "    #print(maxi)\n",
    "    all = list(range(0,maxi))\n",
    "    y = list(itertools.permutations(all, n))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __B4. Similarity for dyads by sub-group__\n",
    "\n",
    "* Attention: These analyses only work if no participant recalled no words (all participants recalled at least 1 word)\n",
    "* The order of str1 and str2 in the code is important, as str1 always needs to correspond to the first SN in the combo and str2 to the second, however, it is completely irrelevant of a specific SN is str1 or str2 as the code usually starts with the longer one anyways, if it makes a difference at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2020-12-17_Similarity_Exp3_AllWords_Clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000= 11\n",
      "011= 11\n",
      "010= 10\n",
      "001= 9\n",
      "100= 9\n",
      "111= 12\n",
      "110= 9\n",
      "101= 12\n"
     ]
    }
   ],
   "source": [
    "# Quick overview about how many participants are in which condition\n",
    "df1 = df.drop_duplicates(subset=['SN'], keep='first')\n",
    "#print(df1)\n",
    "df_000 = len(df1.SN[(df1['collaboration']==0) & (df1['biased']==0) & (df1['order']==0)].tolist())\n",
    "df_011 = len(df1.SN[(df1['collaboration']==0) & (df1['biased']==1) & (df1['order']==1)].tolist())\n",
    "df_010 = len(df1.SN[(df1['collaboration']==0) & (df1['biased']==1) & (df1['order']==0)].tolist())\n",
    "df_001 = len(df1.SN[(df1['collaboration']==0) & (df1['biased']==0) & (df1['order']==1)].tolist())\n",
    "df_100 = len(df1.SN[(df1['collaboration']==1) & (df1['biased']==0) & (df1['order']==0)].tolist())\n",
    "df_111 = len(df1.SN[(df1['collaboration']==1) & (df1['biased']==1) & (df1['order']==1)].tolist())\n",
    "df_110 = len(df1.SN[(df1['collaboration']==1) & (df1['biased']==1) & (df1['order']==0)].tolist())\n",
    "df_101 = len(df1.SN[(df1['collaboration']==1) & (df1['biased']==0) & (df1['order']==1)].tolist())\n",
    "print('000=', df_000)\n",
    "print('011=', df_011)\n",
    "print('010=', df_010)\n",
    "print('001=', df_001)\n",
    "print('100=', df_100)\n",
    "print('111=', df_111)\n",
    "print('110=', df_110)\n",
    "print('101=', df_101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_pP_count_phase1 [   1    2    3    4    5    6    7    8    9   10   11   12   15   16\n",
      "   17   18   19   22   23   24   26   28   29   30   31   32   33   34\n",
      "   35   36   37   38   39   40   41   42   43   46   47   48   49   50\n",
      "   51   52   53   54   55   56   57   58   59   60   61   62   64   66\n",
      "   67   68   69   70   71   72   73   74   75   76   77   78   79   80\n",
      "   81   82   83   84   85   86   87   88   89   90   91   92   93 1112\n",
      " 1718 2223 3031 3435 4041 4849 5253 5657 6061 6970 7374 7778 8182 8384\n",
      " 8586 8788 8990 9192]\n",
      "mean_total biased           20.26506\n",
      "order            20.26506\n",
      "collaboration    20.26506\n",
      "collaborator     20.26506\n",
      "word             20.26506\n",
      "correct          20.26506\n",
      "buffer           20.26506\n",
      "phase            20.26506\n",
      "number           20.26506\n",
      "dtype: float64\n",
      "sd_total biased           9.357512\n",
      "order            9.357512\n",
      "collaboration    9.357512\n",
      "collaborator     9.357512\n",
      "word             9.357512\n",
      "correct          9.357512\n",
      "buffer           9.357512\n",
      "phase            9.357512\n",
      "number           9.357512\n",
      "dtype: float64\n",
      "mean_total biased           26.349398\n",
      "order            26.349398\n",
      "collaboration    26.349398\n",
      "collaborator     26.349398\n",
      "word             26.337349\n",
      "correct          26.349398\n",
      "buffer           26.349398\n",
      "phase            26.349398\n",
      "number           26.349398\n",
      "dtype: float64\n",
      "sd_total biased           13.656441\n",
      "order            13.656441\n",
      "collaboration    13.656441\n",
      "collaborator     13.656441\n",
      "word             13.671474\n",
      "correct          13.656441\n",
      "buffer           13.656441\n",
      "phase            13.656441\n",
      "number           13.656441\n",
      "dtype: float64\n",
      "mean_total biased           29.619048\n",
      "order            29.619048\n",
      "collaboration    29.619048\n",
      "collaborator     29.619048\n",
      "word             29.619048\n",
      "correct          29.619048\n",
      "buffer           29.619048\n",
      "phase            29.619048\n",
      "number           29.619048\n",
      "dtype: float64\n",
      "sd_total biased           15.312919\n",
      "order            15.312919\n",
      "collaboration    15.312919\n",
      "collaborator     15.312919\n",
      "word             15.312919\n",
      "correct          15.312919\n",
      "buffer           15.312919\n",
      "phase            15.312919\n",
      "number           15.312919\n",
      "dtype: float64\n",
      "mean_total biased           23.00000\n",
      "order            23.00000\n",
      "collaboration    23.00000\n",
      "collaborator     23.00000\n",
      "word             22.97561\n",
      "correct          23.00000\n",
      "buffer           23.00000\n",
      "phase            23.00000\n",
      "number           23.00000\n",
      "dtype: float64\n",
      "sd_total biased           10.920165\n",
      "order            10.920165\n",
      "collaboration    10.920165\n",
      "collaborator     10.920165\n",
      "word             10.951000\n",
      "correct          10.920165\n",
      "buffer           10.920165\n",
      "phase            10.920165\n",
      "number           10.920165\n",
      "dtype: float64\n",
      "mean_total biased           20.02439\n",
      "order            20.02439\n",
      "collaboration    20.02439\n",
      "collaborator     20.02439\n",
      "word             20.02439\n",
      "correct          20.02439\n",
      "buffer           20.02439\n",
      "phase            20.02439\n",
      "number           20.02439\n",
      "dtype: float64\n",
      "sd_total biased           9.12274\n",
      "order            9.12274\n",
      "collaboration    9.12274\n",
      "collaborator     9.12274\n",
      "word             9.12274\n",
      "correct          9.12274\n",
      "buffer           9.12274\n",
      "phase            9.12274\n",
      "number           9.12274\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean averages\n",
    "print('sum_pP_count_phase1', df['SN'].unique())\n",
    "# Total word recalled phase 1\n",
    "sum_pP_count_phase1 = df[(df['phase'] == 1)].groupby('SN').count()\n",
    "mean_total_phase1 = sum_pP_count_phase1.mean()\n",
    "sd_total_phase1 = sum_pP_count_phase1.std()\n",
    "print('mean_total', mean_total_phase1)\n",
    "print('sd_total', sd_total_phase1)\n",
    "# Total words recalled phase 3\n",
    "sum_pP_count_phase3 = df[(df['phase'] == 3)].groupby('SN').count()\n",
    "mean_total_phase3 = sum_pP_count_phase3.mean()\n",
    "sd_total_phase3 = sum_pP_count_phase3.std()\n",
    "print('mean_total', mean_total_phase3)\n",
    "print('sd_total', sd_total_phase3)\n",
    "# Total words recalled phase 3 collaborated\n",
    "sum_pP_count_phase3_Collab1 = df[(df['phase'] == 3)&(df['collaboration'] == 1)].groupby('SN').count()\n",
    "mean_total_phase3_Collab1 = sum_pP_count_phase3_Collab1.mean()\n",
    "sd_total_phase3_Collab1 = sum_pP_count_phase3_Collab1.std()\n",
    "print('mean_total', mean_total_phase3_Collab1)\n",
    "print('sd_total', sd_total_phase3_Collab1)\n",
    "# Total words recalled phase 3 not collaborated\n",
    "sum_pP_count_phase3_Collab0 = df[(df['phase'] == 3)&(df['collaboration'] == 0)].groupby('SN').count()\n",
    "mean_total_phase3_Collab0 = sum_pP_count_phase3_Collab0.mean()\n",
    "sd_total_phase3_Collab0 = sum_pP_count_phase3_Collab0.std()\n",
    "print('mean_total', mean_total_phase3_Collab0)\n",
    "print('sd_total', sd_total_phase3_Collab0)\n",
    "# Total words recalled phase 1 not collaborated\n",
    "sum_pP_count_phase3_Collab0 = df[(df['phase'] == 1)&(df['collaboration'] == 0)].groupby('SN').count()\n",
    "mean_total_phase3_Collab0 = sum_pP_count_phase3_Collab0.mean()\n",
    "sd_total_phase3_Collab0 = sum_pP_count_phase3_Collab0.std()\n",
    "print('mean_total', mean_total_phase3_Collab0)\n",
    "print('sd_total', sd_total_phase3_Collab0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(df, collab1, bias1, order1, phase1, collab2, bias2, order2, phase2, TrueCollab=True, Self=True):\n",
    "  \n",
    "    \"\"\"\n",
    "    USAGE:\n",
    "    all similarity measures = similarity(df, collab1, bias1, order1, phase1, collab2, bias2, order2, phase2, TrueCollab=True, Self=True)\n",
    "    Attention!!! If synthetic (nominal collaborative) groups are included in the comparison, they always have to be in the second position!!!\n",
    "\n",
    "    ARGUMENTS:\n",
    "    df          Dataframe to be used\n",
    "    \n",
    "    collab1     Collaborative condition for group 1: 1=yes, 0=no\n",
    "    bias1       Bias condition for group1: 0=no bias, 1=bias1\n",
    "    order1      If biased, then order 0 or 1\n",
    "    phase1      Which recall phase? Recall 2 for experiments 1(A) and 2(1B)\n",
    "    partner1    If this is a 100 group one can indicate whether 100 collaborated with 110 or 111 (collab, bias, order). \n",
    "                A collaborating, unbiased, order 0 (100) participant always collaborated with a collaborating, biased, of any or specific order (110 or 111?)\n",
    "    \n",
    "    collab2     Collaborative condition for group 2: 1=yes, 0=no\n",
    "    bias2       Bias condition for group 2: 0=no bias, 1=bias1\n",
    "    order2      If biased, then order 0 or 1\n",
    "    phase2      Which recall phase? Recall 2 for experiments 1(A) and 2(1B)\n",
    "    partner2    If this is a 100 group one can indicate whether 100 collaborated with 110 or 111. \n",
    "    \n",
    "    TrueCollab  Did the participants on the two groups actually collaborate: Collaborated with each other=True, Nominal/Synthetic groups=False\n",
    "    Self        Only for experiment 3: Is this a pre-post collaboration comparison? (How about within-group?)\n",
    "\n",
    "    DESCRIPTION:\n",
    "    This helps to calculate the different specific group comparisons\n",
    "    \n",
    "    QUESTIONS:\n",
    "    why would bias be 2?\n",
    "    what is the partner for in partner1==1?\n",
    "    where does df come from? it contains all? \n",
    "    \n",
    "    REMINDER:\n",
    "       \n",
    "    \"\"\"\n",
    "\n",
    "    # Identify all participants in the two groups\n",
    "    # Select the correct group 1 (identifies all participants in this group)\n",
    "    Group1 = df.SN[(df['collaboration'] == collab1) & (df['biased'] == bias1)  & (df['order'] == order1) & (df['phase'] == phase1)]\n",
    "    Group1 = Group1.unique().tolist()\n",
    "    print('normalGroup1')\n",
    "    print('SN1', Group1)\n",
    "           \n",
    "    # Select the correct group 2\n",
    "    Group2 = df.SN[(df['collaboration'] == collab2) & (df['biased'] == bias2)  & (df['order'] == order2) & (df['phase'] == phase2)]\n",
    "    Group2 = Group2.unique().tolist()\n",
    "    print('normalGroup2')\n",
    "    print('SN2', Group2)    \n",
    "    \n",
    "    # Merge all participants for both groups\n",
    "    groupSN = Group2 + Group1\n",
    "    print('groupSN', groupSN)\n",
    "    \n",
    "    # Calculate all potential permutations (aka combinations of participants) for ngram \"index\"\n",
    "    # For bigrams\n",
    "    maxi = max(df.number) \n",
    "    all = list(range(0,maxi))\n",
    "    p2 = list(itertools.permutations(all, 2))\n",
    "    # For trigrams\n",
    "    maxi = max(df.number) \n",
    "    all = list(range(0,maxi))\n",
    "    \n",
    "    # Calculate the average descriptive stats (# true, fals words) per participant in first group (1. set up list, 2. calculate mean per SN, 3. save mean per SN, 4. calculate mean across group)\n",
    "    average_recalled_t_Group1 = []\n",
    "    average_correct_recalled_t_Group1 = []\n",
    "    average_intrusion_recalled_t_Group1 = []\n",
    "    for i in Group1:            \n",
    "        average_recalled_Group1 = df.number[(df['SN'] == i) & (df['collaboration'] == collab1) & (df['biased'] == bias1)  & (df['order'] == order1) & (df['phase'] == phase1)].count()\n",
    "        average_correct_recalled_Group1 = df.number[(df['SN'] == i) & (df['correct'] == 1) & (df['collaboration'] == collab1) & (df['biased'] == bias1)  & (df['order'] == order1) & (df['phase'] == phase1)].count()\n",
    "        average_intrusion_recalled_Group1 = df.number[(df['SN'] == i) & (df['correct'] == 0) & (df['collaboration'] == collab1) & (df['biased'] == bias1)  & (df['order'] == order1) & (df['phase'] == phase1)].count()\n",
    "        average_recalled_t_Group1.append(average_recalled_Group1)\n",
    "        average_correct_recalled_t_Group1.append(average_correct_recalled_Group1)\n",
    "        average_intrusion_recalled_t_Group1.append(average_intrusion_recalled_Group1)\n",
    "    average_recalled_t_Group1 = np.mean(average_recalled_t_Group1)\n",
    "    average_correct_recalled_t_Group1 = np.mean(average_correct_recalled_t_Group1)\n",
    "    average_intrusion_recalled_t_Group1 = np.mean(average_intrusion_recalled_t_Group1)\n",
    "    print('average_recalled_t_Group1=', average_recalled_t_Group1)\n",
    "    print('average_correct_recalled_t_Group1=', average_correct_recalled_t_Group1)\n",
    "    print('average_intrusion_recalled_t_Group1=', average_intrusion_recalled_t_Group1)\n",
    "\n",
    "    # Calculate the average descriptive stats (# true, fals words) per participant in second group (1. set up list, 2. calculate mean per SN, 3. save mean per SN, 4. calculate mean across group)\n",
    "    average_recalled_t_Group2 = []\n",
    "    average_correct_recalled_t_Group2 = []\n",
    "    average_intrusion_recalled_t_Group2 = []\n",
    "    for i in Group2:\n",
    "        average_recalled_Group2 = df.number[(df['SN'] == i) & (df['collaboration'] == collab2) & (df['biased'] == bias2)  & (df['order'] == order2) & (df['phase'] == phase2)].count()\n",
    "        average_correct_recalled_Group2 = df.number[(df['SN'] == i) & (df['correct'] == 1) & (df['collaboration'] == collab2) & (df['biased'] == bias2)  & (df['order'] == order2) & (df['phase'] == phase2)].count()\n",
    "        average_intrusion_recalled_Group2 = df.number[(df['SN'] == i) & (df['correct'] == 0) & (df['collaboration'] == collab2) & (df['biased'] == bias2)  & (df['order'] == order2) & (df['phase'] == phase2)].count()\n",
    "        average_recalled_t_Group2.append(average_recalled_Group2)\n",
    "        average_correct_recalled_t_Group2.append(average_correct_recalled_Group2)\n",
    "        average_intrusion_recalled_t_Group2.append(average_intrusion_recalled_Group2)\n",
    "    average_recalled_t_Group2 = np.mean(average_recalled_t_Group2)\n",
    "    average_correct_recalled_t_Group2 = np.mean(average_correct_recalled_t_Group2)\n",
    "    average_intrusion_recalled_t_Group2 = np.mean(average_intrusion_recalled_t_Group2)\n",
    "    print('average_recalled_t_Group2=', average_recalled_t_Group2)\n",
    "    print('average_correct_recalled_t_Group2=', average_correct_recalled_t_Group2)\n",
    "    print('average_intrusion_recalled_t_Group2=', average_intrusion_recalled_t_Group2)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the dyad specific similarity measures \n",
    "\n",
    "    # Identify all potential/theoretical dyads across the two groups\n",
    "    # In theory I only need this for nominal and nominal collaborative groups, but this was the easiest (albeit not the fastest way)\n",
    "    allCombos = list(itertools.product(Group1, Group2))\n",
    "    allCombos2 = len(allCombos)\n",
    "    \n",
    "    # Set up data frame to save results per dyad in\n",
    "    dfresults = pd.DataFrame(columns=['Comparison','Dyad', 'intersection', 'overlap', 'Jaccard', 'SMC', 'lcs', \\\n",
    "                                      'OmEuni', 'OmEbi', 'OdEuni', 'OdEbi', 'OdMuni', 'OdMbi', 'OmEdMuni', 'OmEdMbi', \\\n",
    "                                      'OmEdMmEMuni', 'OmEdMmEMbi', 'pairedFreq', 'ITR2', 'ARC2', 'editdist','mod_editdist', \\\n",
    "                                      'editdist_IDST', 'editdist_IDS', 'editdist_ID', 'editdist_IDT'])\n",
    "\n",
    "    # Calculate similarity measures for every dyad\n",
    "    # for every dyad (in all combos)\n",
    "    for j in allCombos:\n",
    "        # Reset all values\n",
    "        if Self == False and TrueCollab == False:            \n",
    "            #str1 = df.number[(df['collaboration'] == collab1) & (df['biased'] == bias1) & (df['order'] == order1) & (df['SN'] == j[0]) & (df['phase'] == phase1)]\n",
    "            #str2 = df.number[(df['collaboration'] == collab2) & (df['biased'] == bias2) & (df['order'] == order2) & (df['SN'] == j[1]) & (df['phase'] == phase2)]    \n",
    "            str1 = df.number[(df['collaboration'] == collab1) & (df['biased'] == bias1) & (df['order'] == order1) & (df['SN'] == j[0]) & (df['collaborator'] != j[1]) & (df['phase'] == phase1)]\n",
    "            str2 = df.number[(df['collaboration'] == collab2) & (df['biased'] == bias2) & (df['order'] == order2) & (df['SN'] == j[1]) & (df['collaborator'] != j[0]) & (df['phase'] == phase2)] \n",
    "        # This would only need to be done for each SN and not all combos\n",
    "        elif Self == True and TrueCollab == False:\n",
    "            str1 = df.number[(df['collaboration'] == collab1) & (df['biased'] == bias1) & (df['order'] == order1) & (df['SN'] == j[0]) & (df['phase'] == phase1)]\n",
    "            str2 = df.number[(df['collaboration'] == collab2) & (df['biased'] == bias2) & (df['order'] == order2) & (df['SN'] == j[0]) & (df['phase'] == phase2)]           \n",
    "        elif Self == False and TrueCollab == True:\n",
    "            str1 = df.number[(df['collaboration'] == collab1) & (df['biased'] == bias1) & (df['order'] == order1) & (df['SN'] == j[0]) & (df['collaborator'] == j[1]) & (df['phase'] == phase1)]\n",
    "            str2 = df.number[(df['collaboration'] == collab2) & (df['biased'] == bias2) & (df['order'] == order2) & (df['SN'] == j[1]) & (df['collaborator'] == j[0]) & (df['phase'] == phase2)] \n",
    "        \n",
    "        num_comm = len(np.intersect1d(str1, str2))\n",
    "\n",
    "        if len(str1)==0 or len(str2)==0:\n",
    "            pass\n",
    "        else:\n",
    "            #num_pairs = ngram_abs(str1, str2, 2, p2, unidirectional=True)\n",
    "            #num_trip = ngram_abs(str1, str2, 3, p3, unidirectional=True) \n",
    "\n",
    "            a = intersection(str1, str2)\n",
    "            b = overlap(str1, str2)\n",
    "            c = Jaccard(str1, str2)\n",
    "            d = SMC(str1, str2)\n",
    "            f = OmE(str1, str2, 2, unidirectional=True)\n",
    "            g = OmE(str1, str2, 2, unidirectional=False)\n",
    "            h = OdE(str1, str2, 2, unidirectional=True)\n",
    "            z = OdE(str1, str2, 2, unidirectional=False)\n",
    "            k = OdM(str1, str2, 2, unidirectional=True)\n",
    "            l = OdM(str1, str2, 2, unidirectional=False)\n",
    "            m = OmEdM(str1, str2, 2, unidirectional=True)\n",
    "            n = OmEdM(str1, str2, 2, unidirectional=False)\n",
    "            o = OmEdMmE(str1, str2, 2, unidirectional=True)\n",
    "            p = OmEdMmE(str1, str2, 2, unidirectional=False)\n",
    "            q = pairedFreq(str1, str2)\n",
    "            r = ITR2(str1, str2)\n",
    "            s = ARC2(str1, str2)\n",
    "\n",
    "            str1 = str1.reset_index(drop=True)\n",
    "            str2 = str2.reset_index(drop=True)\n",
    "            str1 = np.array(str1)\n",
    "            str2 = np.array(str2)\n",
    "            e = lcs(str1, str2)\n",
    "            t = editdist(str1, str2, min_threshold = 0)\n",
    "            u = mod_editdist(str1, str2, min_threshold = 0)\n",
    "            v = edit_dists(str1, str2, insert=True, delete=True, substitute=True, transpose=True)\n",
    "            w = edit_dists(str1, str2, insert=True, delete=True, substitute=True, transpose=False)\n",
    "            x = edit_dists(str1, str2, insert=True, delete=True, substitute=False, transpose=False)\n",
    "            y = edit_dists(str1, str2, insert=True, delete=True, substitute=False, transpose=True)\n",
    "\n",
    "            # it saves all prior values for abc... if j exists but no update. so have to reset all to zero\n",
    "            trialDict = {'Comparison':(str(collab1) + str(bias1) + str(order1) + str(phase1) + str(int(TrueCollab)) + '_' \\\n",
    "                         + str(collab2) + str(bias2) + str(order2) + str(phase2) + str(int(TrueCollab))),'Dyad':j, \\\n",
    "                         'intersection':a, 'overlap':b, 'Jaccard':c, 'SMC':d, 'lcs':e,  \\\n",
    "                         'OmEuni':f, 'OmEbi':g, 'OdEuni':h, 'OdEbi':z, 'OdMuni':k, 'OdMbi':l, 'OmEdMuni':m, 'OmEdMbi':n, 'OmEdMmEMuni':o, \\\n",
    "                         'OmEdMmEMbi':p, 'pairedFreq':q, 'ITR2':r, 'ARC2':s, 'editdist':t,'mod_editdist':u, \\\n",
    "                         'editdist_IDST':v, 'editdist_IDS':w, 'editdist_ID':x, 'editdist_IDT':y}              \n",
    "                        # 'num_pairs':num_pairs, 'num_trip':num_trip,\n",
    "            dfresults = dfresults.append(trialDict, ignore_index=True)    \n",
    "        \n",
    "    # Calculate mean for descriptive statistics\n",
    "    mean = dfresults.mean()\n",
    "    #mode_0000_0001 = dfresults.mode()\n",
    "    #median_0000_0001 = dfresults.median()\n",
    "    print('Mean=', mean)\n",
    "    #print('Mode=',mode_0000_0001)\n",
    "    #print('Median=', median_0000_0001)\n",
    "    # If file name is changed, it has to also be changed below when merging the different files\n",
    "    dfresults.to_csv('2022-04-18_Similarity_Exp3_AllWords_Clean_'+ str(collab1) + str(bias1) + str(order1) + str(phase1) + str(int(TrueCollab)) + str(int(Self))+ '_' +  str(collab2) + str(bias2) + str(order2) + str(phase2) + str(int(TrueCollab))+ str(int(Self)) +'_Results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalGroup1\n",
      "SN1 [24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62]\n",
      "normalGroup2\n",
      "SN2 [29, 33, 37, 39, 43, 47, 51, 55, 59, 93]\n",
      "groupSN [29, 33, 37, 39, 43, 47, 51, 55, 59, 93, 24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62]\n",
      "average_recalled_t_Group1= 22.363636363636363\n",
      "average_correct_recalled_t_Group1= 21.636363636363637\n",
      "average_intrusion_recalled_t_Group1= 0.7272727272727273\n",
      "average_recalled_t_Group2= 13.8\n",
      "average_correct_recalled_t_Group2= 12.6\n",
      "average_intrusion_recalled_t_Group2= 1.2\n",
      "[(24, 29), (24, 33), (24, 37), (24, 39), (24, 43), (24, 47), (24, 51), (24, 55), (24, 59), (24, 93), (28, 29), (28, 33), (28, 37), (28, 39), (28, 43), (28, 47), (28, 51), (28, 55), (28, 59), (28, 93), (32, 29), (32, 33), (32, 37), (32, 39), (32, 43), (32, 47), (32, 51), (32, 55), (32, 59), (32, 93), (36, 29), (36, 33), (36, 37), (36, 39), (36, 43), (36, 47), (36, 51), (36, 55), (36, 59), (36, 93), (38, 29), (38, 33), (38, 37), (38, 39), (38, 43), (38, 47), (38, 51), (38, 55), (38, 59), (38, 93), (42, 29), (42, 33), (42, 37), (42, 39), (42, 43), (42, 47), (42, 51), (42, 55), (42, 59), (42, 93), (46, 29), (46, 33), (46, 37), (46, 39), (46, 43), (46, 47), (46, 51), (46, 55), (46, 59), (46, 93), (50, 29), (50, 33), (50, 37), (50, 39), (50, 43), (50, 47), (50, 51), (50, 55), (50, 59), (50, 93), (54, 29), (54, 33), (54, 37), (54, 39), (54, 43), (54, 47), (54, 51), (54, 55), (54, 59), (54, 93), (58, 29), (58, 33), (58, 37), (58, 39), (58, 43), (58, 47), (58, 51), (58, 55), (58, 59), (58, 93), (62, 29), (62, 33), (62, 37), (62, 39), (62, 43), (62, 47), (62, 51), (62, 55), (62, 59), (62, 93)]\n",
      "Comparison            inf\n",
      "intersection     3.745455\n",
      "overlap          0.286672\n",
      "Jaccard          0.109670\n",
      "SMC              0.707128\n",
      "lcs              1.000000\n",
      "OmEuni           0.066742\n",
      "OmEbi            0.215303\n",
      "OdEuni           2.232020\n",
      "OdEbi            2.207894\n",
      "OdMuni           0.033485\n",
      "OdMbi            0.064506\n",
      "OmEdMuni         0.021908\n",
      "OmEdMbi          0.041353\n",
      "OmEdMmEMuni      0.022250\n",
      "OmEdMmEMbi       0.042550\n",
      "pairedFreq       0.215303\n",
      "ITR2             0.064506\n",
      "ARC2             0.042550\n",
      "editdist         0.046671\n",
      "mod_editdist     0.046671\n",
      "editdist_IDST    0.046671\n",
      "editdist_IDS     0.046671\n",
      "editdist_ID      0.110703\n",
      "editdist_IDT     0.113899\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [1, 3, 9, 15, 19, 67, 71, 75, 79]\n",
      "normalGroup2\n",
      "SN2 [2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80]\n",
      "groupSN [2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80, 1, 3, 9, 15, 19, 67, 71, 75, 79]\n",
      "average_recalled_t_Group1= 27.22222222222222\n",
      "average_correct_recalled_t_Group1= 26.333333333333332\n",
      "average_intrusion_recalled_t_Group1= 0.8888888888888888\n",
      "average_recalled_t_Group2= 17.454545454545453\n",
      "average_correct_recalled_t_Group2= 16.727272727272727\n",
      "average_intrusion_recalled_t_Group2= 0.7272727272727273\n",
      "[(1, 2), (1, 4), (1, 10), (1, 16), (1, 26), (1, 64), (1, 66), (1, 68), (1, 72), (1, 76), (1, 80), (3, 2), (3, 4), (3, 10), (3, 16), (3, 26), (3, 64), (3, 66), (3, 68), (3, 72), (3, 76), (3, 80), (9, 2), (9, 4), (9, 10), (9, 16), (9, 26), (9, 64), (9, 66), (9, 68), (9, 72), (9, 76), (9, 80), (15, 2), (15, 4), (15, 10), (15, 16), (15, 26), (15, 64), (15, 66), (15, 68), (15, 72), (15, 76), (15, 80), (19, 2), (19, 4), (19, 10), (19, 16), (19, 26), (19, 64), (19, 66), (19, 68), (19, 72), (19, 76), (19, 80), (67, 2), (67, 4), (67, 10), (67, 16), (67, 26), (67, 64), (67, 66), (67, 68), (67, 72), (67, 76), (67, 80), (71, 2), (71, 4), (71, 10), (71, 16), (71, 26), (71, 64), (71, 66), (71, 68), (71, 72), (71, 76), (71, 80), (75, 2), (75, 4), (75, 10), (75, 16), (75, 26), (75, 64), (75, 66), (75, 68), (75, 72), (75, 76), (75, 80), (79, 2), (79, 4), (79, 10), (79, 16), (79, 26), (79, 64), (79, 66), (79, 68), (79, 72), (79, 76), (79, 80)]\n",
      "Comparison            inf\n",
      "intersection     5.676768\n",
      "overlap          0.330639\n",
      "Jaccard          0.142591\n",
      "SMC              0.651286\n",
      "lcs              1.111111\n",
      "OmEuni           0.106973\n",
      "OmEbi            0.244249\n",
      "OdEuni           3.524395\n",
      "OdEbi            3.733716\n",
      "OdMuni           0.039358\n",
      "OdMbi            0.081177\n",
      "OmEdMuni         0.027725\n",
      "OmEdMbi          0.057912\n",
      "OmEdMmEMuni      0.028040\n",
      "OmEdMmEMbi       0.059207\n",
      "pairedFreq       0.244249\n",
      "ITR2             0.081177\n",
      "ARC2             0.059207\n",
      "editdist         0.044131\n",
      "mod_editdist     0.044131\n",
      "editdist_IDST    0.044131\n",
      "editdist_IDS     0.044131\n",
      "editdist_ID      0.122711\n",
      "editdist_IDT     0.125598\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "normalGroup2\n",
      "SN2 [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91]\n",
      "groupSN [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91, 12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "average_recalled_t_Group1= 17.416666666666668\n",
      "average_correct_recalled_t_Group1= 16.416666666666668\n",
      "average_intrusion_recalled_t_Group1= 1.0\n",
      "average_recalled_t_Group2= 21.083333333333332\n",
      "average_correct_recalled_t_Group2= 20.5\n",
      "average_intrusion_recalled_t_Group2= 0.5833333333333334\n",
      "[(12, 11), (12, 23), (12, 31), (12, 35), (12, 41), (12, 49), (12, 53), (12, 57), (12, 61), (12, 83), (12, 87), (12, 91), (22, 11), (22, 23), (22, 31), (22, 35), (22, 41), (22, 49), (22, 53), (22, 57), (22, 61), (22, 83), (22, 87), (22, 91), (30, 11), (30, 23), (30, 31), (30, 35), (30, 41), (30, 49), (30, 53), (30, 57), (30, 61), (30, 83), (30, 87), (30, 91), (34, 11), (34, 23), (34, 31), (34, 35), (34, 41), (34, 49), (34, 53), (34, 57), (34, 61), (34, 83), (34, 87), (34, 91), (40, 11), (40, 23), (40, 31), (40, 35), (40, 41), (40, 49), (40, 53), (40, 57), (40, 61), (40, 83), (40, 87), (40, 91), (48, 11), (48, 23), (48, 31), (48, 35), (48, 41), (48, 49), (48, 53), (48, 57), (48, 61), (48, 83), (48, 87), (48, 91), (52, 11), (52, 23), (52, 31), (52, 35), (52, 41), (52, 49), (52, 53), (52, 57), (52, 61), (52, 83), (52, 87), (52, 91), (56, 11), (56, 23), (56, 31), (56, 35), (56, 41), (56, 49), (56, 53), (56, 57), (56, 61), (56, 83), (56, 87), (56, 91), (60, 11), (60, 23), (60, 31), (60, 35), (60, 41), (60, 49), (60, 53), (60, 57), (60, 61), (60, 83), (60, 87), (60, 91), (84, 11), (84, 23), (84, 31), (84, 35), (84, 41), (84, 49), (84, 53), (84, 57), (84, 61), (84, 83), (84, 87), (84, 91), (88, 11), (88, 23), (88, 31), (88, 35), (88, 41), (88, 49), (88, 53), (88, 57), (88, 61), (88, 83), (88, 87), (88, 91), (92, 11), (92, 23), (92, 31), (92, 35), (92, 41), (92, 49), (92, 53), (92, 57), (92, 61), (92, 83), (92, 87), (92, 91)]\n",
      "Comparison            inf\n",
      "intersection     4.992424\n",
      "overlap          0.337823\n",
      "Jaccard          0.139956\n",
      "SMC              0.704976\n",
      "lcs              1.037879\n",
      "OmEuni           0.073981\n",
      "OmEbi            0.276749\n",
      "OdEuni           1.219094\n",
      "OdEbi            2.213399\n",
      "OdMuni           0.018667\n",
      "OdMbi            0.067409\n",
      "OmEdMuni         0.005825\n",
      "OmEdMbi          0.041725\n",
      "OmEdMmEMuni      0.005912\n",
      "OmEdMmEMbi       0.043094\n",
      "pairedFreq       0.276749\n",
      "ITR2             0.067409\n",
      "ARC2             0.043094\n",
      "editdist         0.047606\n",
      "mod_editdist     0.047800\n",
      "editdist_IDST    0.047800\n",
      "editdist_IDS     0.047606\n",
      "editdist_ID      0.126468\n",
      "editdist_IDT     0.130816\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "normalGroup2\n",
      "SN2 [6, 8, 18, 70, 74, 78, 82, 86, 90]\n",
      "groupSN [6, 8, 18, 70, 74, 78, 82, 86, 90, 5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "average_recalled_t_Group1= 14.777777777777779\n",
      "average_correct_recalled_t_Group1= 13.777777777777779\n",
      "average_intrusion_recalled_t_Group1= 1.0\n",
      "average_recalled_t_Group2= 29.555555555555557\n",
      "average_correct_recalled_t_Group2= 26.0\n",
      "average_intrusion_recalled_t_Group2= 3.5555555555555554\n",
      "[(5, 6), (5, 8), (5, 18), (5, 70), (5, 74), (5, 78), (5, 82), (5, 86), (5, 90), (7, 6), (7, 8), (7, 18), (7, 70), (7, 74), (7, 78), (7, 82), (7, 86), (7, 90), (17, 6), (17, 8), (17, 18), (17, 70), (17, 74), (17, 78), (17, 82), (17, 86), (17, 90), (69, 6), (69, 8), (69, 18), (69, 70), (69, 74), (69, 78), (69, 82), (69, 86), (69, 90), (73, 6), (73, 8), (73, 18), (73, 70), (73, 74), (73, 78), (73, 82), (73, 86), (73, 90), (77, 6), (77, 8), (77, 18), (77, 70), (77, 74), (77, 78), (77, 82), (77, 86), (77, 90), (81, 6), (81, 8), (81, 18), (81, 70), (81, 74), (81, 78), (81, 82), (81, 86), (81, 90), (85, 6), (85, 8), (85, 18), (85, 70), (85, 74), (85, 78), (85, 82), (85, 86), (85, 90), (89, 6), (89, 8), (89, 18), (89, 70), (89, 74), (89, 78), (89, 82), (89, 86), (89, 90)]\n",
      "Comparison            inf\n",
      "intersection     5.652778\n",
      "overlap          0.381484\n",
      "Jaccard          0.141105\n",
      "SMC              0.686711\n",
      "lcs              1.055556\n",
      "OmEuni           0.093983\n",
      "OmEbi            0.312966\n",
      "OdEuni           1.728658\n",
      "OdEbi            2.145229\n",
      "OdMuni           0.025798\n",
      "OdMbi            0.065526\n",
      "OmEdMuni         0.013711\n",
      "OmEdMbi          0.041352\n",
      "OmEdMmEMuni      0.013930\n",
      "OmEdMmEMbi       0.042769\n",
      "pairedFreq       0.312966\n",
      "ITR2             0.065526\n",
      "ARC2             0.042769\n",
      "editdist         0.064095\n",
      "mod_editdist     0.064095\n",
      "editdist_IDST    0.064095\n",
      "editdist_IDS     0.064095\n",
      "editdist_ID      0.120227\n",
      "editdist_IDT     0.123841\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "normalGroup2\n",
      "SN2 [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91]\n",
      "groupSN [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91, 12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "average_recalled_t_Group1= 17.416666666666668\n",
      "average_correct_recalled_t_Group1= 16.416666666666668\n",
      "average_intrusion_recalled_t_Group1= 1.0\n",
      "average_recalled_t_Group2= 21.083333333333332\n",
      "average_correct_recalled_t_Group2= 20.5\n",
      "average_intrusion_recalled_t_Group2= 0.5833333333333334\n",
      "[(12, 11), (12, 23), (12, 31), (12, 35), (12, 41), (12, 49), (12, 53), (12, 57), (12, 61), (12, 83), (12, 87), (12, 91), (22, 11), (22, 23), (22, 31), (22, 35), (22, 41), (22, 49), (22, 53), (22, 57), (22, 61), (22, 83), (22, 87), (22, 91), (30, 11), (30, 23), (30, 31), (30, 35), (30, 41), (30, 49), (30, 53), (30, 57), (30, 61), (30, 83), (30, 87), (30, 91), (34, 11), (34, 23), (34, 31), (34, 35), (34, 41), (34, 49), (34, 53), (34, 57), (34, 61), (34, 83), (34, 87), (34, 91), (40, 11), (40, 23), (40, 31), (40, 35), (40, 41), (40, 49), (40, 53), (40, 57), (40, 61), (40, 83), (40, 87), (40, 91), (48, 11), (48, 23), (48, 31), (48, 35), (48, 41), (48, 49), (48, 53), (48, 57), (48, 61), (48, 83), (48, 87), (48, 91), (52, 11), (52, 23), (52, 31), (52, 35), (52, 41), (52, 49), (52, 53), (52, 57), (52, 61), (52, 83), (52, 87), (52, 91), (56, 11), (56, 23), (56, 31), (56, 35), (56, 41), (56, 49), (56, 53), (56, 57), (56, 61), (56, 83), (56, 87), (56, 91), (60, 11), (60, 23), (60, 31), (60, 35), (60, 41), (60, 49), (60, 53), (60, 57), (60, 61), (60, 83), (60, 87), (60, 91), (84, 11), (84, 23), (84, 31), (84, 35), (84, 41), (84, 49), (84, 53), (84, 57), (84, 61), (84, 83), (84, 87), (84, 91), (88, 11), (88, 23), (88, 31), (88, 35), (88, 41), (88, 49), (88, 53), (88, 57), (88, 61), (88, 83), (88, 87), (88, 91), (92, 11), (92, 23), (92, 31), (92, 35), (92, 41), (92, 49), (92, 53), (92, 57), (92, 61), (92, 83), (92, 87), (92, 91)]\n",
      "Comparison       9.259251e+117\n",
      "intersection      5.166667e+00\n",
      "overlap           3.055252e-01\n",
      "Jaccard           1.371589e-01\n",
      "SMC               7.083333e-01\n",
      "lcs               1.083333e+00\n",
      "OmEuni            1.815294e-01\n",
      "OmEbi             4.463922e-01\n",
      "OdEuni            5.049074e+00\n",
      "OdEbi             4.337900e+00\n",
      "OdMuni            5.803571e-02\n",
      "OdMbi             1.128968e-01\n",
      "OmEdMuni          4.619069e-02\n",
      "OmEdMbi           8.920677e-02\n",
      "OmEdMmEMuni       4.672549e-02\n",
      "OmEdMmEMbi        9.162812e-02\n",
      "pairedFreq        4.463922e-01\n",
      "ITR2              1.128968e-01\n",
      "ARC2              9.162812e-02\n",
      "editdist          4.854942e-02\n",
      "mod_editdist      4.854942e-02\n",
      "editdist_IDST     4.854942e-02\n",
      "editdist_IDS      4.854942e-02\n",
      "editdist_ID       1.217717e-01\n",
      "editdist_IDT      1.257378e-01\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "normalGroup2\n",
      "SN2 [6, 8, 18, 70, 74, 78, 82, 86, 90]\n",
      "groupSN [6, 8, 18, 70, 74, 78, 82, 86, 90, 5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "average_recalled_t_Group1= 14.777777777777779\n",
      "average_correct_recalled_t_Group1= 13.777777777777779\n",
      "average_intrusion_recalled_t_Group1= 1.0\n",
      "average_recalled_t_Group2= 29.555555555555557\n",
      "average_correct_recalled_t_Group2= 26.0\n",
      "average_intrusion_recalled_t_Group2= 3.5555555555555554\n",
      "[(5, 6), (5, 8), (5, 18), (5, 70), (5, 74), (5, 78), (5, 82), (5, 86), (5, 90), (7, 6), (7, 8), (7, 18), (7, 70), (7, 74), (7, 78), (7, 82), (7, 86), (7, 90), (17, 6), (17, 8), (17, 18), (17, 70), (17, 74), (17, 78), (17, 82), (17, 86), (17, 90), (69, 6), (69, 8), (69, 18), (69, 70), (69, 74), (69, 78), (69, 82), (69, 86), (69, 90), (73, 6), (73, 8), (73, 18), (73, 70), (73, 74), (73, 78), (73, 82), (73, 86), (73, 90), (77, 6), (77, 8), (77, 18), (77, 70), (77, 74), (77, 78), (77, 82), (77, 86), (77, 90), (81, 6), (81, 8), (81, 18), (81, 70), (81, 74), (81, 78), (81, 82), (81, 86), (81, 90), (85, 6), (85, 8), (85, 18), (85, 70), (85, 74), (85, 78), (85, 82), (85, 86), (85, 90), (89, 6), (89, 8), (89, 18), (89, 70), (89, 74), (89, 78), (89, 82), (89, 86), (89, 90)]\n",
      "Comparison       1.223456e+88\n",
      "intersection     5.000000e+00\n",
      "overlap          3.638059e-01\n",
      "Jaccard          1.271092e-01\n",
      "SMC              6.717172e-01\n",
      "lcs              1.111111e+00\n",
      "OmEuni           2.764765e-01\n",
      "OmEbi            3.307307e-01\n",
      "OdEuni           5.645679e+00\n",
      "OdEbi            3.652469e+00\n",
      "OdMuni           6.790123e-02\n",
      "OdMbi            9.012346e-02\n",
      "OmEdMuni         5.618839e-02\n",
      "OmEdMbi          6.669777e-02\n",
      "OmEdMmEMuni      5.687465e-02\n",
      "OmEdMmEMbi       6.837400e-02\n",
      "pairedFreq       3.307307e-01\n",
      "ITR2             9.012346e-02\n",
      "ARC2             6.837400e-02\n",
      "editdist         7.320224e-02\n",
      "mod_editdist     7.320224e-02\n",
      "editdist_IDST    7.320224e-02\n",
      "editdist_IDS     7.320224e-02\n",
      "editdist_ID      1.188928e-01\n",
      "editdist_IDT     1.214181e-01\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62]\n",
      "normalGroup2\n",
      "SN2 [29, 33, 37, 39, 43, 47, 51, 55, 59, 93]\n",
      "groupSN [29, 33, 37, 39, 43, 47, 51, 55, 59, 93, 24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62]\n",
      "average_recalled_t_Group1= 26.272727272727273\n",
      "average_correct_recalled_t_Group1= 24.90909090909091\n",
      "average_intrusion_recalled_t_Group1= 1.3636363636363635\n",
      "average_recalled_t_Group2= 17.2\n",
      "average_correct_recalled_t_Group2= 14.9\n",
      "average_intrusion_recalled_t_Group2= 2.3\n",
      "[(24, 29), (24, 33), (24, 37), (24, 39), (24, 43), (24, 47), (24, 51), (24, 55), (24, 59), (24, 93), (28, 29), (28, 33), (28, 37), (28, 39), (28, 43), (28, 47), (28, 51), (28, 55), (28, 59), (28, 93), (32, 29), (32, 33), (32, 37), (32, 39), (32, 43), (32, 47), (32, 51), (32, 55), (32, 59), (32, 93), (36, 29), (36, 33), (36, 37), (36, 39), (36, 43), (36, 47), (36, 51), (36, 55), (36, 59), (36, 93), (38, 29), (38, 33), (38, 37), (38, 39), (38, 43), (38, 47), (38, 51), (38, 55), (38, 59), (38, 93), (42, 29), (42, 33), (42, 37), (42, 39), (42, 43), (42, 47), (42, 51), (42, 55), (42, 59), (42, 93), (46, 29), (46, 33), (46, 37), (46, 39), (46, 43), (46, 47), (46, 51), (46, 55), (46, 59), (46, 93), (50, 29), (50, 33), (50, 37), (50, 39), (50, 43), (50, 47), (50, 51), (50, 55), (50, 59), (50, 93), (54, 29), (54, 33), (54, 37), (54, 39), (54, 43), (54, 47), (54, 51), (54, 55), (54, 59), (54, 93), (58, 29), (58, 33), (58, 37), (58, 39), (58, 43), (58, 47), (58, 51), (58, 55), (58, 59), (58, 93), (62, 29), (62, 33), (62, 37), (62, 39), (62, 43), (62, 47), (62, 51), (62, 55), (62, 59), (62, 93)]\n",
      "Comparison            inf\n",
      "intersection     5.190909\n",
      "overlap          0.324311\n",
      "Jaccard          0.127839\n",
      "SMC              0.673347\n",
      "lcs              1.136364\n",
      "OmEuni           0.151060\n",
      "OmEbi            0.402120\n",
      "OdEuni           2.592729\n",
      "OdEbi            3.373773\n",
      "OdMuni           0.031665\n",
      "OdMbi            0.081734\n",
      "OmEdMuni         0.020742\n",
      "OmEdMbi          0.059887\n",
      "OmEdMmEMuni      0.021009\n",
      "OmEdMmEMbi       0.061470\n",
      "pairedFreq       0.402120\n",
      "ITR2             0.081734\n",
      "ARC2             0.061470\n",
      "editdist         0.049989\n",
      "mod_editdist     0.050228\n",
      "editdist_IDST    0.050228\n",
      "editdist_IDS     0.049989\n",
      "editdist_ID      0.115971\n",
      "editdist_IDT     0.120755\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [1, 3, 9, 15, 19, 67, 71, 75, 79]\n",
      "normalGroup2\n",
      "SN2 [2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80]\n",
      "groupSN [2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80, 1, 3, 9, 15, 19, 67, 71, 75, 79]\n",
      "average_recalled_t_Group1= 31.22222222222222\n",
      "average_correct_recalled_t_Group1= 30.22222222222222\n",
      "average_intrusion_recalled_t_Group1= 1.0\n",
      "average_recalled_t_Group2= 18.272727272727273\n",
      "average_correct_recalled_t_Group2= 15.909090909090908\n",
      "average_intrusion_recalled_t_Group2= 2.3636363636363638\n",
      "[(1, 2), (1, 4), (1, 10), (1, 16), (1, 26), (1, 64), (1, 66), (1, 68), (1, 72), (1, 76), (1, 80), (3, 2), (3, 4), (3, 10), (3, 16), (3, 26), (3, 64), (3, 66), (3, 68), (3, 72), (3, 76), (3, 80), (9, 2), (9, 4), (9, 10), (9, 16), (9, 26), (9, 64), (9, 66), (9, 68), (9, 72), (9, 76), (9, 80), (15, 2), (15, 4), (15, 10), (15, 16), (15, 26), (15, 64), (15, 66), (15, 68), (15, 72), (15, 76), (15, 80), (19, 2), (19, 4), (19, 10), (19, 16), (19, 26), (19, 64), (19, 66), (19, 68), (19, 72), (19, 76), (19, 80), (67, 2), (67, 4), (67, 10), (67, 16), (67, 26), (67, 64), (67, 66), (67, 68), (67, 72), (67, 76), (67, 80), (71, 2), (71, 4), (71, 10), (71, 16), (71, 26), (71, 64), (71, 66), (71, 68), (71, 72), (71, 76), (71, 80), (75, 2), (75, 4), (75, 10), (75, 16), (75, 26), (75, 64), (75, 66), (75, 68), (75, 72), (75, 76), (75, 80), (79, 2), (79, 4), (79, 10), (79, 16), (79, 26), (79, 64), (79, 66), (79, 68), (79, 72), (79, 76), (79, 80)]\n",
      "Comparison            inf\n",
      "intersection     6.717172\n",
      "overlap          0.346092\n",
      "Jaccard          0.149742\n",
      "SMC              0.639922\n",
      "lcs              1.232323\n",
      "OmEuni           0.466516\n",
      "OmEbi            0.781516\n",
      "OdEuni           5.473721\n",
      "OdEbi            5.294973\n",
      "OdMuni           0.070484\n",
      "OdMbi            0.130779\n",
      "OmEdMuni         0.059559\n",
      "OmEdMbi          0.108929\n",
      "OmEdMmEMuni      0.060390\n",
      "OmEdMmEMbi       0.111898\n",
      "pairedFreq       0.781516\n",
      "ITR2             0.130779\n",
      "ARC2             0.111898\n",
      "editdist         0.046676\n",
      "mod_editdist     0.046676\n",
      "editdist_IDST    0.046676\n",
      "editdist_IDS     0.046676\n",
      "editdist_ID      0.120259\n",
      "editdist_IDT     0.125169\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "normalGroup2\n",
      "SN2 [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91]\n",
      "groupSN [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91, 12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "average_recalled_t_Group1= 24.583333333333332\n",
      "average_correct_recalled_t_Group1= 23.916666666666668\n",
      "average_intrusion_recalled_t_Group1= 0.6666666666666666\n",
      "average_recalled_t_Group2= 27.083333333333332\n",
      "average_correct_recalled_t_Group2= 26.083333333333332\n",
      "average_intrusion_recalled_t_Group2= 1.0\n",
      "[(12, 11), (12, 23), (12, 31), (12, 35), (12, 41), (12, 49), (12, 53), (12, 57), (12, 61), (12, 83), (12, 87), (12, 91), (22, 11), (22, 23), (22, 31), (22, 35), (22, 41), (22, 49), (22, 53), (22, 57), (22, 61), (22, 83), (22, 87), (22, 91), (30, 11), (30, 23), (30, 31), (30, 35), (30, 41), (30, 49), (30, 53), (30, 57), (30, 61), (30, 83), (30, 87), (30, 91), (34, 11), (34, 23), (34, 31), (34, 35), (34, 41), (34, 49), (34, 53), (34, 57), (34, 61), (34, 83), (34, 87), (34, 91), (40, 11), (40, 23), (40, 31), (40, 35), (40, 41), (40, 49), (40, 53), (40, 57), (40, 61), (40, 83), (40, 87), (40, 91), (48, 11), (48, 23), (48, 31), (48, 35), (48, 41), (48, 49), (48, 53), (48, 57), (48, 61), (48, 83), (48, 87), (48, 91), (52, 11), (52, 23), (52, 31), (52, 35), (52, 41), (52, 49), (52, 53), (52, 57), (52, 61), (52, 83), (52, 87), (52, 91), (56, 11), (56, 23), (56, 31), (56, 35), (56, 41), (56, 49), (56, 53), (56, 57), (56, 61), (56, 83), (56, 87), (56, 91), (60, 11), (60, 23), (60, 31), (60, 35), (60, 41), (60, 49), (60, 53), (60, 57), (60, 61), (60, 83), (60, 87), (60, 91), (84, 11), (84, 23), (84, 31), (84, 35), (84, 41), (84, 49), (84, 53), (84, 57), (84, 61), (84, 83), (84, 87), (84, 91), (88, 11), (88, 23), (88, 31), (88, 35), (88, 41), (88, 49), (88, 53), (88, 57), (88, 61), (88, 83), (88, 87), (88, 91), (92, 11), (92, 23), (92, 31), (92, 35), (92, 41), (92, 49), (92, 53), (92, 57), (92, 61), (92, 83), (92, 87), (92, 91)]\n",
      "Comparison            inf\n",
      "intersection     9.621212\n",
      "overlap          0.458193\n",
      "Jaccard          0.213155\n",
      "SMC              0.662965\n",
      "lcs              1.492424\n",
      "OmEuni           0.668918\n",
      "OmEbi            1.284805\n",
      "OdEuni           5.331115\n",
      "OdEbi            5.273841\n",
      "OdMuni           0.080681\n",
      "OdMbi            0.163295\n",
      "OmEdMuni         0.066545\n",
      "OmEdMbi          0.135022\n",
      "OmEdMmEMuni      0.067592\n",
      "OmEdMmEMbi       0.139561\n",
      "pairedFreq       1.284805\n",
      "ITR2             0.163295\n",
      "ARC2             0.139561\n",
      "editdist         0.061089\n",
      "mod_editdist     0.061089\n",
      "editdist_IDST    0.061089\n",
      "editdist_IDS     0.061089\n",
      "editdist_ID      0.149962\n",
      "editdist_IDT     0.158462\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "normalGroup2\n",
      "SN2 [6, 8, 18, 70, 74, 78, 82, 86, 90]\n",
      "groupSN [6, 8, 18, 70, 74, 78, 82, 86, 90, 5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "average_recalled_t_Group1= 24.88888888888889\n",
      "average_correct_recalled_t_Group1= 22.555555555555557\n",
      "average_intrusion_recalled_t_Group1= 2.3333333333333335\n",
      "average_recalled_t_Group2= 44.44444444444444\n",
      "average_correct_recalled_t_Group2= 32.44444444444444\n",
      "average_intrusion_recalled_t_Group2= 12.0\n",
      "[(5, 6), (5, 8), (5, 18), (5, 70), (5, 74), (5, 78), (5, 82), (5, 86), (5, 90), (7, 6), (7, 8), (7, 18), (7, 70), (7, 74), (7, 78), (7, 82), (7, 86), (7, 90), (17, 6), (17, 8), (17, 18), (17, 70), (17, 74), (17, 78), (17, 82), (17, 86), (17, 90), (69, 6), (69, 8), (69, 18), (69, 70), (69, 74), (69, 78), (69, 82), (69, 86), (69, 90), (73, 6), (73, 8), (73, 18), (73, 70), (73, 74), (73, 78), (73, 82), (73, 86), (73, 90), (77, 6), (77, 8), (77, 18), (77, 70), (77, 74), (77, 78), (77, 82), (77, 86), (77, 90), (81, 6), (81, 8), (81, 18), (81, 70), (81, 74), (81, 78), (81, 82), (81, 86), (81, 90), (85, 6), (85, 8), (85, 18), (85, 70), (85, 74), (85, 78), (85, 82), (85, 86), (85, 90), (89, 6), (89, 8), (89, 18), (89, 70), (89, 74), (89, 78), (89, 82), (89, 86), (89, 90)]\n",
      "Comparison             inf\n",
      "intersection     10.986111\n",
      "overlap           0.439837\n",
      "Jaccard           0.212407\n",
      "SMC               0.637311\n",
      "lcs               1.500000\n",
      "OmEuni            0.508528\n",
      "OmEbi             0.878167\n",
      "OdEuni            5.119612\n",
      "OdEbi             4.406805\n",
      "OdMuni            0.058989\n",
      "OdMbi             0.104784\n",
      "OmEdMuni          0.047595\n",
      "OmEdMbi           0.081997\n",
      "OmEdMmEMuni       0.048185\n",
      "OmEdMmEMbi        0.084102\n",
      "pairedFreq        0.878167\n",
      "ITR2              0.104784\n",
      "ARC2              0.084102\n",
      "editdist          0.070547\n",
      "mod_editdist      0.070886\n",
      "editdist_IDST     0.070886\n",
      "editdist_IDS      0.070547\n",
      "editdist_ID       0.151218\n",
      "editdist_IDT      0.157002\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "normalGroup2\n",
      "SN2 [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91]\n",
      "groupSN [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91, 12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "average_recalled_t_Group1= 24.583333333333332\n",
      "average_correct_recalled_t_Group1= 23.916666666666668\n",
      "average_intrusion_recalled_t_Group1= 0.6666666666666666\n",
      "average_recalled_t_Group2= 27.083333333333332\n",
      "average_correct_recalled_t_Group2= 26.083333333333332\n",
      "average_intrusion_recalled_t_Group2= 1.0\n",
      "[(12, 11), (12, 23), (12, 31), (12, 35), (12, 41), (12, 49), (12, 53), (12, 57), (12, 61), (12, 83), (12, 87), (12, 91), (22, 11), (22, 23), (22, 31), (22, 35), (22, 41), (22, 49), (22, 53), (22, 57), (22, 61), (22, 83), (22, 87), (22, 91), (30, 11), (30, 23), (30, 31), (30, 35), (30, 41), (30, 49), (30, 53), (30, 57), (30, 61), (30, 83), (30, 87), (30, 91), (34, 11), (34, 23), (34, 31), (34, 35), (34, 41), (34, 49), (34, 53), (34, 57), (34, 61), (34, 83), (34, 87), (34, 91), (40, 11), (40, 23), (40, 31), (40, 35), (40, 41), (40, 49), (40, 53), (40, 57), (40, 61), (40, 83), (40, 87), (40, 91), (48, 11), (48, 23), (48, 31), (48, 35), (48, 41), (48, 49), (48, 53), (48, 57), (48, 61), (48, 83), (48, 87), (48, 91), (52, 11), (52, 23), (52, 31), (52, 35), (52, 41), (52, 49), (52, 53), (52, 57), (52, 61), (52, 83), (52, 87), (52, 91), (56, 11), (56, 23), (56, 31), (56, 35), (56, 41), (56, 49), (56, 53), (56, 57), (56, 61), (56, 83), (56, 87), (56, 91), (60, 11), (60, 23), (60, 31), (60, 35), (60, 41), (60, 49), (60, 53), (60, 57), (60, 61), (60, 83), (60, 87), (60, 91), (84, 11), (84, 23), (84, 31), (84, 35), (84, 41), (84, 49), (84, 53), (84, 57), (84, 61), (84, 83), (84, 87), (84, 91), (88, 11), (88, 23), (88, 31), (88, 35), (88, 41), (88, 49), (88, 53), (88, 57), (88, 61), (88, 83), (88, 87), (88, 91), (92, 11), (92, 23), (92, 31), (92, 35), (92, 41), (92, 49), (92, 53), (92, 57), (92, 61), (92, 83), (92, 87), (92, 91)]\n",
      "Comparison       9.275918e+117\n",
      "intersection      1.516667e+01\n",
      "overlap           6.943062e-01\n",
      "Jaccard           3.883249e-01\n",
      "SMC               7.859848e-01\n",
      "lcs               1.583333e+00\n",
      "OmEuni            1.094958e+00\n",
      "OmEbi             2.689916e+00\n",
      "OdEuni            3.613312e+00\n",
      "OdEbi             4.582697e+00\n",
      "OdMuni            8.217750e-02\n",
      "OdMbi             2.104547e-01\n",
      "OmEdMuni          5.938631e-02\n",
      "OmEdMbi           1.648724e-01\n",
      "OmEdMmEMuni       6.075774e-02\n",
      "OmEdMmEMbi        1.731900e-01\n",
      "pairedFreq        2.689916e+00\n",
      "ITR2              2.104547e-01\n",
      "ARC2              1.731900e-01\n",
      "editdist          1.183940e-01\n",
      "mod_editdist      1.215991e-01\n",
      "editdist_IDST     1.215991e-01\n",
      "editdist_IDS      1.183940e-01\n",
      "editdist_ID       2.248775e-01\n",
      "editdist_IDT      2.422592e-01\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "normalGroup2\n",
      "SN2 [6, 8, 18, 70, 74, 78, 82, 86, 90]\n",
      "groupSN [6, 8, 18, 70, 74, 78, 82, 86, 90, 5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "average_recalled_t_Group1= 24.88888888888889\n",
      "average_correct_recalled_t_Group1= 22.555555555555557\n",
      "average_intrusion_recalled_t_Group1= 2.3333333333333335\n",
      "average_recalled_t_Group2= 44.44444444444444\n",
      "average_correct_recalled_t_Group2= 32.44444444444444\n",
      "average_intrusion_recalled_t_Group2= 12.0\n",
      "[(5, 6), (5, 8), (5, 18), (5, 70), (5, 74), (5, 78), (5, 82), (5, 86), (5, 90), (7, 6), (7, 8), (7, 18), (7, 70), (7, 74), (7, 78), (7, 82), (7, 86), (7, 90), (17, 6), (17, 8), (17, 18), (17, 70), (17, 74), (17, 78), (17, 82), (17, 86), (17, 90), (69, 6), (69, 8), (69, 18), (69, 70), (69, 74), (69, 78), (69, 82), (69, 86), (69, 90), (73, 6), (73, 8), (73, 18), (73, 70), (73, 74), (73, 78), (73, 82), (73, 86), (73, 90), (77, 6), (77, 8), (77, 18), (77, 70), (77, 74), (77, 78), (77, 82), (77, 86), (77, 90), (81, 6), (81, 8), (81, 18), (81, 70), (81, 74), (81, 78), (81, 82), (81, 86), (81, 90), (85, 6), (85, 8), (85, 18), (85, 70), (85, 74), (85, 78), (85, 82), (85, 86), (85, 90), (89, 6), (89, 8), (89, 18), (89, 70), (89, 74), (89, 78), (89, 82), (89, 86), (89, 90)]\n",
      "Comparison       1.225678e+88\n",
      "intersection     1.544444e+01\n",
      "overlap          6.391668e-01\n",
      "Jaccard          3.348681e-01\n",
      "SMC              7.323232e-01\n",
      "lcs              2.000000e+00\n",
      "OmEuni           1.289634e+00\n",
      "OmEbi            1.912602e+00\n",
      "OdEuni           5.439984e+00\n",
      "OdEbi            4.215639e+00\n",
      "OdMuni           1.007037e-01\n",
      "OdMbi            1.553199e-01\n",
      "OmEdMuni         8.407826e-02\n",
      "OmEdMbi          1.220691e-01\n",
      "OmEdMmEMuni      8.572111e-02\n",
      "OmEdMmEMbi       1.269055e-01\n",
      "pairedFreq       1.912602e+00\n",
      "ITR2             1.553199e-01\n",
      "ARC2             1.269055e-01\n",
      "editdist         9.819347e-02\n",
      "mod_editdist     9.819347e-02\n",
      "editdist_IDST    9.819347e-02\n",
      "editdist_IDS     9.819347e-02\n",
      "editdist_ID      1.814334e-01\n",
      "editdist_IDT     1.901774e-01\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# RESEARCH QUESTION 1: Typical comparisons\n",
    "# similarity(df, collab1, bias1, order1, phase1, collab2, bias2, order2, phase2, TrueCollab=True, Self=True)\n",
    "# Reminder: Experiment 3 had half of the lists longer than the other half. Order in experiment 3 means which lists was longer. \n",
    "# Reminder cntd.: Always one biased and one unbiased person collaborated. Participants always collaborated with a person with the same order (two consecutive numbers).\n",
    "# Nominal Pre\n",
    "similarity(df, 0,0,0,1,0,1,0,1,False,False) #unbiased,order1 & biased,order1\n",
    "similarity(df, 0,0,1,1,0,1,1,1,False,False) #unbiased,order2 & biased,order2\n",
    "# Nominal collaborative Pre\n",
    "similarity(df, 1,1,1,1,1,0,1,1,False,False) #biased,order2 & unbiased,order2\n",
    "similarity(df, 1,1,0,1,1,0,0,1,False,False) #biased,order1 & unbiased,order1\n",
    "# Collaborative Pre\n",
    "similarity(df, 1,1,1,1,1,0,1,1,True,False) #biased,order2 & unbiased,order2\n",
    "similarity(df, 1,1,0,1,1,0,0,1,True,False) #biased,order1 & unbiased,order1\n",
    "# Nominal Post\n",
    "similarity(df, 0,0,0,3,0,1,0,3,False,False) #unbiased,order1 & biased,order1\n",
    "similarity(df, 0,0,1,3,0,1,1,3,False,False) #unbiased,order2 & biased,order2\n",
    "# Nominal collaborative Post\n",
    "similarity(df, 1,1,1,3,1,0,1,3,False,False) #biased,order2 & unbiased,order2\n",
    "similarity(df, 1,1,0,3,1,0,0,3,False,False) #biased,order1 & unbiased,order1\n",
    "# Collaborative Post\n",
    "similarity(df, 1,1,1,3,1,0,1,3,True,False) #biased,order2 & unbiased,order2\n",
    "similarity(df, 1,1,0,3,1,0,0,3,True,False) #biased,order1 & unbiased,order1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalGroup1\n",
      "SN1 [6, 8, 18, 70, 74, 78, 82, 86, 90]\n",
      "normalGroup2\n",
      "SN2 [6, 8, 18, 70, 74, 78, 82, 86, 90]\n",
      "groupSN [6, 8, 18, 70, 74, 78, 82, 86, 90, 6, 8, 18, 70, 74, 78, 82, 86, 90]\n",
      "average_recalled_t_Group1= 29.555555555555557\n",
      "average_correct_recalled_t_Group1= 26.0\n",
      "average_intrusion_recalled_t_Group1= 3.5555555555555554\n",
      "average_recalled_t_Group2= 44.44444444444444\n",
      "average_correct_recalled_t_Group2= 32.44444444444444\n",
      "average_intrusion_recalled_t_Group2= 12.0\n",
      "[(6, 6), (6, 8), (6, 18), (6, 70), (6, 74), (6, 78), (6, 82), (6, 86), (6, 90), (8, 6), (8, 8), (8, 18), (8, 70), (8, 74), (8, 78), (8, 82), (8, 86), (8, 90), (18, 6), (18, 8), (18, 18), (18, 70), (18, 74), (18, 78), (18, 82), (18, 86), (18, 90), (70, 6), (70, 8), (70, 18), (70, 70), (70, 74), (70, 78), (70, 82), (70, 86), (70, 90), (74, 6), (74, 8), (74, 18), (74, 70), (74, 74), (74, 78), (74, 82), (74, 86), (74, 90), (78, 6), (78, 8), (78, 18), (78, 70), (78, 74), (78, 78), (78, 82), (78, 86), (78, 90), (82, 6), (82, 8), (82, 18), (82, 70), (82, 74), (82, 78), (82, 82), (82, 86), (82, 90), (86, 6), (86, 8), (86, 18), (86, 70), (86, 74), (86, 78), (86, 82), (86, 86), (86, 90), (90, 6), (90, 8), (90, 18), (90, 70), (90, 74), (90, 78), (90, 82), (90, 86), (90, 90)]\n",
      "Comparison             inf\n",
      "intersection     22.777778\n",
      "overlap           0.755241\n",
      "Jaccard           0.507562\n",
      "SMC               0.853535\n",
      "lcs               2.666667\n",
      "OmEuni            3.102430\n",
      "OmEbi             5.315972\n",
      "OdEuni           25.947806\n",
      "OdEbi            15.883690\n",
      "OdMuni            0.182056\n",
      "OdMbi             0.303427\n",
      "OmEdMuni          0.162893\n",
      "OmEdMbi           0.265102\n",
      "OmEdMmEMuni       0.165121\n",
      "OmEdMmEMbi        0.274155\n",
      "pairedFreq        5.315972\n",
      "ITR2              0.303427\n",
      "ARC2              0.274155\n",
      "editdist          0.092500\n",
      "mod_editdist      0.092500\n",
      "editdist_IDST     0.092500\n",
      "editdist_IDS      0.092500\n",
      "editdist_ID       0.229978\n",
      "editdist_IDT      0.247114\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91]\n",
      "normalGroup2\n",
      "SN2 [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91]\n",
      "groupSN [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91, 11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91]\n",
      "average_recalled_t_Group1= 21.083333333333332\n",
      "average_correct_recalled_t_Group1= 20.5\n",
      "average_intrusion_recalled_t_Group1= 0.5833333333333334\n",
      "average_recalled_t_Group2= 27.083333333333332\n",
      "average_correct_recalled_t_Group2= 26.083333333333332\n",
      "average_intrusion_recalled_t_Group2= 1.0\n",
      "[(11, 11), (11, 23), (11, 31), (11, 35), (11, 41), (11, 49), (11, 53), (11, 57), (11, 61), (11, 83), (11, 87), (11, 91), (23, 11), (23, 23), (23, 31), (23, 35), (23, 41), (23, 49), (23, 53), (23, 57), (23, 61), (23, 83), (23, 87), (23, 91), (31, 11), (31, 23), (31, 31), (31, 35), (31, 41), (31, 49), (31, 53), (31, 57), (31, 61), (31, 83), (31, 87), (31, 91), (35, 11), (35, 23), (35, 31), (35, 35), (35, 41), (35, 49), (35, 53), (35, 57), (35, 61), (35, 83), (35, 87), (35, 91), (41, 11), (41, 23), (41, 31), (41, 35), (41, 41), (41, 49), (41, 53), (41, 57), (41, 61), (41, 83), (41, 87), (41, 91), (49, 11), (49, 23), (49, 31), (49, 35), (49, 41), (49, 49), (49, 53), (49, 57), (49, 61), (49, 83), (49, 87), (49, 91), (53, 11), (53, 23), (53, 31), (53, 35), (53, 41), (53, 49), (53, 53), (53, 57), (53, 61), (53, 83), (53, 87), (53, 91), (57, 11), (57, 23), (57, 31), (57, 35), (57, 41), (57, 49), (57, 53), (57, 57), (57, 61), (57, 83), (57, 87), (57, 91), (61, 11), (61, 23), (61, 31), (61, 35), (61, 41), (61, 49), (61, 53), (61, 57), (61, 61), (61, 83), (61, 87), (61, 91), (83, 11), (83, 23), (83, 31), (83, 35), (83, 41), (83, 49), (83, 53), (83, 57), (83, 61), (83, 83), (83, 87), (83, 91), (87, 11), (87, 23), (87, 31), (87, 35), (87, 41), (87, 49), (87, 53), (87, 57), (87, 61), (87, 83), (87, 87), (87, 91), (91, 11), (91, 23), (91, 31), (91, 35), (91, 41), (91, 49), (91, 53), (91, 57), (91, 61), (91, 83), (91, 87), (91, 91)]\n",
      "Comparison             inf\n",
      "intersection     16.083333\n",
      "overlap           0.674264\n",
      "Jaccard           0.430792\n",
      "SMC               0.847538\n",
      "lcs               1.833333\n",
      "OmEuni            1.633924\n",
      "OmEbi             2.267848\n",
      "OdEuni            4.255250\n",
      "OdEbi             2.908392\n",
      "OdMuni            0.108231\n",
      "OdMbi             0.137141\n",
      "OmEdMuni          0.085386\n",
      "OmEdMbi           0.091450\n",
      "OmEdMmEMuni       0.087846\n",
      "OmEdMmEMbi        0.096202\n",
      "pairedFreq        2.267848\n",
      "ITR2              0.137141\n",
      "ARC2              0.096202\n",
      "editdist          0.104121\n",
      "mod_editdist      0.104121\n",
      "editdist_IDST     0.104121\n",
      "editdist_IDS      0.104121\n",
      "editdist_ID       0.245898\n",
      "editdist_IDT      0.251479\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "normalGroup2\n",
      "SN2 [5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "groupSN [5, 7, 17, 69, 73, 77, 81, 85, 89, 5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "average_recalled_t_Group1= 14.777777777777779\n",
      "average_correct_recalled_t_Group1= 13.777777777777779\n",
      "average_intrusion_recalled_t_Group1= 1.0\n",
      "average_recalled_t_Group2= 24.88888888888889\n",
      "average_correct_recalled_t_Group2= 22.555555555555557\n",
      "average_intrusion_recalled_t_Group2= 2.3333333333333335\n",
      "[(5, 5), (5, 7), (5, 17), (5, 69), (5, 73), (5, 77), (5, 81), (5, 85), (5, 89), (7, 5), (7, 7), (7, 17), (7, 69), (7, 73), (7, 77), (7, 81), (7, 85), (7, 89), (17, 5), (17, 7), (17, 17), (17, 69), (17, 73), (17, 77), (17, 81), (17, 85), (17, 89), (69, 5), (69, 7), (69, 17), (69, 69), (69, 73), (69, 77), (69, 81), (69, 85), (69, 89), (73, 5), (73, 7), (73, 17), (73, 69), (73, 73), (73, 77), (73, 81), (73, 85), (73, 89), (77, 5), (77, 7), (77, 17), (77, 69), (77, 73), (77, 77), (77, 81), (77, 85), (77, 89), (81, 5), (81, 7), (81, 17), (81, 69), (81, 73), (81, 77), (81, 81), (81, 85), (81, 89), (85, 5), (85, 7), (85, 17), (85, 69), (85, 73), (85, 77), (85, 81), (85, 85), (85, 89), (89, 5), (89, 7), (89, 17), (89, 69), (89, 73), (89, 77), (89, 81), (89, 85), (89, 89)]\n",
      "Comparison             inf\n",
      "intersection     11.666667\n",
      "overlap           0.774340\n",
      "Jaccard           0.413855\n",
      "SMC               0.857323\n",
      "lcs               1.888889\n",
      "OmEuni            0.886897\n",
      "OmEbi             1.884905\n",
      "OdEuni            4.249758\n",
      "OdEbi             4.258375\n",
      "OdMuni            0.140941\n",
      "OdMbi             0.259494\n",
      "OmEdMuni          0.108720\n",
      "OmEdMbi           0.195052\n",
      "OmEdMmEMuni       0.112594\n",
      "OmEdMmEMbi        0.208177\n",
      "pairedFreq        1.884905\n",
      "ITR2              0.259494\n",
      "ARC2              0.208177\n",
      "editdist          0.135262\n",
      "mod_editdist      0.135262\n",
      "editdist_IDST     0.135262\n",
      "editdist_IDS      0.135262\n",
      "editdist_ID       0.236249\n",
      "editdist_IDT      0.251982\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "normalGroup2\n",
      "SN2 [12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "groupSN [12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92, 12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "average_recalled_t_Group1= 17.416666666666668\n",
      "average_correct_recalled_t_Group1= 16.416666666666668\n",
      "average_intrusion_recalled_t_Group1= 1.0\n",
      "average_recalled_t_Group2= 24.583333333333332\n",
      "average_correct_recalled_t_Group2= 23.916666666666668\n",
      "average_intrusion_recalled_t_Group2= 0.6666666666666666\n",
      "[(12, 12), (12, 22), (12, 30), (12, 34), (12, 40), (12, 48), (12, 52), (12, 56), (12, 60), (12, 84), (12, 88), (12, 92), (22, 12), (22, 22), (22, 30), (22, 34), (22, 40), (22, 48), (22, 52), (22, 56), (22, 60), (22, 84), (22, 88), (22, 92), (30, 12), (30, 22), (30, 30), (30, 34), (30, 40), (30, 48), (30, 52), (30, 56), (30, 60), (30, 84), (30, 88), (30, 92), (34, 12), (34, 22), (34, 30), (34, 34), (34, 40), (34, 48), (34, 52), (34, 56), (34, 60), (34, 84), (34, 88), (34, 92), (40, 12), (40, 22), (40, 30), (40, 34), (40, 40), (40, 48), (40, 52), (40, 56), (40, 60), (40, 84), (40, 88), (40, 92), (48, 12), (48, 22), (48, 30), (48, 34), (48, 40), (48, 48), (48, 52), (48, 56), (48, 60), (48, 84), (48, 88), (48, 92), (52, 12), (52, 22), (52, 30), (52, 34), (52, 40), (52, 48), (52, 52), (52, 56), (52, 60), (52, 84), (52, 88), (52, 92), (56, 12), (56, 22), (56, 30), (56, 34), (56, 40), (56, 48), (56, 52), (56, 56), (56, 60), (56, 84), (56, 88), (56, 92), (60, 12), (60, 22), (60, 30), (60, 34), (60, 40), (60, 48), (60, 52), (60, 56), (60, 60), (60, 84), (60, 88), (60, 92), (84, 12), (84, 22), (84, 30), (84, 34), (84, 40), (84, 48), (84, 52), (84, 56), (84, 60), (84, 84), (84, 88), (84, 92), (88, 12), (88, 22), (88, 30), (88, 34), (88, 40), (88, 48), (88, 52), (88, 56), (88, 60), (88, 84), (88, 88), (88, 92), (92, 12), (92, 22), (92, 30), (92, 34), (92, 40), (92, 48), (92, 52), (92, 56), (92, 60), (92, 84), (92, 88), (92, 92)]\n",
      "Comparison             inf\n",
      "intersection     14.000000\n",
      "overlap           0.822008\n",
      "Jaccard           0.525004\n",
      "SMC               0.862689\n",
      "lcs               2.083333\n",
      "OmEuni            1.381559\n",
      "OmEbi             2.013118\n",
      "OdEuni            3.932596\n",
      "OdEbi             2.966318\n",
      "OdMuni            0.144118\n",
      "OdMbi             0.214726\n",
      "OmEdMuni          0.104501\n",
      "OmEdMbi           0.135493\n",
      "OmEdMmEMuni       0.109152\n",
      "OmEdMmEMbi        0.146748\n",
      "pairedFreq        2.013118\n",
      "ITR2              0.214726\n",
      "ARC2              0.146748\n",
      "editdist          0.161620\n",
      "mod_editdist      0.161620\n",
      "editdist_IDST     0.161620\n",
      "editdist_IDS      0.161620\n",
      "editdist_ID       0.302739\n",
      "editdist_IDT      0.317441\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "normalGroup2\n",
      "SN2 [6, 8, 18, 70, 74, 78, 82, 86, 90]\n",
      "groupSN [6, 8, 18, 70, 74, 78, 82, 86, 90, 5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "average_recalled_t_Group1= 14.777777777777779\n",
      "average_correct_recalled_t_Group1= 13.777777777777779\n",
      "average_intrusion_recalled_t_Group1= 1.0\n",
      "average_recalled_t_Group2= 44.44444444444444\n",
      "average_correct_recalled_t_Group2= 32.44444444444444\n",
      "average_intrusion_recalled_t_Group2= 12.0\n",
      "[(5, 6), (5, 8), (5, 18), (5, 70), (5, 74), (5, 78), (5, 82), (5, 86), (5, 90), (7, 6), (7, 8), (7, 18), (7, 70), (7, 74), (7, 78), (7, 82), (7, 86), (7, 90), (17, 6), (17, 8), (17, 18), (17, 70), (17, 74), (17, 78), (17, 82), (17, 86), (17, 90), (69, 6), (69, 8), (69, 18), (69, 70), (69, 74), (69, 78), (69, 82), (69, 86), (69, 90), (73, 6), (73, 8), (73, 18), (73, 70), (73, 74), (73, 78), (73, 82), (73, 86), (73, 90), (77, 6), (77, 8), (77, 18), (77, 70), (77, 74), (77, 78), (77, 82), (77, 86), (77, 90), (81, 6), (81, 8), (81, 18), (81, 70), (81, 74), (81, 78), (81, 82), (81, 86), (81, 90), (85, 6), (85, 8), (85, 18), (85, 70), (85, 74), (85, 78), (85, 82), (85, 86), (85, 90), (89, 6), (89, 8), (89, 18), (89, 70), (89, 74), (89, 78), (89, 82), (89, 86), (89, 90)]\n",
      "Comparison       1.223456e+88\n",
      "intersection     8.333333e+00\n",
      "overlap          6.053358e-01\n",
      "Jaccard          1.933442e-01\n",
      "SMC              6.755051e-01\n",
      "lcs              1.444444e+00\n",
      "OmEuni           3.150808e-01\n",
      "OmEbi            1.074606e+00\n",
      "OdEuni           2.723341e+00\n",
      "OdEbi            4.656447e+00\n",
      "OdMuni           5.291005e-02\n",
      "OdMbi            1.597884e-01\n",
      "OmEdMuni         3.702593e-02\n",
      "OmEdMbi          1.280201e-01\n",
      "OmEdMmEMuni      3.778798e-02\n",
      "OmEdMmEMbi       1.326708e-01\n",
      "pairedFreq       1.074606e+00\n",
      "ITR2             1.597884e-01\n",
      "ARC2             1.326708e-01\n",
      "editdist         8.963757e-02\n",
      "mod_editdist     9.256154e-02\n",
      "editdist_IDST    9.256154e-02\n",
      "editdist_IDS     8.963757e-02\n",
      "editdist_ID      1.636748e-01\n",
      "editdist_IDT     1.716886e-01\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "normalGroup2\n",
      "SN2 [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91]\n",
      "groupSN [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91, 12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "average_recalled_t_Group1= 17.416666666666668\n",
      "average_correct_recalled_t_Group1= 16.416666666666668\n",
      "average_intrusion_recalled_t_Group1= 1.0\n",
      "average_recalled_t_Group2= 27.083333333333332\n",
      "average_correct_recalled_t_Group2= 26.083333333333332\n",
      "average_intrusion_recalled_t_Group2= 1.0\n",
      "[(12, 11), (12, 23), (12, 31), (12, 35), (12, 41), (12, 49), (12, 53), (12, 57), (12, 61), (12, 83), (12, 87), (12, 91), (22, 11), (22, 23), (22, 31), (22, 35), (22, 41), (22, 49), (22, 53), (22, 57), (22, 61), (22, 83), (22, 87), (22, 91), (30, 11), (30, 23), (30, 31), (30, 35), (30, 41), (30, 49), (30, 53), (30, 57), (30, 61), (30, 83), (30, 87), (30, 91), (34, 11), (34, 23), (34, 31), (34, 35), (34, 41), (34, 49), (34, 53), (34, 57), (34, 61), (34, 83), (34, 87), (34, 91), (40, 11), (40, 23), (40, 31), (40, 35), (40, 41), (40, 49), (40, 53), (40, 57), (40, 61), (40, 83), (40, 87), (40, 91), (48, 11), (48, 23), (48, 31), (48, 35), (48, 41), (48, 49), (48, 53), (48, 57), (48, 61), (48, 83), (48, 87), (48, 91), (52, 11), (52, 23), (52, 31), (52, 35), (52, 41), (52, 49), (52, 53), (52, 57), (52, 61), (52, 83), (52, 87), (52, 91), (56, 11), (56, 23), (56, 31), (56, 35), (56, 41), (56, 49), (56, 53), (56, 57), (56, 61), (56, 83), (56, 87), (56, 91), (60, 11), (60, 23), (60, 31), (60, 35), (60, 41), (60, 49), (60, 53), (60, 57), (60, 61), (60, 83), (60, 87), (60, 91), (84, 11), (84, 23), (84, 31), (84, 35), (84, 41), (84, 49), (84, 53), (84, 57), (84, 61), (84, 83), (84, 87), (84, 91), (88, 11), (88, 23), (88, 31), (88, 35), (88, 41), (88, 49), (88, 53), (88, 57), (88, 61), (88, 83), (88, 87), (88, 91), (92, 11), (92, 23), (92, 31), (92, 35), (92, 41), (92, 49), (92, 53), (92, 57), (92, 61), (92, 83), (92, 87), (92, 91)]\n",
      "Comparison       9.259251e+117\n",
      "intersection      9.083333e+00\n",
      "overlap           5.598749e-01\n",
      "Jaccard           2.509416e-01\n",
      "SMC               7.329545e-01\n",
      "lcs               1.333333e+00\n",
      "OmEuni            2.435384e-01\n",
      "OmEbi             8.204101e-01\n",
      "OdEuni            2.100430e+00\n",
      "OdEbi             3.380369e+00\n",
      "OdMuni            3.978175e-02\n",
      "OdMbi             1.426948e-01\n",
      "OmEdMuni          1.963260e-02\n",
      "OmEdMbi           1.023965e-01\n",
      "OmEdMmEMuni       1.995570e-02\n",
      "OmEdMmEMbi        1.069428e-01\n",
      "pairedFreq        8.204101e-01\n",
      "ITR2              1.426948e-01\n",
      "ARC2              1.069428e-01\n",
      "editdist          7.648699e-02\n",
      "mod_editdist      7.648699e-02\n",
      "editdist_IDST     7.648699e-02\n",
      "editdist_IDS      7.648699e-02\n",
      "editdist_ID       1.586860e-01\n",
      "editdist_IDT      1.658041e-01\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [6, 8, 18, 70, 74, 78, 82, 86, 90]\n",
      "normalGroup2\n",
      "SN2 [5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "groupSN [5, 7, 17, 69, 73, 77, 81, 85, 89, 6, 8, 18, 70, 74, 78, 82, 86, 90]\n",
      "average_recalled_t_Group1= 29.555555555555557\n",
      "average_correct_recalled_t_Group1= 26.0\n",
      "average_intrusion_recalled_t_Group1= 3.5555555555555554\n",
      "average_recalled_t_Group2= 24.88888888888889\n",
      "average_correct_recalled_t_Group2= 22.555555555555557\n",
      "average_intrusion_recalled_t_Group2= 2.3333333333333335\n",
      "[(6, 5), (6, 7), (6, 17), (6, 69), (6, 73), (6, 77), (6, 81), (6, 85), (6, 89), (8, 5), (8, 7), (8, 17), (8, 69), (8, 73), (8, 77), (8, 81), (8, 85), (8, 89), (18, 5), (18, 7), (18, 17), (18, 69), (18, 73), (18, 77), (18, 81), (18, 85), (18, 89), (70, 5), (70, 7), (70, 17), (70, 69), (70, 73), (70, 77), (70, 81), (70, 85), (70, 89), (74, 5), (74, 7), (74, 17), (74, 69), (74, 73), (74, 77), (74, 81), (74, 85), (74, 89), (78, 5), (78, 7), (78, 17), (78, 69), (78, 73), (78, 77), (78, 81), (78, 85), (78, 89), (82, 5), (82, 7), (82, 17), (82, 69), (82, 73), (82, 77), (82, 81), (82, 85), (82, 89), (86, 5), (86, 7), (86, 17), (86, 69), (86, 73), (86, 77), (86, 81), (86, 85), (86, 89), (90, 5), (90, 7), (90, 17), (90, 69), (90, 73), (90, 77), (90, 81), (90, 85), (90, 89)]\n",
      "Comparison       1.112346e+88\n",
      "intersection     1.133333e+01\n",
      "overlap          4.825752e-01\n",
      "Jaccard          2.647510e-01\n",
      "SMC              7.146465e-01\n",
      "lcs              1.000000e+00\n",
      "OmEuni          -7.127123e-02\n",
      "OmEbi            7.463464e-01\n",
      "OdEuni           3.664216e-01\n",
      "OdEbi            2.433808e+00\n",
      "OdMuni           6.944444e-03\n",
      "OdMbi            8.543617e-02\n",
      "OmEdMuni        -8.302428e-03\n",
      "OmEdMbi          5.494242e-02\n",
      "OmEdMmEMuni     -8.441829e-03\n",
      "OmEdMmEMbi       5.701109e-02\n",
      "pairedFreq       7.463464e-01\n",
      "ITR2             8.543617e-02\n",
      "ARC2             5.701109e-02\n",
      "editdist         8.565917e-02\n",
      "mod_editdist     8.565917e-02\n",
      "editdist_IDST    8.565917e-02\n",
      "editdist_IDS     8.565917e-02\n",
      "editdist_ID      1.648729e-01\n",
      "editdist_IDT     1.748116e-01\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91]\n",
      "normalGroup2\n",
      "SN2 [12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "groupSN [12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92, 11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91]\n",
      "average_recalled_t_Group1= 21.083333333333332\n",
      "average_correct_recalled_t_Group1= 20.5\n",
      "average_intrusion_recalled_t_Group1= 0.5833333333333334\n",
      "average_recalled_t_Group2= 24.583333333333332\n",
      "average_correct_recalled_t_Group2= 23.916666666666668\n",
      "average_intrusion_recalled_t_Group2= 0.6666666666666666\n",
      "[(11, 12), (11, 22), (11, 30), (11, 34), (11, 40), (11, 48), (11, 52), (11, 56), (11, 60), (11, 84), (11, 88), (11, 92), (23, 12), (23, 22), (23, 30), (23, 34), (23, 40), (23, 48), (23, 52), (23, 56), (23, 60), (23, 84), (23, 88), (23, 92), (31, 12), (31, 22), (31, 30), (31, 34), (31, 40), (31, 48), (31, 52), (31, 56), (31, 60), (31, 84), (31, 88), (31, 92), (35, 12), (35, 22), (35, 30), (35, 34), (35, 40), (35, 48), (35, 52), (35, 56), (35, 60), (35, 84), (35, 88), (35, 92), (41, 12), (41, 22), (41, 30), (41, 34), (41, 40), (41, 48), (41, 52), (41, 56), (41, 60), (41, 84), (41, 88), (41, 92), (49, 12), (49, 22), (49, 30), (49, 34), (49, 40), (49, 48), (49, 52), (49, 56), (49, 60), (49, 84), (49, 88), (49, 92), (53, 12), (53, 22), (53, 30), (53, 34), (53, 40), (53, 48), (53, 52), (53, 56), (53, 60), (53, 84), (53, 88), (53, 92), (57, 12), (57, 22), (57, 30), (57, 34), (57, 40), (57, 48), (57, 52), (57, 56), (57, 60), (57, 84), (57, 88), (57, 92), (61, 12), (61, 22), (61, 30), (61, 34), (61, 40), (61, 48), (61, 52), (61, 56), (61, 60), (61, 84), (61, 88), (61, 92), (83, 12), (83, 22), (83, 30), (83, 34), (83, 40), (83, 48), (83, 52), (83, 56), (83, 60), (83, 84), (83, 88), (83, 92), (87, 12), (87, 22), (87, 30), (87, 34), (87, 40), (87, 48), (87, 52), (87, 56), (87, 60), (87, 84), (87, 88), (87, 92), (91, 12), (91, 22), (91, 30), (91, 34), (91, 40), (91, 48), (91, 52), (91, 56), (91, 60), (91, 84), (91, 88), (91, 92)]\n",
      "Comparison       8.425926e+117\n",
      "intersection      1.025000e+01\n",
      "overlap           5.512349e-01\n",
      "Jaccard           2.672574e-01\n",
      "SMC               7.386364e-01\n",
      "lcs               1.416667e+00\n",
      "OmEuni            6.521609e-01\n",
      "OmEbi             1.220989e+00\n",
      "OdEuni            2.586672e+00\n",
      "OdEbi             2.538096e+00\n",
      "OdMuni            5.015415e-02\n",
      "OdMbi             9.040153e-02\n",
      "OmEdMuni          2.859327e-02\n",
      "OmEdMbi           4.727978e-02\n",
      "OmEdMmEMuni       2.898670e-02\n",
      "OmEdMmEMbi        4.809790e-02\n",
      "pairedFreq        1.220989e+00\n",
      "ITR2              9.040153e-02\n",
      "ARC2              4.809790e-02\n",
      "editdist          8.204371e-02\n",
      "mod_editdist      8.204371e-02\n",
      "editdist_IDST     8.204371e-02\n",
      "editdist_IDS      8.204371e-02\n",
      "editdist_ID       1.757838e-01\n",
      "editdist_IDT      1.794317e-01\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "normalGroup2\n",
      "SN2 [6, 8, 18, 70, 74, 78, 82, 86, 90]\n",
      "groupSN [6, 8, 18, 70, 74, 78, 82, 86, 90, 5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "average_recalled_t_Group1= 14.777777777777779\n",
      "average_correct_recalled_t_Group1= 13.777777777777779\n",
      "average_intrusion_recalled_t_Group1= 1.0\n",
      "average_recalled_t_Group2= 44.44444444444444\n",
      "average_correct_recalled_t_Group2= 32.44444444444444\n",
      "average_intrusion_recalled_t_Group2= 12.0\n",
      "[(5, 6), (5, 8), (5, 18), (5, 70), (5, 74), (5, 78), (5, 82), (5, 86), (5, 90), (7, 6), (7, 8), (7, 18), (7, 70), (7, 74), (7, 78), (7, 82), (7, 86), (7, 90), (17, 6), (17, 8), (17, 18), (17, 70), (17, 74), (17, 78), (17, 82), (17, 86), (17, 90), (69, 6), (69, 8), (69, 18), (69, 70), (69, 74), (69, 78), (69, 82), (69, 86), (69, 90), (73, 6), (73, 8), (73, 18), (73, 70), (73, 74), (73, 78), (73, 82), (73, 86), (73, 90), (77, 6), (77, 8), (77, 18), (77, 70), (77, 74), (77, 78), (77, 82), (77, 86), (77, 90), (81, 6), (81, 8), (81, 18), (81, 70), (81, 74), (81, 78), (81, 82), (81, 86), (81, 90), (85, 6), (85, 8), (85, 18), (85, 70), (85, 74), (85, 78), (85, 82), (85, 86), (85, 90), (89, 6), (89, 8), (89, 18), (89, 70), (89, 74), (89, 78), (89, 82), (89, 86), (89, 90)]\n",
      "Comparison            inf\n",
      "intersection     6.972222\n",
      "overlap          0.467130\n",
      "Jaccard          0.152241\n",
      "SMC              0.644571\n",
      "lcs              1.152778\n",
      "OmEuni           0.094818\n",
      "OmEbi            0.425747\n",
      "OdEuni           1.570094\n",
      "OdEbi            3.056519\n",
      "OdMuni           0.021908\n",
      "OdMbi            0.080486\n",
      "OmEdMuni         0.009812\n",
      "OmEdMbi          0.056294\n",
      "OmEdMmEMuni      0.009955\n",
      "OmEdMmEMbi       0.057932\n",
      "pairedFreq       0.425747\n",
      "ITR2             0.080486\n",
      "ARC2             0.057932\n",
      "editdist         0.069441\n",
      "mod_editdist     0.069441\n",
      "editdist_IDST    0.069441\n",
      "editdist_IDS     0.069441\n",
      "editdist_ID      0.127945\n",
      "editdist_IDT     0.133415\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "normalGroup2\n",
      "SN2 [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91]\n",
      "groupSN [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91, 12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "average_recalled_t_Group1= 17.416666666666668\n",
      "average_correct_recalled_t_Group1= 16.416666666666668\n",
      "average_intrusion_recalled_t_Group1= 1.0\n",
      "average_recalled_t_Group2= 27.083333333333332\n",
      "average_correct_recalled_t_Group2= 26.083333333333332\n",
      "average_intrusion_recalled_t_Group2= 1.0\n",
      "[(12, 11), (12, 23), (12, 31), (12, 35), (12, 41), (12, 49), (12, 53), (12, 57), (12, 61), (12, 83), (12, 87), (12, 91), (22, 11), (22, 23), (22, 31), (22, 35), (22, 41), (22, 49), (22, 53), (22, 57), (22, 61), (22, 83), (22, 87), (22, 91), (30, 11), (30, 23), (30, 31), (30, 35), (30, 41), (30, 49), (30, 53), (30, 57), (30, 61), (30, 83), (30, 87), (30, 91), (34, 11), (34, 23), (34, 31), (34, 35), (34, 41), (34, 49), (34, 53), (34, 57), (34, 61), (34, 83), (34, 87), (34, 91), (40, 11), (40, 23), (40, 31), (40, 35), (40, 41), (40, 49), (40, 53), (40, 57), (40, 61), (40, 83), (40, 87), (40, 91), (48, 11), (48, 23), (48, 31), (48, 35), (48, 41), (48, 49), (48, 53), (48, 57), (48, 61), (48, 83), (48, 87), (48, 91), (52, 11), (52, 23), (52, 31), (52, 35), (52, 41), (52, 49), (52, 53), (52, 57), (52, 61), (52, 83), (52, 87), (52, 91), (56, 11), (56, 23), (56, 31), (56, 35), (56, 41), (56, 49), (56, 53), (56, 57), (56, 61), (56, 83), (56, 87), (56, 91), (60, 11), (60, 23), (60, 31), (60, 35), (60, 41), (60, 49), (60, 53), (60, 57), (60, 61), (60, 83), (60, 87), (60, 91), (84, 11), (84, 23), (84, 31), (84, 35), (84, 41), (84, 49), (84, 53), (84, 57), (84, 61), (84, 83), (84, 87), (84, 91), (88, 11), (88, 23), (88, 31), (88, 35), (88, 41), (88, 49), (88, 53), (88, 57), (88, 61), (88, 83), (88, 87), (88, 91), (92, 11), (92, 23), (92, 31), (92, 35), (92, 41), (92, 49), (92, 53), (92, 57), (92, 61), (92, 83), (92, 87), (92, 91)]\n",
      "Comparison            inf\n",
      "intersection     6.568182\n",
      "overlap          0.395591\n",
      "Jaccard          0.159433\n",
      "SMC              0.677342\n",
      "lcs              1.181818\n",
      "OmEuni           0.216995\n",
      "OmEbi            0.426413\n",
      "OdEuni           2.604027\n",
      "OdEbi            2.909891\n",
      "OdMuni           0.038728\n",
      "OdMbi            0.082535\n",
      "OmEdMuni         0.025580\n",
      "OmEdMbi          0.056238\n",
      "OmEdMmEMuni      0.026008\n",
      "OmEdMmEMbi       0.057923\n",
      "pairedFreq       0.426413\n",
      "ITR2             0.082535\n",
      "ARC2             0.057923\n",
      "editdist         0.053387\n",
      "mod_editdist     0.053387\n",
      "editdist_IDST    0.053387\n",
      "editdist_IDS     0.053387\n",
      "editdist_ID      0.127273\n",
      "editdist_IDT     0.131509\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [6, 8, 18, 70, 74, 78, 82, 86, 90]\n",
      "normalGroup2\n",
      "SN2 [5, 7, 17, 69, 73, 77, 81, 85, 89]\n",
      "groupSN [5, 7, 17, 69, 73, 77, 81, 85, 89, 6, 8, 18, 70, 74, 78, 82, 86, 90]\n",
      "average_recalled_t_Group1= 29.555555555555557\n",
      "average_correct_recalled_t_Group1= 26.0\n",
      "average_intrusion_recalled_t_Group1= 3.5555555555555554\n",
      "average_recalled_t_Group2= 24.88888888888889\n",
      "average_correct_recalled_t_Group2= 22.555555555555557\n",
      "average_intrusion_recalled_t_Group2= 2.3333333333333335\n",
      "[(6, 5), (6, 7), (6, 17), (6, 69), (6, 73), (6, 77), (6, 81), (6, 85), (6, 89), (8, 5), (8, 7), (8, 17), (8, 69), (8, 73), (8, 77), (8, 81), (8, 85), (8, 89), (18, 5), (18, 7), (18, 17), (18, 69), (18, 73), (18, 77), (18, 81), (18, 85), (18, 89), (70, 5), (70, 7), (70, 17), (70, 69), (70, 73), (70, 77), (70, 81), (70, 85), (70, 89), (74, 5), (74, 7), (74, 17), (74, 69), (74, 73), (74, 77), (74, 81), (74, 85), (74, 89), (78, 5), (78, 7), (78, 17), (78, 69), (78, 73), (78, 77), (78, 81), (78, 85), (78, 89), (82, 5), (82, 7), (82, 17), (82, 69), (82, 73), (82, 77), (82, 81), (82, 85), (82, 89), (86, 5), (86, 7), (86, 17), (86, 69), (86, 73), (86, 77), (86, 81), (86, 85), (86, 89), (90, 5), (90, 7), (90, 17), (90, 69), (90, 73), (90, 77), (90, 81), (90, 85), (90, 89)]\n",
      "Comparison            inf\n",
      "intersection     8.736111\n",
      "overlap          0.361952\n",
      "Jaccard          0.187361\n",
      "SMC              0.658144\n",
      "lcs              1.194444\n",
      "OmEuni           0.242199\n",
      "OmEbi            0.609399\n",
      "OdEuni           2.989109\n",
      "OdEbi            3.622320\n",
      "OdMuni           0.036892\n",
      "OdMbi            0.088415\n",
      "OmEdMuni         0.025704\n",
      "OmEdMbi          0.066038\n",
      "OmEdMmEMuni      0.026033\n",
      "OmEdMmEMbi       0.067745\n",
      "pairedFreq       0.609399\n",
      "ITR2             0.088415\n",
      "ARC2             0.067745\n",
      "editdist         0.055616\n",
      "mod_editdist     0.055616\n",
      "editdist_IDST    0.055616\n",
      "editdist_IDS     0.055616\n",
      "editdist_ID      0.138301\n",
      "editdist_IDT     0.143542\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91]\n",
      "normalGroup2\n",
      "SN2 [12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92]\n",
      "groupSN [12, 22, 30, 34, 40, 48, 52, 56, 60, 84, 88, 92, 11, 23, 31, 35, 41, 49, 53, 57, 61, 83, 87, 91]\n",
      "average_recalled_t_Group1= 21.083333333333332\n",
      "average_correct_recalled_t_Group1= 20.5\n",
      "average_intrusion_recalled_t_Group1= 0.5833333333333334\n",
      "average_recalled_t_Group2= 24.583333333333332\n",
      "average_correct_recalled_t_Group2= 23.916666666666668\n",
      "average_intrusion_recalled_t_Group2= 0.6666666666666666\n",
      "[(11, 12), (11, 22), (11, 30), (11, 34), (11, 40), (11, 48), (11, 52), (11, 56), (11, 60), (11, 84), (11, 88), (11, 92), (23, 12), (23, 22), (23, 30), (23, 34), (23, 40), (23, 48), (23, 52), (23, 56), (23, 60), (23, 84), (23, 88), (23, 92), (31, 12), (31, 22), (31, 30), (31, 34), (31, 40), (31, 48), (31, 52), (31, 56), (31, 60), (31, 84), (31, 88), (31, 92), (35, 12), (35, 22), (35, 30), (35, 34), (35, 40), (35, 48), (35, 52), (35, 56), (35, 60), (35, 84), (35, 88), (35, 92), (41, 12), (41, 22), (41, 30), (41, 34), (41, 40), (41, 48), (41, 52), (41, 56), (41, 60), (41, 84), (41, 88), (41, 92), (49, 12), (49, 22), (49, 30), (49, 34), (49, 40), (49, 48), (49, 52), (49, 56), (49, 60), (49, 84), (49, 88), (49, 92), (53, 12), (53, 22), (53, 30), (53, 34), (53, 40), (53, 48), (53, 52), (53, 56), (53, 60), (53, 84), (53, 88), (53, 92), (57, 12), (57, 22), (57, 30), (57, 34), (57, 40), (57, 48), (57, 52), (57, 56), (57, 60), (57, 84), (57, 88), (57, 92), (61, 12), (61, 22), (61, 30), (61, 34), (61, 40), (61, 48), (61, 52), (61, 56), (61, 60), (61, 84), (61, 88), (61, 92), (83, 12), (83, 22), (83, 30), (83, 34), (83, 40), (83, 48), (83, 52), (83, 56), (83, 60), (83, 84), (83, 88), (83, 92), (87, 12), (87, 22), (87, 30), (87, 34), (87, 40), (87, 48), (87, 52), (87, 56), (87, 60), (87, 84), (87, 88), (87, 92), (91, 12), (91, 22), (91, 30), (91, 34), (91, 40), (91, 48), (91, 52), (91, 56), (91, 60), (91, 84), (91, 88), (91, 92)]\n",
      "Comparison            inf\n",
      "intersection     7.000000\n",
      "overlap          0.401610\n",
      "Jaccard          0.169196\n",
      "SMC              0.667011\n",
      "lcs              1.287879\n",
      "OmEuni           0.356660\n",
      "OmEbi            0.576956\n",
      "OdEuni           3.939068\n",
      "OdEbi            3.337994\n",
      "OdMuni           0.055060\n",
      "OdMbi            0.094590\n",
      "OmEdMuni         0.041979\n",
      "OmEdMbi          0.068428\n",
      "OmEdMmEMuni      0.042609\n",
      "OmEdMmEMbi       0.070584\n",
      "pairedFreq       0.576956\n",
      "ITR2             0.094590\n",
      "ARC2             0.070584\n",
      "editdist         0.062109\n",
      "mod_editdist     0.062109\n",
      "editdist_IDST    0.062109\n",
      "editdist_IDS     0.062109\n",
      "editdist_ID      0.143179\n",
      "editdist_IDT     0.146727\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# RESEARCH QUESTION 2: Similarity of people before and after collaboration\n",
    "# Participant pre & post collab (so they needed to collab; therefore only 4 and not 8 comaprisons)\n",
    "similarity(df, 1,0,0,1,1,0,0,3,False,True)\n",
    "similarity(df, 1,0,1,1,1,0,1,3,False,True)\n",
    "similarity(df, 1,1,0,1,1,1,0,3,False,True)\n",
    "similarity(df, 1,1,1,1,1,1,1,3,False,True)\n",
    "# Partner pre & participant post collab\n",
    "similarity(df, 1,1,0,1,1,0,0,3,True,False) \n",
    "similarity(df, 1,1,1,1,1,0,1,3,True,False) \n",
    "similarity(df, 1,0,0,1,1,1,0,3,True,False) \n",
    "similarity(df, 1,0,1,1,1,1,1,3,True,False) \n",
    "# non-partner pre & participant post collab\n",
    "similarity(df, 1,1,0,1,1,0,0,3,False,False) \n",
    "similarity(df, 1,1,1,1,1,0,1,3,False,False) \n",
    "similarity(df, 1,0,0,1,1,1,0,3,False,False) \n",
    "similarity(df, 1,0,1,1,1,1,1,3,False,False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalGroup1\n",
      "SN1 [24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62]\n",
      "normalGroup2\n",
      "SN2 [24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62]\n",
      "groupSN [24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62, 24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62]\n",
      "average_recalled_t_Group1= 22.363636363636363\n",
      "average_correct_recalled_t_Group1= 21.636363636363637\n",
      "average_intrusion_recalled_t_Group1= 0.7272727272727273\n",
      "average_recalled_t_Group2= 21.818181818181817\n",
      "average_correct_recalled_t_Group2= 20.454545454545453\n",
      "average_intrusion_recalled_t_Group2= 1.3636363636363635\n",
      "[(24, 24), (24, 28), (24, 32), (24, 36), (24, 38), (24, 42), (24, 46), (24, 50), (24, 54), (24, 58), (24, 62), (28, 24), (28, 28), (28, 32), (28, 36), (28, 38), (28, 42), (28, 46), (28, 50), (28, 54), (28, 58), (28, 62), (32, 24), (32, 28), (32, 32), (32, 36), (32, 38), (32, 42), (32, 46), (32, 50), (32, 54), (32, 58), (32, 62), (36, 24), (36, 28), (36, 32), (36, 36), (36, 38), (36, 42), (36, 46), (36, 50), (36, 54), (36, 58), (36, 62), (38, 24), (38, 28), (38, 32), (38, 36), (38, 38), (38, 42), (38, 46), (38, 50), (38, 54), (38, 58), (38, 62), (42, 24), (42, 28), (42, 32), (42, 36), (42, 38), (42, 42), (42, 46), (42, 50), (42, 54), (42, 58), (42, 62), (46, 24), (46, 28), (46, 32), (46, 36), (46, 38), (46, 42), (46, 46), (46, 50), (46, 54), (46, 58), (46, 62), (50, 24), (50, 28), (50, 32), (50, 36), (50, 38), (50, 42), (50, 46), (50, 50), (50, 54), (50, 58), (50, 62), (54, 24), (54, 28), (54, 32), (54, 36), (54, 38), (54, 42), (54, 46), (54, 50), (54, 54), (54, 58), (54, 62), (58, 24), (58, 28), (58, 32), (58, 36), (58, 38), (58, 42), (58, 46), (58, 50), (58, 54), (58, 58), (58, 62), (62, 24), (62, 28), (62, 32), (62, 36), (62, 38), (62, 42), (62, 46), (62, 50), (62, 54), (62, 58), (62, 62)]\n",
      "Comparison             inf\n",
      "intersection     17.454545\n",
      "overlap           0.789350\n",
      "Jaccard           0.606320\n",
      "SMC               0.922521\n",
      "lcs               1.909091\n",
      "OmEuni            2.449992\n",
      "OmEbi             3.718165\n",
      "OdEuni            4.521093\n",
      "OdEbi             3.568243\n",
      "OdMuni            0.145359\n",
      "OdMbi             0.225706\n",
      "OmEdMuni          0.110032\n",
      "OmEdMbi           0.155053\n",
      "OmEdMmEMuni       0.113908\n",
      "OmEdMmEMbi        0.165479\n",
      "pairedFreq        3.718165\n",
      "ITR2              0.225706\n",
      "ARC2              0.165479\n",
      "editdist          0.150844\n",
      "mod_editdist      0.155556\n",
      "editdist_IDST     0.155556\n",
      "editdist_IDS      0.150844\n",
      "editdist_ID       0.322853\n",
      "editdist_IDT      0.336176\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [1, 3, 9, 15, 19, 67, 71, 75, 79]\n",
      "normalGroup2\n",
      "SN2 [1, 3, 9, 15, 19, 67, 71, 75, 79]\n",
      "groupSN [1, 3, 9, 15, 19, 67, 71, 75, 79, 1, 3, 9, 15, 19, 67, 71, 75, 79]\n",
      "average_recalled_t_Group1= 27.22222222222222\n",
      "average_correct_recalled_t_Group1= 26.333333333333332\n",
      "average_intrusion_recalled_t_Group1= 0.8888888888888888\n",
      "average_recalled_t_Group2= 27.333333333333332\n",
      "average_correct_recalled_t_Group2= 25.88888888888889\n",
      "average_intrusion_recalled_t_Group2= 1.4444444444444444\n",
      "[(1, 1), (1, 3), (1, 9), (1, 15), (1, 19), (1, 67), (1, 71), (1, 75), (1, 79), (3, 1), (3, 3), (3, 9), (3, 15), (3, 19), (3, 67), (3, 71), (3, 75), (3, 79), (9, 1), (9, 3), (9, 9), (9, 15), (9, 19), (9, 67), (9, 71), (9, 75), (9, 79), (15, 1), (15, 3), (15, 9), (15, 15), (15, 19), (15, 67), (15, 71), (15, 75), (15, 79), (19, 1), (19, 3), (19, 9), (19, 15), (19, 19), (19, 67), (19, 71), (19, 75), (19, 79), (67, 1), (67, 3), (67, 9), (67, 15), (67, 19), (67, 67), (67, 71), (67, 75), (67, 79), (71, 1), (71, 3), (71, 9), (71, 15), (71, 19), (71, 67), (71, 71), (71, 75), (71, 79), (75, 1), (75, 3), (75, 9), (75, 15), (75, 19), (75, 67), (75, 71), (75, 75), (75, 79), (79, 1), (79, 3), (79, 9), (79, 15), (79, 19), (79, 67), (79, 71), (79, 75), (79, 79)]\n",
      "Comparison             inf\n",
      "intersection     23.222222\n",
      "overlap           0.895407\n",
      "Jaccard           0.751146\n",
      "SMC               0.939394\n",
      "lcs               2.333333\n",
      "OmEuni            2.520352\n",
      "OmEbi             4.374038\n",
      "OdEuni            4.725476\n",
      "OdEbi             4.217362\n",
      "OdMuni            0.157528\n",
      "OdMbi             0.284456\n",
      "OmEdMuni          0.122169\n",
      "OmEdMbi           0.213740\n",
      "OmEdMmEMuni       0.126834\n",
      "OmEdMmEMbi        0.231040\n",
      "pairedFreq        4.374038\n",
      "ITR2              0.284456\n",
      "ARC2              0.231040\n",
      "editdist          0.119550\n",
      "mod_editdist      0.126495\n",
      "editdist_IDST     0.126495\n",
      "editdist_IDS      0.119550\n",
      "editdist_ID       0.343448\n",
      "editdist_IDT      0.364343\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [29, 33, 37, 39, 43, 47, 51, 55, 59, 93]\n",
      "normalGroup2\n",
      "SN2 [29, 33, 37, 39, 43, 47, 51, 55, 59, 93]\n",
      "groupSN [29, 33, 37, 39, 43, 47, 51, 55, 59, 93, 29, 33, 37, 39, 43, 47, 51, 55, 59, 93]\n",
      "average_recalled_t_Group1= 13.8\n",
      "average_correct_recalled_t_Group1= 12.6\n",
      "average_intrusion_recalled_t_Group1= 1.2\n",
      "average_recalled_t_Group2= 14.1\n",
      "average_correct_recalled_t_Group2= 12.7\n",
      "average_intrusion_recalled_t_Group2= 1.4\n",
      "[(29, 29), (29, 33), (29, 37), (29, 39), (29, 43), (29, 47), (29, 51), (29, 55), (29, 59), (29, 93), (33, 29), (33, 33), (33, 37), (33, 39), (33, 43), (33, 47), (33, 51), (33, 55), (33, 59), (33, 93), (37, 29), (37, 33), (37, 37), (37, 39), (37, 43), (37, 47), (37, 51), (37, 55), (37, 59), (37, 93), (39, 29), (39, 33), (39, 37), (39, 39), (39, 43), (39, 47), (39, 51), (39, 55), (39, 59), (39, 93), (43, 29), (43, 33), (43, 37), (43, 39), (43, 43), (43, 47), (43, 51), (43, 55), (43, 59), (43, 93), (47, 29), (47, 33), (47, 37), (47, 39), (47, 43), (47, 47), (47, 51), (47, 55), (47, 59), (47, 93), (51, 29), (51, 33), (51, 37), (51, 39), (51, 43), (51, 47), (51, 51), (51, 55), (51, 59), (51, 93), (55, 29), (55, 33), (55, 37), (55, 39), (55, 43), (55, 47), (55, 51), (55, 55), (55, 59), (55, 93), (59, 29), (59, 33), (59, 37), (59, 39), (59, 43), (59, 47), (59, 51), (59, 55), (59, 59), (59, 93), (93, 29), (93, 33), (93, 37), (93, 39), (93, 43), (93, 47), (93, 51), (93, 55), (93, 59), (93, 93)]\n",
      "Comparison             inf\n",
      "intersection     10.600000\n",
      "overlap           0.851522\n",
      "Jaccard           0.616894\n",
      "SMC               0.950000\n",
      "lcs               2.100000\n",
      "OmEuni            1.077680\n",
      "OmEbi             1.655360\n",
      "OdEuni            3.113981\n",
      "OdEbi             2.524684\n",
      "OdMuni            0.161515\n",
      "OdMbi             0.267803\n",
      "OmEdMuni          0.093797\n",
      "OmEdMbi           0.132367\n",
      "OmEdMmEMuni       0.096802\n",
      "OmEdMmEMbi        0.137154\n",
      "pairedFreq        1.655360\n",
      "ITR2              0.267803\n",
      "ARC2              0.137154\n",
      "editdist          0.215740\n",
      "mod_editdist      0.215740\n",
      "editdist_IDST     0.215740\n",
      "editdist_IDS      0.215740\n",
      "editdist_ID       0.397777\n",
      "editdist_IDT      0.413420\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80]\n",
      "normalGroup2\n",
      "SN2 [2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80]\n",
      "groupSN [2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80, 2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80]\n",
      "average_recalled_t_Group1= 17.454545454545453\n",
      "average_correct_recalled_t_Group1= 16.727272727272727\n",
      "average_intrusion_recalled_t_Group1= 0.7272727272727273\n",
      "average_recalled_t_Group2= 17.636363636363637\n",
      "average_correct_recalled_t_Group2= 16.818181818181817\n",
      "average_intrusion_recalled_t_Group2= 0.8181818181818182\n",
      "[(2, 2), (2, 4), (2, 10), (2, 16), (2, 26), (2, 64), (2, 66), (2, 68), (2, 72), (2, 76), (2, 80), (4, 2), (4, 4), (4, 10), (4, 16), (4, 26), (4, 64), (4, 66), (4, 68), (4, 72), (4, 76), (4, 80), (10, 2), (10, 4), (10, 10), (10, 16), (10, 26), (10, 64), (10, 66), (10, 68), (10, 72), (10, 76), (10, 80), (16, 2), (16, 4), (16, 10), (16, 16), (16, 26), (16, 64), (16, 66), (16, 68), (16, 72), (16, 76), (16, 80), (26, 2), (26, 4), (26, 10), (26, 16), (26, 26), (26, 64), (26, 66), (26, 68), (26, 72), (26, 76), (26, 80), (64, 2), (64, 4), (64, 10), (64, 16), (64, 26), (64, 64), (64, 66), (64, 68), (64, 72), (64, 76), (64, 80), (66, 2), (66, 4), (66, 10), (66, 16), (66, 26), (66, 64), (66, 66), (66, 68), (66, 72), (66, 76), (66, 80), (68, 2), (68, 4), (68, 10), (68, 16), (68, 26), (68, 64), (68, 66), (68, 68), (68, 72), (68, 76), (68, 80), (72, 2), (72, 4), (72, 10), (72, 16), (72, 26), (72, 64), (72, 66), (72, 68), (72, 72), (72, 76), (72, 80), (76, 2), (76, 4), (76, 10), (76, 16), (76, 26), (76, 64), (76, 66), (76, 68), (76, 72), (76, 76), (76, 80), (80, 2), (80, 4), (80, 10), (80, 16), (80, 26), (80, 64), (80, 66), (80, 68), (80, 72), (80, 76), (80, 80)]\n",
      "Comparison             inf\n",
      "intersection     14.181818\n",
      "overlap           0.883417\n",
      "Jaccard           0.698312\n",
      "SMC               0.939050\n",
      "lcs               1.727273\n",
      "OmEuni            1.104327\n",
      "OmEbi             1.208654\n",
      "OdEuni            2.700990\n",
      "OdEbi             1.920681\n",
      "OdMuni            0.105707\n",
      "OdMbi             0.149854\n",
      "OmEdMuni          0.049434\n",
      "OmEdMbi           0.037308\n",
      "OmEdMmEMuni       0.049268\n",
      "OmEdMmEMbi        0.029641\n",
      "pairedFreq        1.208654\n",
      "ITR2              0.149854\n",
      "ARC2              0.029641\n",
      "editdist          0.163069\n",
      "mod_editdist      0.167615\n",
      "editdist_IDST     0.167615\n",
      "editdist_IDS      0.163069\n",
      "editdist_ID       0.369951\n",
      "editdist_IDT      0.377878\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62]\n",
      "normalGroup2\n",
      "SN2 [24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62]\n",
      "groupSN [24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62, 24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62]\n",
      "average_recalled_t_Group1= 21.818181818181817\n",
      "average_correct_recalled_t_Group1= 20.454545454545453\n",
      "average_intrusion_recalled_t_Group1= 1.3636363636363635\n",
      "average_recalled_t_Group2= 26.272727272727273\n",
      "average_correct_recalled_t_Group2= 24.90909090909091\n",
      "average_intrusion_recalled_t_Group2= 1.3636363636363635\n",
      "[(24, 24), (24, 28), (24, 32), (24, 36), (24, 38), (24, 42), (24, 46), (24, 50), (24, 54), (24, 58), (24, 62), (28, 24), (28, 28), (28, 32), (28, 36), (28, 38), (28, 42), (28, 46), (28, 50), (28, 54), (28, 58), (28, 62), (32, 24), (32, 28), (32, 32), (32, 36), (32, 38), (32, 42), (32, 46), (32, 50), (32, 54), (32, 58), (32, 62), (36, 24), (36, 28), (36, 32), (36, 36), (36, 38), (36, 42), (36, 46), (36, 50), (36, 54), (36, 58), (36, 62), (38, 24), (38, 28), (38, 32), (38, 36), (38, 38), (38, 42), (38, 46), (38, 50), (38, 54), (38, 58), (38, 62), (42, 24), (42, 28), (42, 32), (42, 36), (42, 38), (42, 42), (42, 46), (42, 50), (42, 54), (42, 58), (42, 62), (46, 24), (46, 28), (46, 32), (46, 36), (46, 38), (46, 42), (46, 46), (46, 50), (46, 54), (46, 58), (46, 62), (50, 24), (50, 28), (50, 32), (50, 36), (50, 38), (50, 42), (50, 46), (50, 50), (50, 54), (50, 58), (50, 62), (54, 24), (54, 28), (54, 32), (54, 36), (54, 38), (54, 42), (54, 46), (54, 50), (54, 54), (54, 58), (54, 62), (58, 24), (58, 28), (58, 32), (58, 36), (58, 38), (58, 42), (58, 46), (58, 50), (58, 54), (58, 58), (58, 62), (62, 24), (62, 28), (62, 32), (62, 36), (62, 38), (62, 42), (62, 46), (62, 50), (62, 54), (62, 58), (62, 62)]\n",
      "Comparison             inf\n",
      "intersection     18.272727\n",
      "overlap           0.758535\n",
      "Jaccard           0.587050\n",
      "SMC               0.900826\n",
      "lcs               2.363636\n",
      "OmEuni            3.289104\n",
      "OmEbi             5.214572\n",
      "OdEuni            6.240475\n",
      "OdEbi             4.979113\n",
      "OdMuni            0.194018\n",
      "OdMbi             0.307793\n",
      "OmEdMuni          0.161383\n",
      "OmEdMbi           0.242525\n",
      "OmEdMmEMuni       0.167307\n",
      "OmEdMmEMbi        0.260493\n",
      "pairedFreq        5.214572\n",
      "ITR2              0.307793\n",
      "ARC2              0.260493\n",
      "editdist          0.166750\n",
      "mod_editdist      0.169347\n",
      "editdist_IDST     0.169347\n",
      "editdist_IDS      0.166750\n",
      "editdist_ID       0.302055\n",
      "editdist_IDT      0.330753\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [1, 3, 9, 15, 19, 67, 71, 75, 79]\n",
      "normalGroup2\n",
      "SN2 [1, 3, 9, 15, 19, 67, 71, 75, 79]\n",
      "groupSN [1, 3, 9, 15, 19, 67, 71, 75, 79, 1, 3, 9, 15, 19, 67, 71, 75, 79]\n",
      "average_recalled_t_Group1= 27.333333333333332\n",
      "average_correct_recalled_t_Group1= 25.88888888888889\n",
      "average_intrusion_recalled_t_Group1= 1.4444444444444444\n",
      "average_recalled_t_Group2= 31.22222222222222\n",
      "average_correct_recalled_t_Group2= 30.22222222222222\n",
      "average_intrusion_recalled_t_Group2= 1.0\n",
      "[(1, 1), (1, 3), (1, 9), (1, 15), (1, 19), (1, 67), (1, 71), (1, 75), (1, 79), (3, 1), (3, 3), (3, 9), (3, 15), (3, 19), (3, 67), (3, 71), (3, 75), (3, 79), (9, 1), (9, 3), (9, 9), (9, 15), (9, 19), (9, 67), (9, 71), (9, 75), (9, 79), (15, 1), (15, 3), (15, 9), (15, 15), (15, 19), (15, 67), (15, 71), (15, 75), (15, 79), (19, 1), (19, 3), (19, 9), (19, 15), (19, 19), (19, 67), (19, 71), (19, 75), (19, 79), (67, 1), (67, 3), (67, 9), (67, 15), (67, 19), (67, 67), (67, 71), (67, 75), (67, 79), (71, 1), (71, 3), (71, 9), (71, 15), (71, 19), (71, 67), (71, 71), (71, 75), (71, 79), (75, 1), (75, 3), (75, 9), (75, 15), (75, 19), (75, 67), (75, 71), (75, 75), (75, 79), (79, 1), (79, 3), (79, 9), (79, 15), (79, 19), (79, 67), (79, 71), (79, 75), (79, 79)]\n",
      "Comparison             inf\n",
      "intersection     23.333333\n",
      "overlap           0.875285\n",
      "Jaccard           0.653858\n",
      "SMC               0.897727\n",
      "lcs               2.444444\n",
      "OmEuni            4.621703\n",
      "OmEbi             6.132295\n",
      "OdEuni            8.529498\n",
      "OdEbi             6.032681\n",
      "OdMuni            0.213737\n",
      "OdMbi             0.318531\n",
      "OmEdMuni          0.183612\n",
      "OmEdMbi           0.258281\n",
      "OmEdMmEMuni       0.188626\n",
      "OmEdMmEMbi        0.273953\n",
      "pairedFreq        6.132295\n",
      "ITR2              0.318531\n",
      "ARC2              0.273953\n",
      "editdist          0.143812\n",
      "mod_editdist      0.143812\n",
      "editdist_IDST     0.143812\n",
      "editdist_IDS      0.143812\n",
      "editdist_ID       0.319666\n",
      "editdist_IDT      0.338249\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [29, 33, 37, 39, 43, 47, 51, 55, 59, 93]\n",
      "normalGroup2\n",
      "SN2 [29, 33, 37, 39, 43, 47, 51, 55, 59, 93]\n",
      "groupSN [29, 33, 37, 39, 43, 47, 51, 55, 59, 93, 29, 33, 37, 39, 43, 47, 51, 55, 59, 93]\n",
      "average_recalled_t_Group1= 14.1\n",
      "average_correct_recalled_t_Group1= 12.7\n",
      "average_intrusion_recalled_t_Group1= 1.4\n",
      "average_recalled_t_Group2= 17.2\n",
      "average_correct_recalled_t_Group2= 14.9\n",
      "average_intrusion_recalled_t_Group2= 2.3\n",
      "[(29, 29), (29, 33), (29, 37), (29, 39), (29, 43), (29, 47), (29, 51), (29, 55), (29, 59), (29, 93), (33, 29), (33, 33), (33, 37), (33, 39), (33, 43), (33, 47), (33, 51), (33, 55), (33, 59), (33, 93), (37, 29), (37, 33), (37, 37), (37, 39), (37, 43), (37, 47), (37, 51), (37, 55), (37, 59), (37, 93), (39, 29), (39, 33), (39, 37), (39, 39), (39, 43), (39, 47), (39, 51), (39, 55), (39, 59), (39, 93), (43, 29), (43, 33), (43, 37), (43, 39), (43, 43), (43, 47), (43, 51), (43, 55), (43, 59), (43, 93), (47, 29), (47, 33), (47, 37), (47, 39), (47, 43), (47, 47), (47, 51), (47, 55), (47, 59), (47, 93), (51, 29), (51, 33), (51, 37), (51, 39), (51, 43), (51, 47), (51, 51), (51, 55), (51, 59), (51, 93), (55, 29), (55, 33), (55, 37), (55, 39), (55, 43), (55, 47), (55, 51), (55, 55), (55, 59), (55, 93), (59, 29), (59, 33), (59, 37), (59, 39), (59, 43), (59, 47), (59, 51), (59, 55), (59, 59), (59, 93), (93, 29), (93, 33), (93, 37), (93, 39), (93, 43), (93, 47), (93, 51), (93, 55), (93, 59), (93, 93)]\n",
      "Comparison             inf\n",
      "intersection     11.200000\n",
      "overlap           0.783486\n",
      "Jaccard           0.527745\n",
      "SMC               0.935227\n",
      "lcs               2.100000\n",
      "OmEuni            1.862839\n",
      "OmEbi             2.825677\n",
      "OdEuni            3.979721\n",
      "OdEbi             3.841753\n",
      "OdMuni            0.164446\n",
      "OdMbi             0.331041\n",
      "OmEdMuni          0.113146\n",
      "OmEdMbi           0.228441\n",
      "OmEdMmEMuni       0.118978\n",
      "OmEdMmEMbi        0.251522\n",
      "pairedFreq        2.825677\n",
      "ITR2              0.331041\n",
      "ARC2              0.251522\n",
      "editdist          0.174224\n",
      "mod_editdist      0.179224\n",
      "editdist_IDST     0.179224\n",
      "editdist_IDS      0.174224\n",
      "editdist_ID       0.342653\n",
      "editdist_IDT      0.369278\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80]\n",
      "normalGroup2\n",
      "SN2 [2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80]\n",
      "groupSN [2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80, 2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80]\n",
      "average_recalled_t_Group1= 17.636363636363637\n",
      "average_correct_recalled_t_Group1= 16.818181818181817\n",
      "average_intrusion_recalled_t_Group1= 0.8181818181818182\n",
      "average_recalled_t_Group2= 18.272727272727273\n",
      "average_correct_recalled_t_Group2= 15.909090909090908\n",
      "average_intrusion_recalled_t_Group2= 2.3636363636363638\n",
      "[(2, 2), (2, 4), (2, 10), (2, 16), (2, 26), (2, 64), (2, 66), (2, 68), (2, 72), (2, 76), (2, 80), (4, 2), (4, 4), (4, 10), (4, 16), (4, 26), (4, 64), (4, 66), (4, 68), (4, 72), (4, 76), (4, 80), (10, 2), (10, 4), (10, 10), (10, 16), (10, 26), (10, 64), (10, 66), (10, 68), (10, 72), (10, 76), (10, 80), (16, 2), (16, 4), (16, 10), (16, 16), (16, 26), (16, 64), (16, 66), (16, 68), (16, 72), (16, 76), (16, 80), (26, 2), (26, 4), (26, 10), (26, 16), (26, 26), (26, 64), (26, 66), (26, 68), (26, 72), (26, 76), (26, 80), (64, 2), (64, 4), (64, 10), (64, 16), (64, 26), (64, 64), (64, 66), (64, 68), (64, 72), (64, 76), (64, 80), (66, 2), (66, 4), (66, 10), (66, 16), (66, 26), (66, 64), (66, 66), (66, 68), (66, 72), (66, 76), (66, 80), (68, 2), (68, 4), (68, 10), (68, 16), (68, 26), (68, 64), (68, 66), (68, 68), (68, 72), (68, 76), (68, 80), (72, 2), (72, 4), (72, 10), (72, 16), (72, 26), (72, 64), (72, 66), (72, 68), (72, 72), (72, 76), (72, 80), (76, 2), (76, 4), (76, 10), (76, 16), (76, 26), (76, 64), (76, 66), (76, 68), (76, 72), (76, 76), (76, 80), (80, 2), (80, 4), (80, 10), (80, 16), (80, 26), (80, 64), (80, 66), (80, 68), (80, 72), (80, 76), (80, 80)]\n",
      "Comparison             inf\n",
      "intersection     12.272727\n",
      "overlap           0.668188\n",
      "Jaccard           0.491303\n",
      "SMC               0.905992\n",
      "lcs               1.818182\n",
      "OmEuni            1.843619\n",
      "OmEbi             2.323601\n",
      "OdEuni            4.070182\n",
      "OdEbi             2.942785\n",
      "OdMuni            0.163976\n",
      "OdMbi             0.220199\n",
      "OmEdMuni          0.128620\n",
      "OmEdMbi           0.149487\n",
      "OmEdMmEMuni       0.134560\n",
      "OmEdMmEMbi        0.161987\n",
      "pairedFreq        2.323601\n",
      "ITR2              0.220199\n",
      "ARC2              0.161987\n",
      "editdist          0.105712\n",
      "mod_editdist      0.105712\n",
      "editdist_IDST     0.105712\n",
      "editdist_IDS      0.105712\n",
      "editdist_ID       0.277903\n",
      "editdist_IDT      0.284262\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62]\n",
      "normalGroup2\n",
      "SN2 [24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62]\n",
      "groupSN [24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62, 24, 28, 32, 36, 38, 42, 46, 50, 54, 58, 62]\n",
      "average_recalled_t_Group1= 22.363636363636363\n",
      "average_correct_recalled_t_Group1= 21.636363636363637\n",
      "average_intrusion_recalled_t_Group1= 0.7272727272727273\n",
      "average_recalled_t_Group2= 26.272727272727273\n",
      "average_correct_recalled_t_Group2= 24.90909090909091\n",
      "average_intrusion_recalled_t_Group2= 1.3636363636363635\n",
      "[(24, 24), (24, 28), (24, 32), (24, 36), (24, 38), (24, 42), (24, 46), (24, 50), (24, 54), (24, 58), (24, 62), (28, 24), (28, 28), (28, 32), (28, 36), (28, 38), (28, 42), (28, 46), (28, 50), (28, 54), (28, 58), (28, 62), (32, 24), (32, 28), (32, 32), (32, 36), (32, 38), (32, 42), (32, 46), (32, 50), (32, 54), (32, 58), (32, 62), (36, 24), (36, 28), (36, 32), (36, 36), (36, 38), (36, 42), (36, 46), (36, 50), (36, 54), (36, 58), (36, 62), (38, 24), (38, 28), (38, 32), (38, 36), (38, 38), (38, 42), (38, 46), (38, 50), (38, 54), (38, 58), (38, 62), (42, 24), (42, 28), (42, 32), (42, 36), (42, 38), (42, 42), (42, 46), (42, 50), (42, 54), (42, 58), (42, 62), (46, 24), (46, 28), (46, 32), (46, 36), (46, 38), (46, 42), (46, 46), (46, 50), (46, 54), (46, 58), (46, 62), (50, 24), (50, 28), (50, 32), (50, 36), (50, 38), (50, 42), (50, 46), (50, 50), (50, 54), (50, 58), (50, 62), (54, 24), (54, 28), (54, 32), (54, 36), (54, 38), (54, 42), (54, 46), (54, 50), (54, 54), (54, 58), (54, 62), (58, 24), (58, 28), (58, 32), (58, 36), (58, 38), (58, 42), (58, 46), (58, 50), (58, 54), (58, 58), (58, 62), (62, 24), (62, 28), (62, 32), (62, 36), (62, 38), (62, 42), (62, 46), (62, 50), (62, 54), (62, 58), (62, 62)]\n",
      "Comparison             inf\n",
      "intersection     17.000000\n",
      "overlap           0.768597\n",
      "Jaccard           0.501060\n",
      "SMC               0.861570\n",
      "lcs               2.181818\n",
      "OmEuni            2.300967\n",
      "OmEbi             3.238297\n",
      "OdEuni            5.450429\n",
      "OdEbi             3.851942\n",
      "OdMuni            0.147196\n",
      "OdMbi             0.202171\n",
      "OmEdMuni          0.116086\n",
      "OmEdMbi           0.139952\n",
      "OmEdMmEMuni       0.119180\n",
      "OmEdMmEMbi        0.146783\n",
      "pairedFreq        3.238297\n",
      "ITR2              0.202171\n",
      "ARC2              0.146783\n",
      "editdist          0.112232\n",
      "mod_editdist      0.116364\n",
      "editdist_IDST     0.116364\n",
      "editdist_IDS      0.112232\n",
      "editdist_ID       0.268521\n",
      "editdist_IDT      0.280200\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [1, 3, 9, 15, 19, 67, 71, 75, 79]\n",
      "normalGroup2\n",
      "SN2 [1, 3, 9, 15, 19, 67, 71, 75, 79]\n",
      "groupSN [1, 3, 9, 15, 19, 67, 71, 75, 79, 1, 3, 9, 15, 19, 67, 71, 75, 79]\n",
      "average_recalled_t_Group1= 27.22222222222222\n",
      "average_correct_recalled_t_Group1= 26.333333333333332\n",
      "average_intrusion_recalled_t_Group1= 0.8888888888888888\n",
      "average_recalled_t_Group2= 31.22222222222222\n",
      "average_correct_recalled_t_Group2= 30.22222222222222\n",
      "average_intrusion_recalled_t_Group2= 1.0\n",
      "[(1, 1), (1, 3), (1, 9), (1, 15), (1, 19), (1, 67), (1, 71), (1, 75), (1, 79), (3, 1), (3, 3), (3, 9), (3, 15), (3, 19), (3, 67), (3, 71), (3, 75), (3, 79), (9, 1), (9, 3), (9, 9), (9, 15), (9, 19), (9, 67), (9, 71), (9, 75), (9, 79), (15, 1), (15, 3), (15, 9), (15, 15), (15, 19), (15, 67), (15, 71), (15, 75), (15, 79), (19, 1), (19, 3), (19, 9), (19, 15), (19, 19), (19, 67), (19, 71), (19, 75), (19, 79), (67, 1), (67, 3), (67, 9), (67, 15), (67, 19), (67, 67), (67, 71), (67, 75), (67, 79), (71, 1), (71, 3), (71, 9), (71, 15), (71, 19), (71, 67), (71, 71), (71, 75), (71, 79), (75, 1), (75, 3), (75, 9), (75, 15), (75, 19), (75, 67), (75, 71), (75, 75), (75, 79), (79, 1), (79, 3), (79, 9), (79, 15), (79, 19), (79, 67), (79, 71), (79, 75), (79, 79)]\n",
      "Comparison             inf\n",
      "intersection     22.444444\n",
      "overlap           0.847080\n",
      "Jaccard           0.618174\n",
      "SMC               0.875000\n",
      "lcs               2.333333\n",
      "OmEuni            2.659322\n",
      "OmEbi             4.763088\n",
      "OdEuni            5.413338\n",
      "OdEbi             5.269860\n",
      "OdMuni            0.146261\n",
      "OdMbi             0.295218\n",
      "OmEdMuni          0.117685\n",
      "OmEdMbi           0.238066\n",
      "OmEdMmEMuni       0.121182\n",
      "OmEdMmEMbi        0.253458\n",
      "pairedFreq        4.763088\n",
      "ITR2              0.295218\n",
      "ARC2              0.253458\n",
      "editdist          0.136992\n",
      "mod_editdist      0.136992\n",
      "editdist_IDST     0.136992\n",
      "editdist_IDS      0.136992\n",
      "editdist_ID       0.297068\n",
      "editdist_IDT      0.316462\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [29, 33, 37, 39, 43, 47, 51, 55, 59, 93]\n",
      "normalGroup2\n",
      "SN2 [29, 33, 37, 39, 43, 47, 51, 55, 59, 93]\n",
      "groupSN [29, 33, 37, 39, 43, 47, 51, 55, 59, 93, 29, 33, 37, 39, 43, 47, 51, 55, 59, 93]\n",
      "average_recalled_t_Group1= 13.8\n",
      "average_correct_recalled_t_Group1= 12.6\n",
      "average_intrusion_recalled_t_Group1= 1.2\n",
      "average_recalled_t_Group2= 17.2\n",
      "average_correct_recalled_t_Group2= 14.9\n",
      "average_intrusion_recalled_t_Group2= 2.3\n",
      "[(29, 29), (29, 33), (29, 37), (29, 39), (29, 43), (29, 47), (29, 51), (29, 55), (29, 59), (29, 93), (33, 29), (33, 33), (33, 37), (33, 39), (33, 43), (33, 47), (33, 51), (33, 55), (33, 59), (33, 93), (37, 29), (37, 33), (37, 37), (37, 39), (37, 43), (37, 47), (37, 51), (37, 55), (37, 59), (37, 93), (39, 29), (39, 33), (39, 37), (39, 39), (39, 43), (39, 47), (39, 51), (39, 55), (39, 59), (39, 93), (43, 29), (43, 33), (43, 37), (43, 39), (43, 43), (43, 47), (43, 51), (43, 55), (43, 59), (43, 93), (47, 29), (47, 33), (47, 37), (47, 39), (47, 43), (47, 47), (47, 51), (47, 55), (47, 59), (47, 93), (51, 29), (51, 33), (51, 37), (51, 39), (51, 43), (51, 47), (51, 51), (51, 55), (51, 59), (51, 93), (55, 29), (55, 33), (55, 37), (55, 39), (55, 43), (55, 47), (55, 51), (55, 55), (55, 59), (55, 93), (59, 29), (59, 33), (59, 37), (59, 39), (59, 43), (59, 47), (59, 51), (59, 55), (59, 59), (59, 93), (93, 29), (93, 33), (93, 37), (93, 39), (93, 43), (93, 47), (93, 51), (93, 55), (93, 59), (93, 93)]\n",
      "Comparison             inf\n",
      "intersection     10.200000\n",
      "overlap           0.737858\n",
      "Jaccard           0.480713\n",
      "SMC               0.915909\n",
      "lcs               1.900000\n",
      "OmEuni            0.916671\n",
      "OmEbi             1.833342\n",
      "OdEuni            3.412749\n",
      "OdEbi             3.128874\n",
      "OdMuni            0.146046\n",
      "OdMbi             0.265541\n",
      "OmEdMuni          0.098101\n",
      "OmEdMbi           0.169652\n",
      "OmEdMmEMuni       0.103008\n",
      "OmEdMmEMbi        0.186616\n",
      "pairedFreq        1.833342\n",
      "ITR2              0.265541\n",
      "ARC2              0.186616\n",
      "editdist          0.142983\n",
      "mod_editdist      0.148246\n",
      "editdist_IDST     0.148246\n",
      "editdist_IDS      0.142983\n",
      "editdist_ID       0.281880\n",
      "editdist_IDT      0.296399\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80]\n",
      "normalGroup2\n",
      "SN2 [2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80]\n",
      "groupSN [2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80, 2, 4, 10, 16, 26, 64, 66, 68, 72, 76, 80]\n",
      "average_recalled_t_Group1= 17.454545454545453\n",
      "average_correct_recalled_t_Group1= 16.727272727272727\n",
      "average_intrusion_recalled_t_Group1= 0.7272727272727273\n",
      "average_recalled_t_Group2= 18.272727272727273\n",
      "average_correct_recalled_t_Group2= 15.909090909090908\n",
      "average_intrusion_recalled_t_Group2= 2.3636363636363638\n",
      "[(2, 2), (2, 4), (2, 10), (2, 16), (2, 26), (2, 64), (2, 66), (2, 68), (2, 72), (2, 76), (2, 80), (4, 2), (4, 4), (4, 10), (4, 16), (4, 26), (4, 64), (4, 66), (4, 68), (4, 72), (4, 76), (4, 80), (10, 2), (10, 4), (10, 10), (10, 16), (10, 26), (10, 64), (10, 66), (10, 68), (10, 72), (10, 76), (10, 80), (16, 2), (16, 4), (16, 10), (16, 16), (16, 26), (16, 64), (16, 66), (16, 68), (16, 72), (16, 76), (16, 80), (26, 2), (26, 4), (26, 10), (26, 16), (26, 26), (26, 64), (26, 66), (26, 68), (26, 72), (26, 76), (26, 80), (64, 2), (64, 4), (64, 10), (64, 16), (64, 26), (64, 64), (64, 66), (64, 68), (64, 72), (64, 76), (64, 80), (66, 2), (66, 4), (66, 10), (66, 16), (66, 26), (66, 64), (66, 66), (66, 68), (66, 72), (66, 76), (66, 80), (68, 2), (68, 4), (68, 10), (68, 16), (68, 26), (68, 64), (68, 66), (68, 68), (68, 72), (68, 76), (68, 80), (72, 2), (72, 4), (72, 10), (72, 16), (72, 26), (72, 64), (72, 66), (72, 68), (72, 72), (72, 76), (72, 80), (76, 2), (76, 4), (76, 10), (76, 16), (76, 26), (76, 64), (76, 66), (76, 68), (76, 72), (76, 76), (76, 80), (80, 2), (80, 4), (80, 10), (80, 16), (80, 26), (80, 64), (80, 66), (80, 68), (80, 72), (80, 76), (80, 80)]\n",
      "Comparison             inf\n",
      "intersection     11.545455\n",
      "overlap           0.620925\n",
      "Jaccard           0.458964\n",
      "SMC               0.890496\n",
      "lcs               1.454545\n",
      "OmEuni            0.790562\n",
      "OmEbi             1.308396\n",
      "OdEuni            2.345867\n",
      "OdEbi             2.190631\n",
      "OdMuni            0.074238\n",
      "OdMbi             0.137020\n",
      "OmEdMuni          0.042451\n",
      "OmEdMbi           0.073446\n",
      "OmEdMmEMuni       0.043631\n",
      "OmEdMmEMbi        0.077412\n",
      "pairedFreq        1.308396\n",
      "ITR2              0.137020\n",
      "ARC2              0.077412\n",
      "editdist          0.126999\n",
      "mod_editdist      0.126999\n",
      "editdist_IDST     0.126999\n",
      "editdist_IDS      0.126999\n",
      "editdist_ID       0.272894\n",
      "editdist_IDT      0.278886\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# RESEARCH QUESTION 3: Individual memory similairt pre/post collab\n",
    "# Self-similarity recall 1-2\n",
    "similarity(df, 0,0,0,1,0,0,0,2,False,True) \n",
    "similarity(df, 0,0,1,1,0,0,1,2,False,True) \n",
    "similarity(df, 0,1,0,1,0,1,0,2,False,True) \n",
    "similarity(df, 0,1,1,1,0,1,1,2,False,True) \n",
    "\n",
    "# Self-similarity recall 2-3\n",
    "similarity(df, 0,0,0,2,0,0,0,3,False,True) \n",
    "similarity(df, 0,0,1,2,0,0,1,3,False,True) \n",
    "similarity(df, 0,1,0,2,0,1,0,3,False,True) \n",
    "similarity(df, 0,1,1,2,0,1,1,3,False,True) \n",
    "\n",
    "# Self-similarity recall 1-3\n",
    "similarity(df, 0,0,0,1,0,0,0,3,False,True) \n",
    "similarity(df, 0,0,1,1,0,0,1,3,False,True) \n",
    "similarity(df, 0,1,0,1,0,1,0,3,False,True) \n",
    "similarity(df, 0,1,1,1,0,1,1,3,False,True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B5. Group means (Results presented in Manuscript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022-04-18_Similarity_Exp3_AllWords_Clean_000100_010100_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_000101_000201_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_000101_000301_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_000201_000301_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_000300_010300_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_001100_011100_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_001101_001201_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_001101_001301_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_001201_001301_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_001300_011300_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_010101_010201_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_010101_010301_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_010201_010301_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_011101_011201_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_011101_011301_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_011201_011301_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_100100_110300_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_100101_100301_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_100110_110310_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_101100_111300_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_101101_101301_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_101110_111310_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_110100_100100_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_110100_100300_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_110101_110301_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_110110_100110_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_110110_100310_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_110300_100300_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_110310_100310_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_111100_101100_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_111100_101300_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_111101_111301_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_111110_101110_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_111110_101310_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_111300_101300_Results.csv', '2022-04-18_Similarity_Exp3_AllWords_Clean_111310_101310_Results.csv']\n",
      "AComp None\n"
     ]
    }
   ],
   "source": [
    "# Merge all self-similarity in one file\n",
    "# If file name was changed above, it has to also be changed here\n",
    "all_files = glob.glob(\"2022-04-18_Similarity_Exp3_AllWords_Clean_*_Results.csv\") \n",
    "print(all_files)\n",
    "results = pd.concat((pd.read_csv(f) for f in all_files),sort=False)\n",
    "results.to_csv('2022-04-18_Similarity_Exp3_AllWords_Clean_Comparisons.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEANS                            intersection  overlap  Jaccard   SMC   lcs  OmEuni  \\\n",
      "Group                                                                           \n",
      "11_Nominal_Pre                     4.66     0.31     0.13  0.68  1.05    0.09   \n",
      "12_NominalCollab_Pre               5.23     0.35     0.14  0.70  1.04    0.08   \n",
      "13_Collab_Pre                      5.10     0.33     0.13  0.69  1.10    0.22   \n",
      "14_Nominal_Post                    5.91     0.33     0.14  0.66  1.18    0.30   \n",
      "15_NominalCollab_Post             10.10     0.45     0.21  0.65  1.50    0.61   \n",
      "16_Collab_Post                    15.29     0.67     0.37  0.76  1.76    1.18   \n",
      "21_SelfPrePost                    15.83     0.75     0.47  0.86  2.07    1.68   \n",
      "22_PartnerPre_SelfPost             9.74     0.55     0.25  0.72  1.31    0.31   \n",
      "23_NonPartnerPre_SelfPost          7.16     0.40     0.17  0.66  1.21    0.25   \n",
      "31_Self1_2                        16.00     0.85     0.66  0.94  1.98    1.75   \n",
      "32_Self2_3                        15.85     0.76     0.56  0.91  2.16    2.79   \n",
      "33_Self1_3                        14.87     0.73     0.51  0.89  1.94    1.61   \n",
      "\n",
      "                           OmEbi  OdEuni  OdEbi  OdMuni  ...  OmEdMmEMbi  \\\n",
      "Group                                                    ...               \n",
      "11_Nominal_Pre              0.23    2.84   2.93    0.04  ...        0.05   \n",
      "12_NominalCollab_Pre        0.29    1.40   2.19    0.02  ...        0.04   \n",
      "13_Collab_Pre               0.40    5.30   4.04    0.06  ...        0.08   \n",
      "14_Nominal_Post             0.58    3.96   4.28    0.05  ...        0.09   \n",
      "15_NominalCollab_Post       1.14    5.26   4.97    0.07  ...        0.12   \n",
      "16_Collab_Post              2.36    4.40   4.43    0.09  ...        0.15   \n",
      "21_SelfPrePost              2.67    8.06   5.51    0.14  ...        0.16   \n",
      "22_PartnerPre_SelfPost      0.97    2.00   3.21    0.04  ...        0.08   \n",
      "23_NonPartnerPre_SelfPost   0.51    2.92   3.20    0.04  ...        0.06   \n",
      "31_Self1_2                  2.64    3.71   2.97    0.14  ...        0.13   \n",
      "32_Self2_3                  4.00    5.52   4.33    0.18  ...        0.23   \n",
      "33_Self1_3                  2.65    4.07   3.48    0.13  ...        0.16   \n",
      "\n",
      "                           pairedFreq  ITR2  ARC2  editdist  mod_editdist  \\\n",
      "Group                                                                       \n",
      "11_Nominal_Pre                   0.23  0.07  0.05      0.05          0.05   \n",
      "12_NominalCollab_Pre             0.29  0.07  0.04      0.05          0.05   \n",
      "13_Collab_Pre                    0.40  0.10  0.08      0.06          0.06   \n",
      "14_Nominal_Post                  0.58  0.10  0.09      0.05          0.05   \n",
      "15_NominalCollab_Post            1.14  0.14  0.12      0.06          0.06   \n",
      "16_Collab_Post                   2.36  0.19  0.15      0.11          0.11   \n",
      "21_SelfPrePost                   2.67  0.21  0.16      0.13          0.13   \n",
      "22_PartnerPre_SelfPost           0.97  0.12  0.08      0.08          0.08   \n",
      "23_NonPartnerPre_SelfPost        0.51  0.09  0.06      0.06          0.06   \n",
      "31_Self1_2                       2.64  0.23  0.13      0.16          0.17   \n",
      "32_Self2_3                       4.00  0.29  0.23      0.15          0.15   \n",
      "33_Self1_3                       2.65  0.22  0.16      0.13          0.13   \n",
      "\n",
      "                           editdist_IDST  editdist_IDS  editdist_ID  \\\n",
      "Group                                                                 \n",
      "11_Nominal_Pre                      0.05          0.05         0.12   \n",
      "12_NominalCollab_Pre                0.05          0.05         0.12   \n",
      "13_Collab_Pre                       0.06          0.06         0.12   \n",
      "14_Nominal_Post                     0.05          0.05         0.12   \n",
      "15_NominalCollab_Post               0.06          0.06         0.15   \n",
      "16_Collab_Post                      0.11          0.11         0.21   \n",
      "21_SelfPrePost                      0.13          0.13         0.26   \n",
      "22_PartnerPre_SelfPost              0.08          0.08         0.17   \n",
      "23_NonPartnerPre_SelfPost           0.06          0.06         0.13   \n",
      "31_Self1_2                          0.17          0.16         0.36   \n",
      "32_Self2_3                          0.15          0.15         0.31   \n",
      "33_Self1_3                          0.13          0.13         0.28   \n",
      "\n",
      "                           editdist_IDT  \n",
      "Group                                    \n",
      "11_Nominal_Pre                     0.12  \n",
      "12_NominalCollab_Pre               0.13  \n",
      "13_Collab_Pre                      0.12  \n",
      "14_Nominal_Post                    0.12  \n",
      "15_NominalCollab_Post              0.16  \n",
      "16_Collab_Post                     0.22  \n",
      "21_SelfPrePost                     0.27  \n",
      "22_PartnerPre_SelfPost             0.17  \n",
      "23_NonPartnerPre_SelfPost          0.14  \n",
      "31_Self1_2                         0.37  \n",
      "32_Self2_3                         0.33  \n",
      "33_Self1_3                         0.29  \n",
      "\n",
      "[12 rows x 24 columns]\n",
      "SD                            intersection  overlap  Jaccard   SMC   lcs  OmEuni  \\\n",
      "Group                                                                           \n",
      "11_Nominal_Pre                     3.17     0.16     0.07  0.09  0.42    0.41   \n",
      "12_NominalCollab_Pre               3.40     0.19     0.08  0.09  0.45    0.41   \n",
      "13_Collab_Pre                      3.74     0.21     0.08  0.08  0.62    0.54   \n",
      "14_Nominal_Post                    4.36     0.19     0.08  0.09  0.66    0.65   \n",
      "15_NominalCollab_Post              5.82     0.20     0.10  0.08  0.71    0.97   \n",
      "16_Collab_Post                     8.23     0.24     0.16  0.07  0.70    1.31   \n",
      "21_SelfPrePost                     9.42     0.20     0.18  0.07  0.97    2.18   \n",
      "22_PartnerPre_SelfPost             5.74     0.22     0.12  0.08  0.60    0.88   \n",
      "23_NonPartnerPre_SelfPost          4.51     0.20     0.09  0.09  0.56    0.59   \n",
      "31_Self1_2                         8.57     0.17     0.18  0.04  0.77    2.14   \n",
      "32_Self2_3                        10.36     0.25     0.23  0.05  1.04    3.25   \n",
      "33_Self1_3                         9.11     0.23     0.20  0.06  0.83    1.80   \n",
      "\n",
      "                           OmEbi  OdEuni  OdEbi  OdMuni  ...  OmEdMmEMbi  \\\n",
      "Group                                                    ...               \n",
      "11_Nominal_Pre              0.57   10.83   6.83    0.13  ...        0.15   \n",
      "12_NominalCollab_Pre        0.64    4.55   3.80    0.06  ...        0.11   \n",
      "13_Collab_Pre               0.69   14.08   7.31    0.15  ...        0.17   \n",
      "14_Nominal_Post             0.88    7.33   5.78    0.09  ...        0.13   \n",
      "15_NominalCollab_Post       1.29    7.75   4.67    0.10  ...        0.13   \n",
      "16_Collab_Post              2.27    4.05   2.76    0.09  ...        0.11   \n",
      "21_SelfPrePost              2.91   25.24  12.53    0.13  ...        0.16   \n",
      "22_PartnerPre_SelfPost      1.63    3.24   3.56    0.06  ...        0.14   \n",
      "23_NonPartnerPre_SelfPost   0.81    6.21   4.55    0.08  ...        0.11   \n",
      "31_Self1_2                  3.02    3.31   2.27    0.12  ...        0.20   \n",
      "32_Self2_3                  4.04    4.77   2.89    0.13  ...        0.15   \n",
      "33_Self1_3                  2.62    3.32   2.45    0.09  ...        0.15   \n",
      "\n",
      "                           pairedFreq  ITR2  ARC2  editdist  mod_editdist  \\\n",
      "Group                                                                       \n",
      "11_Nominal_Pre                   0.57  0.15  0.15      0.04          0.04   \n",
      "12_NominalCollab_Pre             0.64  0.11  0.11      0.04          0.04   \n",
      "13_Collab_Pre                    0.69  0.17  0.17      0.05          0.05   \n",
      "14_Nominal_Post                  0.88  0.13  0.13      0.04          0.04   \n",
      "15_NominalCollab_Post            1.29  0.13  0.13      0.04          0.04   \n",
      "16_Collab_Post                   2.27  0.11  0.11      0.06          0.06   \n",
      "21_SelfPrePost                   2.91  0.15  0.16      0.07          0.07   \n",
      "22_PartnerPre_SelfPost           1.63  0.14  0.14      0.04          0.04   \n",
      "23_NonPartnerPre_SelfPost        0.81  0.11  0.11      0.04          0.04   \n",
      "31_Self1_2                       3.02  0.15  0.20      0.10          0.10   \n",
      "32_Self2_3                       4.04  0.14  0.15      0.10          0.10   \n",
      "33_Self1_3                       2.62  0.13  0.15      0.07          0.08   \n",
      "\n",
      "                           editdist_IDST  editdist_IDS  editdist_ID  \\\n",
      "Group                                                                 \n",
      "11_Nominal_Pre                      0.04          0.04         0.06   \n",
      "12_NominalCollab_Pre                0.04          0.04         0.06   \n",
      "13_Collab_Pre                       0.05          0.05         0.07   \n",
      "14_Nominal_Post                     0.04          0.04         0.06   \n",
      "15_NominalCollab_Post               0.04          0.04         0.07   \n",
      "16_Collab_Post                      0.06          0.06         0.10   \n",
      "21_SelfPrePost                      0.07          0.07         0.08   \n",
      "22_PartnerPre_SelfPost              0.04          0.04         0.07   \n",
      "23_NonPartnerPre_SelfPost           0.04          0.04         0.06   \n",
      "31_Self1_2                          0.10          0.10         0.12   \n",
      "32_Self2_3                          0.10          0.10         0.13   \n",
      "33_Self1_3                          0.08          0.07         0.10   \n",
      "\n",
      "                           editdist_IDT  \n",
      "Group                                    \n",
      "11_Nominal_Pre                     0.06  \n",
      "12_NominalCollab_Pre               0.06  \n",
      "13_Collab_Pre                      0.07  \n",
      "14_Nominal_Post                    0.07  \n",
      "15_NominalCollab_Post              0.07  \n",
      "16_Collab_Post                     0.10  \n",
      "21_SelfPrePost                     0.09  \n",
      "22_PartnerPre_SelfPost             0.08  \n",
      "23_NonPartnerPre_SelfPost          0.06  \n",
      "31_Self1_2                         0.12  \n",
      "32_Self2_3                         0.13  \n",
      "33_Self1_3                         0.10  \n",
      "\n",
      "[12 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Reminder: Naming convention of groups\n",
    "# collab1) + (bias1) + (order1) + (phase1) + ((TrueCollab)) + ((Self))+ '_' +  str(collab2) + str(bias2) + str(order2) + str(phase2) + str(int(TrueCollab))+ str(int(Self))\n",
    "\n",
    "# Step 1: Introduce new naming that allows to average across order\n",
    "#RESEARCH Q1 (usual groups)\n",
    "results.loc[results['Comparison'] == '00010_01010', 'Group'] = '11_Nominal_Pre' \n",
    "results.loc[results['Comparison'] == '00110_01110', 'Group'] = '11_Nominal_Pre' \n",
    "results.loc[results['Comparison'] == '11010_10010', 'Group'] = '12_NominalCollab_Pre' \n",
    "results.loc[results['Comparison'] == '11110_10110', 'Group'] = '12_NominalCollab_Pre' \n",
    "results.loc[results['Comparison'] == '11011_10011', 'Group'] = '13_Collab_Pre' \n",
    "results.loc[results['Comparison'] == '11111_10111', 'Group'] = '13_Collab_Pre' \n",
    "\n",
    "results.loc[results['Comparison'] == '00030_01030', 'Group'] = '14_Nominal_Post' \n",
    "results.loc[results['Comparison'] == '00130_01130', 'Group'] = '14_Nominal_Post' \n",
    "results.loc[results['Comparison'] == '11030_10030', 'Group'] = '15_NominalCollab_Post' \n",
    "results.loc[results['Comparison'] == '11130_10130', 'Group'] = '15_NominalCollab_Post' \n",
    "results.loc[results['Comparison'] == '11031_10031', 'Group'] = '16_Collab_Post' \n",
    "results.loc[results['Comparison'] == '11131_10131', 'Group'] = '16_Collab_Post' \n",
    "\n",
    "# RESEARCH Q2 (collab only)\n",
    "results.loc[results['Comparison'] == '10010_10030', 'Group'] = '21_SelfPrePost' \n",
    "results.loc[results['Comparison'] == '10110_10130', 'Group'] = '21_SelfPrePost' \n",
    "results.loc[results['Comparison'] == '11010_11030', 'Group'] = '21_SelfPrePost' \n",
    "results.loc[results['Comparison'] == '11110_11130', 'Group'] = '21_SelfPrePost' \n",
    "\n",
    "results.loc[results['Comparison'] == '11011_10031', 'Group'] = '22_PartnerPre_SelfPost' \n",
    "results.loc[results['Comparison'] == '11111_10131', 'Group'] = '22_PartnerPre_SelfPost'\n",
    "results.loc[results['Comparison'] == '10011_11031', 'Group'] = '22_PartnerPre_SelfPost'\n",
    "results.loc[results['Comparison'] == '10111_11131', 'Group'] = '22_PartnerPre_SelfPost'\n",
    "\n",
    "results.loc[results['Comparison'] == '11010_10030', 'Group'] = '23_NonPartnerPre_SelfPost' \n",
    "results.loc[results['Comparison'] == '11110_10130', 'Group'] = '23_NonPartnerPre_SelfPost'\n",
    "results.loc[results['Comparison'] == '10010_11030', 'Group'] = '23_NonPartnerPre_SelfPost'\n",
    "results.loc[results['Comparison'] == '10110_11130', 'Group'] = '23_NonPartnerPre_SelfPost'\n",
    "\n",
    "\n",
    "# RESEARCH Q3 (individual only) \n",
    "results.loc[results['Comparison'] == '00010_00020', 'Group'] = '31_Self1_2' \n",
    "results.loc[results['Comparison'] == '00110_00120', 'Group'] = '31_Self1_2'\n",
    "results.loc[results['Comparison'] == '01010_01020', 'Group'] = '31_Self1_2'\n",
    "results.loc[results['Comparison'] == '01110_01120', 'Group'] = '31_Self1_2'\n",
    "\n",
    "results.loc[results['Comparison'] == '00020_00030', 'Group'] = '32_Self2_3'\n",
    "results.loc[results['Comparison'] == '00120_00130', 'Group'] = '32_Self2_3'\n",
    "results.loc[results['Comparison'] == '01020_01030', 'Group'] = '32_Self2_3'\n",
    "results.loc[results['Comparison'] == '01120_01130', 'Group'] = '32_Self2_3'\n",
    "\n",
    "results.loc[results['Comparison'] == '00010_00030', 'Group'] = '33_Self1_3'\n",
    "results.loc[results['Comparison'] == '00110_00130', 'Group'] = '33_Self1_3'\n",
    "results.loc[results['Comparison'] == '01010_01030', 'Group'] = '33_Self1_3'\n",
    "results.loc[results['Comparison'] == '01110_01130', 'Group'] = '33_Self1_3'\n",
    "\n",
    "# Step 2: Average across order using new naming convention\n",
    "results.to_csv('2022-04-18_Similarity_Exp3_AllWords_Clean-RelevantComparisons2.csv', index=False)\n",
    "GroupMeans = round(results.groupby('Group').mean(),2)\n",
    "print('MEANS', GroupMeans)\n",
    "GroupSD = round(results.groupby('Group').std(),2)\n",
    "print('SD', GroupSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be Continued. Alex to add semi-partial calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
