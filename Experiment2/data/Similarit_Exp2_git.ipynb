{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import bz2\n",
    "import difflib\n",
    "import logging\n",
    "import time\n",
    "import zlib\n",
    "import itertools\n",
    "import collections\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sn\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from itertools import chain, combinations, tee, islice, permutations\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from pathlib import Path\n",
    "from collections import Counter \n",
    "from IPython.display import display\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "## Content\n",
    "### A. For general use:\n",
    "* [A1. Similarity measures](#A1.-Similarity-measures) \n",
    "    * [A1.1. Set-theoretic measures (Intersection, Overlap, Jaccard, SMC, LCS, ngram_abs)](#A1.1.-Set-theoretic-measures-(Intersection,-Overlap,-Jaccard,-SMC,-LCS,-ngram_abs)) \n",
    "    * [A1.2. ITR measures](#A1.2.-ITR-measures)\n",
    "        * [A1.2.1. Here we first calculate the observed, expected and maximum items of a specific unit size](#A1.2.1.-Here-we-first-calculate-the-observed,-expected-and-maximum-items-of-a-specific-unit-size)\n",
    "        * [A1.2.2. Then, we summarize them in final metrics (e.g., OmE (Pair Frequency/SOMA), OdE, OmEdM, OdM, OmEdMmE)](#A1.2.2.-Then,-we-summarize-them-in-final-metrics-(e.g.,-OmE-(Pair-Frequency/SOMA),-OdE,-OmEdM,-OdM,-OmEdMmE))\n",
    "    * [A1.3. Edit distance measures](#A1.3.-Edit-distance-measures)\n",
    "    * [A1.4. Graveyard for old similarity measure functions](#1.4.-Graveyard-for-old-similarity-measure-functions)\n",
    "\n",
    "### B. Project specific: Data wrangling for experiments\n",
    "* [B1. Experiment & Data overview](#B1.-Experiment-&-Data-overview)\n",
    "* [B2. Pickle file conversion (old --> new)](#B2.-Pickle-file-conversion-(old--->-new))\n",
    "* [B3. Data frame creation (incl. spell check); to be used for subsequent analysis](#B3.-Data-frame-creation-(incl.-spell-check);-to-be-used-for-subsequent-analysis)\n",
    "* [B4. Similarity for dyads by sub-group](#B4.-Similarity-for-dyads-by-sub-group)\n",
    "* [B5. Group means (Results presented in Manuscript)](#B5.-Group-means-(Results-presented-in-Manuscript))\n",
    "* [B6. Correlation-Matrix (Results presented in Manuscript)](#B6.-Correlation-Matrix-(Results-presented-in-Manuscript))\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __A1. Similarity measures__\n",
    "#### A1.1. Set-theoretic measures (Intersection, Overlap, Jaccard, SMC, LCS, ngram_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(str1, str2):\n",
    "    intersection = len(np.intersect1d(str1, str2))\n",
    "    return intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(str1, str2):\n",
    "    intersection = len(np.intersect1d(str1, str2))\n",
    "\n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(str1) >= len(str2):\n",
    "        larger = len(str1) \n",
    "        smaller = len(str2)\n",
    "    else:\n",
    "        smaller = len(str1)\n",
    "        larger = len(str2) \n",
    "    overlap = intersection / smaller\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(str1, str2): \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    union = len(np.union1d(str1, str2))\n",
    "    intersection = len(np.intersect1d(str1, str2))\n",
    "\n",
    "    Jaccard = intersection / union\n",
    "    #print(Jaccard)\n",
    "    return Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMC(str1, str2): # This counts the number of muturally forgotten items as \"similar\". TBD in case of Experiments 1B and 3\n",
    "    encoding = list(range(1,85)) + [157,158,159,160] # Manual entry to get all words from the original study list             \n",
    "    \n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    intersection = len(np.intersect1d(str1, str2))\n",
    "    \n",
    "    Forgotten = 0\n",
    "    for word in encoding:\n",
    "        if word in str1 or word in str2:\n",
    "            continue\n",
    "        else:\n",
    "            Forgotten += 1\n",
    "    \n",
    "    return (((Forgotten+intersection) / len(encoding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_abs(str1, str2, unitSize, y, unidirectional=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    USAGE:\n",
    "    #of bigrams/trigrams/ngrams = ngram_abs(str1, str2, unitSize, y, unidirectional=True)\n",
    "\n",
    "    ARGUMENTS:\n",
    "    str1            The first string\n",
    "    str2            The second string\n",
    "    unitSize        Size of unit of interest (word pair, triple, quartuple etc.)\n",
    "    unidirectional  Order/Direction of unit (only 1-2 valid, or also 2-1)\n",
    "\n",
    "    DESCRIPTION:\n",
    "    Can be used to calculate the joint number of word pairs/bigrams or triplets/trigrams across two participants\n",
    "    \"\"\"\n",
    "     \n",
    "    #str1 = str1.values.tolist()\n",
    "    #str2 = str2.values.tolist()\n",
    "    string1 = None\n",
    "    string2 = None\n",
    "    \n",
    "    if unidirectional == True:\n",
    "        #return [np.array(x) for x in zip(string[0:-1], string[1:])]\n",
    "        iters = tee(str1, unitSize)                                                     \n",
    "        for i, it in enumerate(iters):                                               \n",
    "            next(islice(it, i, i), None)\n",
    "            \n",
    "        iters2 = tee(str2, unitSize)                                                     \n",
    "        for i, it in enumerate(iters2):                                               \n",
    "            next(islice(it, i, i), None)\n",
    "           \n",
    "        string1 = list(zip(*iters))\n",
    "        string2 = list(zip(*iters2))\n",
    "    else:\n",
    "        s = list(str1)\n",
    "        powerset = chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "        s2 = list(str2)\n",
    "        powerset2 = chain.from_iterable(combinations(s2, r) for r in range(len(s2)+1))\n",
    "        string1 = [(x) for x in powerset if len(x)==unitSize]\n",
    "        string2 = [(x) for x in powerset2 if len(x)==unitSize]\n",
    "    #print(string1)\n",
    "    #print(string2)\n",
    "\n",
    "    # This is for assining unique numbers to the different n-grams. At the moment it is executed elsewhere,\\ \n",
    "    # but one could also execute it in here\n",
    "    #y = permu(unitSize)\n",
    "    #all_p = pd.DataFrame()\n",
    "    #all_p['AllPermutations'] = y\n",
    "    \n",
    "    a_list = []\n",
    "    for i in (string1):\n",
    "        if i in y:\n",
    "            x = y.index(i)\n",
    "        else:\n",
    "            continue\n",
    "        a_list.append(x)\n",
    "\n",
    "    b_list = []\n",
    "    for i in (string2):\n",
    "        if i in y:\n",
    "            x = y.index(i)\n",
    "        else:\n",
    "            continue\n",
    "        b_list.append(x)\n",
    "    \n",
    "    count = 0\n",
    "    for i in string1:\n",
    "        if i in string1 and i in string2:\n",
    "            count += 1\n",
    "        else:\n",
    "            count += 0\n",
    "    #print(count)\n",
    "    #a = np.array(a_list)\n",
    "    #b = np.array(b_list)\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from source: https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Longest_common_substring#Python\n",
    "def lcs(str1, str2):\n",
    "    m = [[0] * (1 + len(str2)) for i in range(1 + len(str1))]\n",
    "    longest, x_longest = 0, 0\n",
    "    for x in range(1, 1 + len(str1)):\n",
    "        for y in range(1, 1 + len(str2)):\n",
    "            if str1[x - 1] == str2[y - 1]:\n",
    "                m[x][y] = m[x - 1][y - 1] + 1\n",
    "                if m[x][y] > longest:\n",
    "                    longest = m[x][y]\n",
    "                    x_longest = x\n",
    "            else:\n",
    "                m[x][y] = 0\n",
    "    string = (str1[x_longest - longest: x_longest])\n",
    "    lcs = len(string)\n",
    "    #print(string)\n",
    "    return lcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENTION! This needs to be manually adjusted over the experiments to identify the max of permutations, which is specific to the experiment\n",
    "def permu(n):\n",
    "    df_max = pd.read_csv('2020-08-14_Similarity_Availability_AllWords_Clean.csv')\n",
    "    maxi = max(df_max.number) \n",
    "    #print(maxi)\n",
    "    all = list(range(0,maxi))\n",
    "    y = list(itertools.permutations(all, n))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __A1.2. ITR measures__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A1.2.1. Here we first calculate the observed, expected and maximum items of a specific unit size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observed(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    USAGE:\n",
    "    score = observed(str1, str2, unitSize, unidirectional=True)\n",
    "\n",
    "    ARGUMENTS:\n",
    "    str1            The first string\n",
    "    str2            The second string\n",
    "    unitSize        Size of unit of interest (word pair, triple, quartuple etc.)\n",
    "    unidirectional  Direction of unit (only 1-2 valid, or also 2-1)\n",
    "\n",
    "    DESCRIPTION:\n",
    "    This calculates the observed words of a specific unit size to be used in subsequent shared organization measures\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(str1) == 0:\n",
    "        return []\n",
    "    elif len(str2) ==0:\n",
    "        return []\n",
    "            \n",
    "    ob_freq = 0\n",
    "    for i in range(len(str1)-1):\n",
    "        p1 = str1[i]\n",
    "        p2 = str1[i+1]\n",
    "        if unitSize ==3:\n",
    "            p3 = str1[i+2]\n",
    "        elif unitSize ==4:\n",
    "            p3 = str1[i+2]\n",
    "            p4 = str1[i+3]\n",
    "\n",
    "        if unitSize == 2:\n",
    "            if p1 in str2 and p2 in str2:\n",
    "                i1 = np.nonzero(np.array(str2) == p1)\n",
    "                i2 = np.nonzero(np.array(str2) == p2)\n",
    "                # Directionality. Difference absolute or not absolute\n",
    "                if unidirectional == True:\n",
    "                    if (i2[0] - i1[0]) == 1:\n",
    "                        ob_freq += 1\n",
    "                else:\n",
    "                    if abs(i2[0] - i1[0]) == 1:\n",
    "                        ob_freq += 1                \n",
    "            \n",
    "        elif unitSize == 3:\n",
    "            if p1 in str2 and p2 in str2 and p3 in str2:\n",
    "                i1 = np.nonzero(np.array(str2) == p1)\n",
    "                i2 = np.nonzero(np.array(str2) == p2)\n",
    "                i3 = np.nonzero(np.array(str2) == p3)\n",
    "                # Directionality. Difference absolute or not absolute\n",
    "                if unidirectional == True:\n",
    "                    if (i2[0] - i1[0]) == 1 and (i3[0] - i1[0]) == 2 and (i3[0] - i2[0]) == 1:\n",
    "                        ob_freq += 1\n",
    "                else:\n",
    "                    if abs(i2[0] - i1[0]) <= 2 and abs(i3[0] - i2[0]) <= 2 and abs(i3[0] - i1[0]) <= 2: # I think I might not need the last and\n",
    "                        ob_freq += 1  \n",
    "            \n",
    "        elif unitSize == 4:\n",
    "            if p1 in str2 and p2 in str2 and p3 in str2:\n",
    "                i1 = np.nonzero(np.array(str2) == p1)\n",
    "                i2 = np.nonzero(np.array(str2) == p2)\n",
    "                i3 = np.nonzero(np.array(str2) == p3)\n",
    "                i4 = np.nonzero(np.array(str2) == p4)\n",
    "                # Directionality. Difference absolute or not absolute\n",
    "                if unidirectional == True:\n",
    "                    if (i4[0] - i1[0]) == 3 and (i3[0] - i1[0]) == 2 and (i2[0] - i1[0]) == 1 and \\\n",
    "                    (i4[0] - i2[0]) == 2 and (i4[0] - i3[0] == 1) and (i3[0]-i2[0] == 1):\n",
    "                        ob_freq += 1\n",
    "                else:\n",
    "                    if \\\n",
    "                    abs(i2[0] - i1[0]) <= 3 and abs(i3[0] - i1[0]) <= 3 and abs(i4[0] - i1[0]) <= 3 and \\\n",
    "                    abs(i3[0] - i2[0]) <= 3 and abs(i4[0] - i2[0]) <= 3 and \\\n",
    "                    abs(i4[0] - i3[0]) <= 3: \n",
    "                        ob_freq += 1     \n",
    "           \n",
    "    return ob_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_BB(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    USAGE:\n",
    "    score = expected_BB(str1, str2, unitSize, unidirectional=True)\n",
    "\n",
    "    ARGUMENTS:\n",
    "    str1            The first string\n",
    "    str2            The second string\n",
    "    unitSize        Size of unit of interest (word pair, triple, quartuple etc.) --> Is always 2 in this case\n",
    "    unidirectional  Direction of unit (only 1-2 valid, or also 2-1)\n",
    "\n",
    "    DESCRIPTION:\n",
    "    This calculates the expected organization of word pairs to be used in subsequent shared organization measures\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # This is only for pairs   \n",
    "    if unitSize != 2:\n",
    "        return \"Use different expected calculation for higher order unit sizes\"\n",
    "    \n",
    "    # Calc expected\n",
    "    num_common_items = len(np.intersect1d(str1, str2))\n",
    "    if unidirectional == True:\n",
    "        x = 1\n",
    "    else:\n",
    "        x = 2\n",
    "        \n",
    "    exp_freq = (x*num_common_items*(num_common_items-1)) / float(len(str1)*len(str2))\n",
    "\n",
    "    if num_common_items == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return exp_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_generalized(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    USAGE:\n",
    "    score = expected_BB(str1, str2, unitSize, unidirectional=True)\n",
    "\n",
    "    ARGUMENTS:\n",
    "    str1            The first string\n",
    "    str2            The second string\n",
    "    unitSize        Size of unit of interest (word pair, triple, quartuple etc.)\n",
    "    unidirectional  Direction of unit (only 1-2 valid, or also 2-1)\n",
    "\n",
    "    DESCRIPTION:\n",
    "    This calculates the expected organization of word pairs to be used in subsequent shared organization measures: ((N-X-1)! * A * (M-X + 1-R)) / N!\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    # Calculate expected value according to Boulsfield & Boulsfield\n",
    "    num_common_items = len(np.intersect1d(str1, str2))\n",
    "    if unidirectional == True:\n",
    "        x = 1\n",
    "    else:\n",
    "        x = 2\n",
    "    exp_freq = (x*num_common_items*(num_common_items-1)) / float(len(str1)*len(str2))\n",
    "    #print('x', x)\n",
    "    \n",
    "    return exp_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    USAGE:\n",
    "    score = maximum(str1, str2, unitSize, unidirectional=True)\n",
    "\n",
    "    ARGUMENTS:\n",
    "    str1            The first string\n",
    "    str2            The second string\n",
    "    unitSize        Size of unit of interest (word pair, triple, quartuple etc.)\n",
    "    unidirectional  Direction of unit (only 1-2 valid, or also 2-1)\n",
    "\n",
    "    DESCRIPTION:\n",
    "    This calculates the maximum possible organization of word pairs to be used in subsequent shared organization measures: (c - x + 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_common_items = len(np.intersect1d(str1, str2))\n",
    "\n",
    "    if num_common_items == 0:\n",
    "        return 0\n",
    "    elif num_common_items == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return (num_common_items - unitSize + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A1.2.2. Then, we summarize them in final metrics (e.g., OmE (Pair Frequency/SOMA), OdE, OmEdM, OdM, OmEdMmE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OmE(str1, str2, unitSize, unidirectional=True):   \n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "    Unidirectional & pair = ITR (Bousfield & Bousfield (1966))\n",
    "    Bidirectional & pari = Pair(ed) Frequency (Anderson & Watts (1969); Rosner (1970))\n",
    "    \"\"\"\n",
    "    \n",
    "    if unidirectional==True:\n",
    "        return observed(str1, str2, unitSize, unidirectional=True) - expected_BB(str1, str2, unitSize, unidirectional=True)\n",
    "    else:\n",
    "        return observed(str1, str2, unitSize, unidirectional=False) - expected_BB(str1, str2, unitSize, unidirectional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OdE(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "    Referred to as Sequential Consistency by Sternberg & Tulving (1977), developed by Gorfein, Blair, & Rowland (1968)\n",
    "    \"\"\"\n",
    "        \n",
    "    if expected_BB(str1, str2, unitSize, unidirectional=True)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        if unidirectional==True:\n",
    "            return observed(str1, str2, unitSize, unidirectional=True) / expected_BB(str1, str2, unitSize, unidirectional=True)\n",
    "        else:\n",
    "            return observed(str1, str2, unitSize, unidirectional=False) / expected_BB(str1, str2, unitSize, unidirectional=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OmEdM(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "    Referred to as Sequential Consistency by Sternberg & Tulving (1977)\n",
    "    Unidirectional pairs, Fagan (1968)\n",
    "    Bidirectional pairs, Postman (1970)\n",
    "    \"\"\"\n",
    "    \n",
    "    if maximum(str1, str2, unitSize, unidirectional=True)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        if unidirectional==True:\n",
    "            return ((observed(str1, str2, unitSize, unidirectional=True) - expected_BB(str1, str2, unitSize, unidirectional=True)) / maximum(str1, str2, unitSize, unidirectional=True))\n",
    "        else:\n",
    "            return ((observed(str1, str2, unitSize, unidirectional=False) - expected_BB(str1, str2, unitSize, unidirectional=False)) / maximum(str1, str2, unitSize, unidirectional=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OdM(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "    For bidirectional pairs referred to as ITR(2) (Mandler & Dean (1969))\n",
    "    \"\"\"\n",
    "    \n",
    "    if maximum(str1, str2, unitSize, unidirectional=True)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        if unidirectional==True:\n",
    "            return (observed(str1, str2, unitSize, unidirectional=True) / maximum(str1, str2, unitSize, unidirectional=True))\n",
    "        else:\n",
    "            return (observed(str1, str2, unitSize, unidirectional=False) / maximum(str1, str2, unitSize, unidirectional=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OmEdMmE(str1, str2, unitSize, unidirectional=True):\n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "    Generally referred to as ARC' (Pellegrino (1971); Pellegrino & Battig (1974))\n",
    "    \"\"\"\n",
    "    \n",
    "    if maximum(str1, str2, unitSize, unidirectional=True)==0 and expected_BB(str1, str2, unitSize, unidirectional=True)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        if unidirectional==True:\n",
    "            return ((observed(str1, str2, unitSize, unidirectional=True) - expected_BB(str1, str2, unitSize, unidirectional=True)) / (maximum(str1, str2, unitSize, unidirectional=True) - expected_BB(str1, str2, unitSize, unidirectional=True)))\n",
    "        else:\n",
    "            return ((observed(str1, str2, unitSize, unidirectional=False) - expected_BB(str1, str2, unitSize, unidirectional=False)) / (maximum(str1, str2, unitSize, unidirectional=False) - expected_BB(str1, str2, unitSize, unidirectional=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __A1.3. Edit distance measures__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_dists(string1, string2, insert=True, delete=True, substitute=True, transpose=True):\n",
    "\n",
    "    if len(string1) > len(string2):\n",
    "        string1, string2 = string2, string1\n",
    "\n",
    "    n1 = len(string1)\n",
    "    n2 = len(string2)\n",
    "    d = np.zeros((n1 + 1, n2 + 1), dtype=int)\n",
    "\n",
    "    for i in range(n1 + 1):\n",
    "        d[i, 0] = i\n",
    "\n",
    "    for j in range(n2 + 1):\n",
    "        d[0, j] = j\n",
    "\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            options = []\n",
    "            # insertion\n",
    "            if insert:\n",
    "                options += [d[i, j+1] + 1]\n",
    "            # deletion\n",
    "            if delete:\n",
    "                options += [d[i+1, j] + 1]\n",
    "            # substitution\n",
    "            if substitute and not(string1[i] == string2[j]):\n",
    "                options += [d[i, j] + 1]\n",
    "            # identical entries are free\n",
    "            elif string1[i] == string2[j]:\n",
    "                options += [d[i, j] + 0]\n",
    "\n",
    "            d[i+1, j+1] = min(options)\n",
    "            #d[i+1, j+1] = min(d[i, j+1] + 1, # insert\n",
    "            #                  d[i+1, j] + 1, # delete\n",
    "            #                  d[i, j] + cost) # replace\n",
    "\n",
    "            if transpose:\n",
    "                if i > 0 and j > 0 and string1[i] == string2[j-1] and string1[i-1] == string2[j]:\n",
    "                    d[i+1, j+1] = min(d[i+1, j+1], d[i-1, j-1] + int(not(string1[i] == string2[j]))) # transpose\n",
    "\n",
    "\n",
    "    # if substitution, max # of edits is max(n1, n2)\n",
    "\n",
    "    if substitute:\n",
    "        return 1 - ( d[n1, n2] / max(n1, n2) )\n",
    "\n",
    "    # otherwise, it's n1 + n2 (delete each of s1, then insert each of s2)\n",
    "    else:\n",
    "        return 1 - ( d[n1, n2] / (n1 + n2) )\n",
    "    #return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1.4. Graveyard for old similarity measure functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairedFreq(str2, str1):\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(str2) == 0:\n",
    "        return []\n",
    "    ob_freq = 0\n",
    "    for i in range(len(str2)-1):\n",
    "        p1 = str2[i]\n",
    "        p2 = str2[i+1]\n",
    "        if p1 in str1 and p2 in str1:\n",
    "            i1 = np.nonzero(np.array(str1) == p1)\n",
    "            i2 = np.nonzero(np.array(str1) == p2)\n",
    "            if abs(i1[0] - i2[0]) == 1:\n",
    "                ob_freq += 1\n",
    "                #print(ob_freq)\n",
    "    #print(str2)\n",
    "    #print(str1)\n",
    "    #print(\"ob_freq=\", ob_freq)\n",
    "    \n",
    "    if str2 == []:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    # This is the code from Christian\n",
    "    # num_common_items = len(final)\n",
    "    # \n",
    "    # num_common_items = np.intersect1d(final, orig)\n",
    "    # print(\"num_common_items=\", num_common_items)\n",
    "    # \n",
    "    # if type(num_common_items) == np.dtype(int):\n",
    "    #    num_common_items = 1\n",
    "    # else:\n",
    "    #    print(num_common_items)\n",
    "    #    num_common_items = len(num_common_items)\n",
    "\n",
    "    num_common_items = len(np.intersect1d(str1, str2)) #Alex New\n",
    "\n",
    "    exp_freq = (2*num_common_items*(num_common_items-1)) / float(len(str2)*len(str1))\n",
    "    #print(\"exp_freq=\", exp_freq)\n",
    "    PF = ob_freq - exp_freq\n",
    "    #print(\"PF=\", PF)\n",
    "    return PF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized observed bidirectional Inter-Trial-Repetition (O(ITR2) - Max(ITR2)) (by Alex, build on adapted PF from Christian)\n",
    "# \"The maximum ITR value is a function of the number of items common to both sets of events and does not depend on the absolute \n",
    "# number of words recalled or presented. It is equal to the number of items common to both events minus one.\" (Mandler & Dean, 1969)\n",
    "def ITR2(str1, str2, shortest = True):\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(str2) == 0:\n",
    "        return []\n",
    "    ob_freq = 0\n",
    "    for i in range(len(str2)-1):\n",
    "        p1 = str2[i]\n",
    "        p2 = str2[i+1]\n",
    "        if p1 in str1 and p2 in str1:\n",
    "            i1 = np.nonzero(np.array(str1) == p1)\n",
    "            i2 = np.nonzero(np.array(str1) == p2)\n",
    "            if abs(i1[0] - i2[0]) == 1:\n",
    "                ob_freq += 1\n",
    "    #print(\"ob_freq=\", ob_freq)\n",
    "    \n",
    "    if str2 == []:\n",
    "        return 0\n",
    "\n",
    "    num_common_items = len(np.intersect1d(str1, str2))\n",
    "    if num_common_items == 0: #Check with Christian\n",
    "        ITR2 = 0\n",
    "    elif num_common_items == 1:\n",
    "        ITR2 = 0\n",
    "    else:\n",
    "        ITR2 = (ob_freq / (num_common_items - 1)) #M(ITR) = M(ITR2) = c-1\n",
    "    \n",
    "    return ITR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different normalization for observed bidirectional Inter-Trial-Repetition \n",
    "# (O(ITR2) - E(ITR2)) / (M(ITR2) - E(ITR2)) (by Alex, build on adapted PF from Christian)\n",
    "def ARC2(str1, str2):\n",
    "    \n",
    "    # Transform to correct input format, if it isn't already a list or a numpy array\n",
    "    try:\n",
    "        str1 = str1.values.tolist()\n",
    "        str2 = str2.values.tolist()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(str2) == 0:\n",
    "        return []\n",
    "    ob_freq = 0\n",
    "    for i in range(len(str2)-1):\n",
    "        p1 = str2[i]\n",
    "        p2 = str2[i+1]\n",
    "        if p1 in str1 and p2 in str1:\n",
    "            i1 = np.nonzero(np.array(str1) == p1)\n",
    "            i2 = np.nonzero(np.array(str1) == p2)\n",
    "            if abs(i1[0] - i2[0]) == 1:\n",
    "                ob_freq += 1\n",
    "    #print(\"ob_freq=\", ob_freq)\n",
    "    \n",
    "    if str2 == []:\n",
    "        return 0\n",
    "\n",
    "    num_common_items = len(np.intersect1d(str1, str2))\n",
    "    #print(\"num_common_items=\", num_common_items) \n",
    "\n",
    "    exp_freq = (2*num_common_items*(num_common_items-1)) / float(len(str2)*len(str1))\n",
    "    #print(\"exp_freq=\", exp_freq) \n",
    "\n",
    "    max_freq = (num_common_items - 1)\n",
    "    #print(\"max_freq=\", max_freq) \n",
    "    \n",
    "    if (max_freq - exp_freq)== 0 : # Double check with Christian (happens when c=1)\n",
    "        ARC2 = 0\n",
    "        #print('watch out')\n",
    "    else:\n",
    "        ARC2 = (ob_freq - exp_freq) / (max_freq - exp_freq)\n",
    "    #print(ob_freq, exp_freq, max_freq)\n",
    "    return ARC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editdist(str1, str2, min_threshold = None):\n",
    "  #\"\"\"Return approximate string comparator measure (between 0.0 and 1.0)\n",
    "   #  using the edit (or Levenshtein) distance.\n",
    "\n",
    "#  USAGE:\n",
    "#    score = editdist(str1, str2, min_threshold)\n",
    "\n",
    "#  ARGUMENTS:\n",
    "#    str1           The first string\n",
    "#    str2           The second string\n",
    "#    min_threshold  Minimum threshold between 0 and 1\n",
    "#\n",
    "#  DESCRIPTION:\n",
    "#    The edit distance is the minimal number of insertions, deletions and\n",
    "#    substitutions needed to make two strings equal.#\n",
    "#\n",
    "#    For more information on the modified Soundex see:\n",
    "#    - http://www.nist.gov/dads/HTML/editdistance.html\n",
    "#  \"\"\"\n",
    "\n",
    "#  # Quick check if the strings are empty or the same - - - - - - - - - - - - -\n",
    "#  #\n",
    "#  #if (str1 == '') or (str2 == ''):\n",
    "    if (str1.size == 0) or (str2.size == 0):\n",
    "        return 0.0\n",
    "    #if len(str1) == len(str2) and len(str1) > 1 and all(str1 == str2):\n",
    "    #    return 1.0\n",
    "    #elif len(str1) == len(str2) and len(str1) == 1 and (str1 == str2):\n",
    "    #    return 1.0\n",
    "\n",
    "    n = len(str1)\n",
    "    m = len(str2)\n",
    "    max_len = max(n,m)\n",
    "\n",
    "    if (min_threshold != None):\n",
    "        if (isinstance(min_threshold, float)) and (min_threshold > 0.0) and (min_threshold > 0.0):\n",
    "\n",
    "            len_diff = abs(n-m)\n",
    "            w = 1.0 - float(len_diff) / float(max_len)\n",
    "\n",
    "            if (w  < min_threshold):\n",
    "                return 0.0  # Similariy is smaller than minimum threshold\n",
    "\n",
    "        else: # Calculate the maximum distance possible with this threshold\n",
    "            max_dist = (1.0-min_threshold)*max_len\n",
    "\n",
    "    else:\n",
    "        logging.exception('Illegal value for minimum threshold (not between' + \\\n",
    "                        ' 0 and 1): %f' % (min_threshold))\n",
    "        raise Exception\n",
    "\n",
    "    if (n > m):  # Make sure n <= m, to use O(min(n,m)) space\n",
    "        str1, str2 = str2, str1\n",
    "        n, m =       m, n\n",
    "\n",
    "    current = range(n+1)\n",
    "\n",
    "    for i in range(1, m+1):\n",
    "\n",
    "        previous = current\n",
    "        current =  [i]+n*[0]\n",
    "        str2char = str2[i-1]\n",
    "\n",
    "        for j in range(1,n+1):\n",
    "            substitute = previous[j-1]\n",
    "            if (str1[j-1] != str2char):\n",
    "                substitute += 1\n",
    "\n",
    "      # Get minimum of insert, delete and substitute\n",
    "      #\n",
    "            current[j] = min(previous[j]+1, current[j-1]+1, substitute)\n",
    "\n",
    "        if (min_threshold != None) and (min(current) > max_dist):\n",
    "            return 1.0 - float(max_dist+1) / float(max_len)\n",
    "\n",
    "    w = 1.0 - float(current[n]) / float(max_len)\n",
    "\n",
    "    assert (w >= 0.0) and (w <= 1.0), 'Similarity weight outside 0-1: %f' % (w)\n",
    "\n",
    "  # A log message - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "  #\n",
    "    logging.debug('Edit-distance comparator string \"%s\" with \"%s\" value: %.3f' \\\n",
    "                % (str1, str2, w))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_editdist(str1, str2, min_threshold = None):\n",
    "#Return approximate string comparator measure (between 0.0 and 1.0)\n",
    "#     using a modified edit (or Levenshtein) distance that counts transpositions\n",
    "#     as elementary operations as well. This is also called the Damerau-\n",
    "#     Levenshtein distance.\n",
    "\n",
    "#  USAGE:\n",
    "#    score = mod_editdist(str1, str2, min_threshold)\n",
    "\n",
    "#  ARGUMENTS:\n",
    "#    str1           The first string\n",
    "#    str2           The second string\n",
    "#    min_threshold  Minimum threshold between 0 and 1\n",
    "\n",
    "#  DESCRIPTION:\n",
    "#    The edit distance is the minimal number of insertions, deletions,\n",
    "#    substitutions and transpositions needed to make two strings equal.\n",
    "\n",
    "#    Compared to the original editdist function, which handles a transposition\n",
    "#    (like: 'sydney' <-> 'sydeny' as 2 operations (two substitutions or one\n",
    "#    insert and one delet), this modified version handles this as 1 operation.\n",
    "\n",
    "#    Based on code from Justin Zobel's 'vrank'.\n",
    "    \n",
    "# Quick check if the strings are empty or the same - - - - - - - - - - - - -\n",
    "#\n",
    "#if (str1 == '') or (str2 == ''):\n",
    "#print([str1, str2])\n",
    "    #assert(0)\n",
    "    if (str1.size == 0) or (str2.size == 0):\n",
    "        return 0.0\n",
    "    #elif (str1 == str2):\n",
    "    elif np.array_equal(str1, str2):\n",
    "        return 1.0\n",
    "\n",
    "    n = len(str1)\n",
    "    m = len(str2)\n",
    "    max_len = max(n,m)\n",
    "    #print('n', n)\n",
    "    #print('m', m)\n",
    "    #print('max_len',max_len)\n",
    "\n",
    "    if (min_threshold != None):\n",
    "        if (isinstance(min_threshold, float)) and (min_threshold > 0.0) and (min_threshold > 0.0): #I don't get this one\n",
    "        \n",
    "            len_diff = abs(n-m)\n",
    "            w = 1.0 - float(len_diff) / float(max_len)\n",
    "\n",
    "            if (w  < min_threshold):\n",
    "                return 0.0  # Similariy is smaller than minimum threshold\n",
    "\n",
    "        else: # Calculate the maximum distance possible with this threshold\n",
    "            max_dist = (1.0-min_threshold)*max_len\n",
    "\n",
    "    else:\n",
    "        logging.exception('Illegal value for minimum threshold (not between' + ' 0 and 1): %f' % (min_threshold))\n",
    "        raise Exception\n",
    "\n",
    "    if (n > m):  # Make sure n <= m, to use O(min(n,m)) space\n",
    "        str1, str2 = str2, str1\n",
    "        n, m =       m, n\n",
    "\n",
    "    d = []  # Table with the full distance matrix\n",
    "\n",
    "    current = range(n+1)\n",
    "    d.append(current)\n",
    "\n",
    "    for i in range(1,m+1):\n",
    "\n",
    "        previous = current\n",
    "        current =  [i]+n*[0]\n",
    "        str2char = str2[i-1]\n",
    "\n",
    "        for j in range(1,n+1):\n",
    "            substitute = previous[j-1]\n",
    "            if (str1[j-1] != str2char):\n",
    "                substitute += 1\n",
    "\n",
    "            if (i == 1) or (j == 1):  # First characters, no transposition possible\n",
    "\n",
    "            # Get minimum of insert, delete and substitute\n",
    "            #\n",
    "                current[j] = min(previous[j]+1, current[j-1]+1, substitute)\n",
    "\n",
    "            else:\n",
    "                if (str1[j-2] == str2[i-1]) and (str1[j-1] == str2[i-2]):\n",
    "                    transpose = d[i-2][j-2] + 1\n",
    "                else:\n",
    "                    transpose = d[i-2][j-2] + 3\n",
    "\n",
    "                current[j] = min(previous[j]+1, current[j-1]+1, substitute, transpose)\n",
    "\n",
    "        d.append(current)\n",
    "\n",
    "        if (min_threshold != None) and (min(current) > max_dist):\n",
    "            return 1.0 - float(max_dist+1) / float(max_len)\n",
    "\n",
    "    w = 1.0 - float(current[n]) / float(max_len)\n",
    "    \n",
    "    assert (w >= 0.0) and (w <= 1.0), 'Similarity weight outside 0-1: %f' % (w)\n",
    "\n",
    "  # A log message - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "  #\n",
    "    logging.debug('Modified edit-distance comparator string \"%s\" with \"%s\" ' % \\\n",
    "                (str1, str2) + 'value: %.3f' % (w))\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "## __B. Project specific: Data wrangling for experiments__\n",
    "### __B1. Experiment & Data overview__\n",
    "* 2 phases: individual or collaborative recall --> individual recall\n",
    "* 2 \"real\" variables: individual/collaborative recall; biased/non-biased (deep/shallow encoding)\n",
    "    * Biased encoding was established by having participants encode half of the lists shallowly and the other half deeply. The unbiased condition encoded all lists deeply. \n",
    "        * Encoding files: 0 at the end = unbiased, 1 at the end = biased\n",
    "    * Collaboration:\n",
    "        * if the middle number is 0 = individual retrieval, middle number 1 = collaborative retrieval\n",
    "        * If last number is 1 = first retrieval phase, if last number is 2 = second retrieval phase \n",
    "    * Always a biased and unbiased participant collaborated\n",
    "* 1 \"counter balancing\" variables were considered: word list 1 or 2 which is referred to as \"order\"    \n",
    "    * If subject number is odd and biased, word lists 1 to 7 were encoded shallowly\n",
    "    * If subject number is even and biased, word lists 8 to 14 were encoded shallowly\n",
    "    * If unbiased, all 14 lists were encoded deeply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __B2. Pickle file conversion (old --> new)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------DESCRIPTION----------------------------------------\n",
    "# Convert files to new pickle files\n",
    "\n",
    "#------------------------------SCRIPT---------------------------------------------\n",
    "first = 1\n",
    "last = 192193\n",
    "skip = []\n",
    "\n",
    "# Encoding\n",
    "for subjectNumber in range(first, last+1):\n",
    "\n",
    "    if subjectNumber in skip:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        original = ('encoding_' + str(subjectNumber) +'_1.pkl')\n",
    "        destination = ('new_encoding_' + str(subjectNumber) +'_1.pkl')\n",
    "\n",
    "        content = ''\n",
    "        outsize = 0\n",
    "        with open(original, 'rb') as infile:\n",
    "            content = infile.read()\n",
    "        with open(destination, 'wb') as output:\n",
    "            for line in content.splitlines():\n",
    "                outsize += len(line) + 1\n",
    "                output.write(line + str.encode('\\n'))\n",
    "\n",
    "        with open(destination, 'rb') as f:\n",
    "            try:\n",
    "                test = pickle.load(f, encoding='latin1')\n",
    "                print(test)\n",
    "            except EOFError:\n",
    "                pass\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "print(\"Done. Saved %s bytes.\" % (len(content)-outsize))\n",
    "\n",
    "for subjectNumber in range(first, last+1):\n",
    "\n",
    "    if subjectNumber in skip:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        original = ('encoding_' + str(subjectNumber) +'_0.pkl')\n",
    "        destination = ('new_encoding_' + str(subjectNumber) +'_0.pkl')\n",
    "\n",
    "        content = ''\n",
    "        outsize = 0\n",
    "        with open(original, 'rb') as infile:\n",
    "            content = infile.read()\n",
    "        with open(destination, 'wb') as output:\n",
    "            for line in content.splitlines():\n",
    "                outsize += len(line) + 1\n",
    "                output.write(line + str.encode('\\n'))\n",
    "\n",
    "        with open(destination, 'rb') as f:\n",
    "            try:\n",
    "                test = pickle.load(f, encoding='latin1')\n",
    "                print(test)\n",
    "            except EOFError:\n",
    "                pass\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "print(\"Done. Saved %s bytes.\" % (len(content)-outsize))\n",
    "\n",
    "\n",
    "# Retrieval 1\n",
    "for subjectNumber in range(first, last+1):\n",
    "\n",
    "    if subjectNumber in skip:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        original = ('retrieve_' + str(subjectNumber) +'_0_1.pkl')\n",
    "        destination = ('new_retrieve_' + str(subjectNumber) +'_0_1.pkl')\n",
    "\n",
    "        content = ''\n",
    "        outsize = 0\n",
    "        with open(original, 'rb') as infile:\n",
    "            content = infile.read()\n",
    "        with open(destination, 'wb') as output:\n",
    "            for line in content.splitlines():\n",
    "                outsize += len(line) + 1\n",
    "                output.write(line + str.encode('\\n'))\n",
    "\n",
    "        with open(destination, 'rb') as f:\n",
    "            try:\n",
    "                test = pickle.load(f, encoding='latin1')\n",
    "                print(test)\n",
    "            except EOFError:\n",
    "                pass\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "for subjectNumber in range(first, last+1):\n",
    "\n",
    "    if subjectNumber in skip:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        original = ('retrieve_' + str(subjectNumber) +'_1_1.pkl')\n",
    "        destination = ('new_retrieve_' + str(subjectNumber) +'_1_1.pkl')\n",
    "\n",
    "        content = ''\n",
    "        outsize = 0\n",
    "        with open(original, 'rb') as infile:\n",
    "            content = infile.read()\n",
    "        with open(destination, 'wb') as output:\n",
    "            for line in content.splitlines():\n",
    "                outsize += len(line) + 1\n",
    "                output.write(line + str.encode('\\n'))\n",
    "\n",
    "        with open(destination, 'rb') as f:\n",
    "            try:\n",
    "                test = pickle.load(f, encoding='latin1')\n",
    "                print(test)\n",
    "            except EOFError:\n",
    "                pass\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "print(\"Done. Saved %s bytes.\" % (len(content)-outsize))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Retrieval 2\n",
    "for subjectNumber in range(first, last+1):\n",
    "\n",
    "    if subjectNumber in skip:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        original = ('retrieve_' + str(subjectNumber) +'_0_2.pkl')\n",
    "        destination = ('new_retrieve_' + str(subjectNumber) +'_0_2.pkl')\n",
    "\n",
    "        content = ''\n",
    "        outsize = 0\n",
    "        with open(original, 'rb') as infile:\n",
    "            content = infile.read()\n",
    "        with open(destination, 'wb') as output:\n",
    "            for line in content.splitlines():\n",
    "                outsize += len(line) + 1\n",
    "                output.write(line + str.encode('\\n'))\n",
    "\n",
    "        with open(destination, 'rb') as f:\n",
    "            try:\n",
    "                test = pickle.load(f, encoding='latin1')\n",
    "                print(test)\n",
    "            except EOFError:\n",
    "                pass\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "print(\"Done. Saved %s bytes.\" % (len(content)-outsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __B3. Data frame creation (incl. spell check); to be used for subsequent analysis__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afron\\.conda\\envs\\Similarity\\lib\\site-packages\\ipykernel_launcher.py:385: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\afron\\.conda\\envs\\Similarity\\lib\\site-packages\\ipykernel_launcher.py:393: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------Input----------------------------------------------\n",
    "studyList1 = ['crow','eagle','finch','parrot','pigeon','cardinal','nitrogen','helium','chlorine','calcium','oxygen','mercury','trout',\\\n",
    "              'flounder','halibut','guppy','piranha','shark','carnation','orchid','pansy','daffodil','violet','rose','nectarine','pear',\\\n",
    "              'apple','grape','raspberry','cherry','tuba','drum','trumpet','saxophone','piano','organ','tree','ocean','canyon','mountain',\\\n",
    "              'plateau','cave','cinnamon','mustard','basil','oregano','paprika','cotton','wool', 'velvet','linen','leather','flyer',\\\n",
    "              'newspaper','comic','essay','pamphlet','tornado','hail','blizzard','rain','drought','jacket','dress','blouse','underwear',\\\n",
    "              'shoes','lamp','desk','bookcase','dresser','chair','banker','dentist','secretary','engineer','nurse','hour', 'arms', 'green',\\\n",
    "              'uncle']\n",
    "    \n",
    "studyList2 = ['crow','eagle','finch','parrot','pigeon','nitrogen','helium','chlorine','calcium','oxygen','trout','flounder','halibut',\\\n",
    "              'guppy','piranha','carnation','orchid','pansy','daffodil','violet','nectarine','pear','apple','grape','raspberry','tuba',\\\n",
    "              'drum','trumpet','saxophone','piano','tree','ocean','canyon','mountain','plateau','cinnamon','mustard','basil','oregano',\\\n",
    "              'paprika','salt','cotton','wool','velvet','linen','leather','denim','flyer','newspaper','comic','essay','pamphlet','book',\\\n",
    "              'tornado','hail','blizzard','rain','drought','lightning','jacket','dress','blouse','underwear','shoes','shirt','lamp','desk',\\\n",
    "              'bookcase','dresser','chair','recliner','banker','dentist','secretary','engineer','nurse','doctor','hour', 'arms', 'green', 'uncle']    \n",
    "    \n",
    "#-------------------------------------Create Data Frame----------------------------------------------\n",
    "\n",
    "df = pd.DataFrame(columns=['SN', 'biased','order', 'collaboration', 'collaborator', 'word', 'correct', 'buffer', 'phase'])\n",
    "\n",
    "first = 1\n",
    "last = 194\n",
    "skip = [25, 92, 115, 126, 136, 147]\n",
    "# retrieve_25_0_2.pkl does not exist and also no dlm file: Both recalls saved in same dlm file --> manually changed and saved in new dlm file\n",
    "# retrieve_126_0_2.pkl does not exist and also no dlm file: Mislabeled as retrieve_126_2_0.dlm --> manually changed and saved in new dlm file\n",
    "# retrieve_147_0_2.pkl does not exist and also no dlm file: retrieve_147_0_1.pkl is retrieve_147_0_2.pkl --> manually changed (same with dlm)\n",
    "#                                                           (proof for mislabeling: s this participant collaborated so retrieve_146147_1_1.dlm)\n",
    "# These three files will be processed and added separately in the next step/cell\n",
    "\n",
    "# retrieve_92_0_2.pkl does not exist and also no dlm file: Is completely missing \n",
    "# retrieve_115_0_2.pkl does not exist and also no dlm file: Is completely missing\n",
    "# retrieve_136_0_2.pkl does not exist and also no dlm file: Is completely missing\n",
    "\n",
    "\n",
    "# A: Any subjects we should exclude?\n",
    "for subjectNumber in range(first, last+1):\n",
    "\n",
    "    if subjectNumber in skip:\n",
    "        continue\n",
    "\n",
    "    # A: Import all words from all participants for recall phase 2\n",
    "    with open('new_retrieve_' + str(subjectNumber) +'_0_2.pkl', 'rb') as f:\n",
    "        try:\n",
    "            exp_data = pickle.load(f, encoding='latin1')\n",
    "        except EOFError:\n",
    "            pass\n",
    "\n",
    "        for trial in exp_data:\n",
    "            biased = 0\n",
    "            order = 0\n",
    "            collaboration = 0\n",
    "            phase = 2\n",
    "            correct = 0\n",
    "            buffer = 0\n",
    "\n",
    "            # Select the word from the pickle files\n",
    "            word = trial['word']\n",
    "            \n",
    "            # Spell checking\n",
    "            for x in word:\n",
    "                word = word.lower()\n",
    "                #word = word.strip()\n",
    "                word = word.replace(\"\\\\\", '')\n",
    "                #word = ''.join('DELETE' if word == '\\\\t' else word for word in word.split())\n",
    "                # From experiment 1\n",
    "                word = ''.join('canyon' if word == 'canyoan' else word for word in word.split())\n",
    "                word = ''.join('canyon' if word == 'canyons' else word for word in word.split())\n",
    "                word = ''.join('cherry' if word == 'cherries' else word for word in word.split())\n",
    "                word = ''.join('chlorine' if word == 'clorine' else word for word in word.split())      \n",
    "                word = ''.join('cinnamon' if word == 'cinammon' else word for word in word.split())  \n",
    "                word = ''.join('cinnamon' if word == 'cinnaman' else word for word in word.split())      \n",
    "                word = ''.join('cinnamon' if word == 'cinnimon' else word for word in word.split())\n",
    "                word = ''.join('cinnamon' if word == 'cinomman' else word for word in word.split())\n",
    "                word = ''.join('cotton' if word == 'cotten' else word for word in word.split())\n",
    "                word = ''.join('daffodil' if word == 'daffidil' else word for word in word.split())\n",
    "                word = ''.join('daffodil' if word == 'dafodill' else word for word in word.split())      \n",
    "                word = ''.join('daffodil' if word == 'daphodile' else word for word in word.split())\n",
    "                word = ''.join('daffodil' if word == 'doffodil' else word for word in word.split())   \n",
    "                word = ''.join('denim' if word == 'denin' else word for word in word.split())\n",
    "                word = ''.join('drum' if word == 'drums' else word for word in word.split())\n",
    "                word = ''.join('engineer' if word == 'enginner' else word for word in word.split())\n",
    "                word = ''.join('flounder' if word == 'flunder' else word for word in word.split())\n",
    "                word = ''.join('flyer' if word == 'flier' else word for word in word.split())      \n",
    "                word = ''.join('flyer' if word == 'flyers' else word for word in word.split())\n",
    "                word = ''.join('grape' if word == 'grapes' else word for word in word.split())   \n",
    "                word = ''.join('guppy' if word == 'gupppy' else word for word in word.split())\n",
    "                word = ''.join('halibut' if word == 'hailbut' else word for word in word.split())\n",
    "                word = ''.join('halibut' if word == 'halibet' else word for word in word.split())\n",
    "                word = ''.join('linen' if word == 'linens' else word for word in word.split())      \n",
    "                word = ''.join('linen' if word == 'linnen' else word for word in word.split())\n",
    "                word = ''.join('mercury' if word == 'ercury' else word for word in word.split())   \n",
    "                word = ''.join('mountain' if word == 'mountains' else word for word in word.split())\n",
    "                word = ''.join('mountain' if word == 'moutain' else word for word in word.split())\n",
    "                word = ''.join('nectarine' if word == 'necratine' else word for word in word.split())\n",
    "                word = ''.join('nectarine' if word == 'necterine' else word for word in word.split())      \n",
    "                word = ''.join('orchid' if word == 'orchad' else word for word in word.split())\n",
    "                word = ''.join('oregano' if word == 'aregano' else word for word in word.split())   \n",
    "                word = ''.join('oregano' if word == 'orageno' else word for word in word.split())\n",
    "                word = ''.join('oregano' if word == 'regano' else word for word in word.split())\n",
    "                word = ''.join('pamphlet' if word == 'amphlet' else word for word in word.split())\n",
    "                word = ''.join('pamphlet' if word == 'pamplet' else word for word in word.split())      \n",
    "                word = ''.join('pamphlet' if word == 'panplet' else word for word in word.split())\n",
    "                word = ''.join('pamphlet' if word == 'phamlet' else word for word in word.split())\n",
    "                word = ''.join('pansy' if word == 'ansy' else word for word in word.split())\n",
    "                word = ''.join('paprika' if word == 'paparika' else word for word in word.split())\n",
    "                word = ''.join('paprika' if word == 'papprika' else word for word in word.split())\n",
    "                word = ''.join('paprika' if word == 'paprica' else word for word in word.split())      \n",
    "                word = ''.join('paprika' if word == 'peprica' else word for word in word.split())\n",
    "                word = ''.join('pigeon' if word == 'pegion' else word for word in word.split())\n",
    "                word = ''.join('pigeon' if word == 'pidgeon' else word for word in word.split())\n",
    "                word = ''.join('pigeon' if word == 'pidgeons' else word for word in word.split())\n",
    "                word = ''.join('pigeon' if word == 'pidgieon' else word for word in word.split())      \n",
    "                word = ''.join('piranha' if word == 'paranha' else word for word in word.split())\n",
    "                word = ''.join('piranha' if word == 'pirahana' else word for word in word.split())      \n",
    "                word = ''.join('piranha' if word == 'pirahna' else word for word in word.split())\n",
    "                word = ''.join('piranha' if word == 'pirahnna' else word for word in word.split())\n",
    "                word = ''.join('piranha' if word == 'pirannah' else word for word in word.split())\n",
    "                word = ''.join('piranha' if word == 'pirannha' else word for word in word.split())      \n",
    "                word = ''.join('piranha' if word == 'pirhana' else word for word in word.split())\n",
    "                word = ''.join('piranha' if word == 'pirranha' else word for word in word.split())   \n",
    "                word = ''.join('plateau' if word == 'plateu' else word for word in word.split())\n",
    "                word = ''.join('plateau' if word == 'platue' else word for word in word.split())\n",
    "                word = ''.join('raspberry' if word == 'rasberries' else word for word in word.split())\n",
    "                word = ''.join('raspberry' if word == 'rasberry' else word for word in word.split())      \n",
    "                word = ''.join('raspberry' if word == 'raspberries' else word for word in word.split())\n",
    "                word = ''.join('raspberry' if word == 'rasphberry' else word for word in word.split())   \n",
    "                word = ''.join('saxophone' if word == 'saxaphone' else word for word in word.split())\n",
    "                word = ''.join('saxophone' if word == 'saxephone' else word for word in word.split())\n",
    "                word = ''.join('saxophone' if word == 'saxiphone' else word for word in word.split())\n",
    "                word = ''.join('secretary' if word == 'secratary' else word for word in word.split())      \n",
    "                word = ''.join('secretary' if word == 'secrertary' else word for word in word.split())\n",
    "                word = ''.join('shoes' if word == 'shoe' else word for word in word.split())   \n",
    "                word = ''.join('trumpet' if word == 'trumphet' else word for word in word.split())\n",
    "                word = ''.join('trumpet' if word == 'tumpet' else word for word in word.split())\n",
    "                word = ''.join('underwear' if word == 'nderwear' else word for word in word.split())\n",
    "                word = ''.join('underwear' if word == 'udnerwear' else word for word in word.split())      \n",
    "                word = ''.join('bookcase' if word == 'bookshelf' else word for word in word.split())\n",
    "                word = ''.join('chair' if word == 'chari' else word for word in word.split())   \n",
    "                word = ''.join('lightning' if word == 'lightening' else word for word in word.split())\n",
    "                word = ''.join('lightning' if word == 'lightenings' else word for word in word.split())\n",
    "                word = ''.join('lightning' if word == 'lighting' else word for word in word.split())\n",
    "                word = ''.join('lightning' if word == 'lightning' else word for word in word.split())\n",
    "                word = ''.join('lightning' if word == 'lightnening' else word for word in word.split())      \n",
    "                word = ''.join('tree' if word == 'tress' else word for word in word.split())\n",
    "                word = ''.join('violet' if word == 'voilet' else word for word in word.split())\n",
    "                word = ''.join('blizzard' if word == 'blizarrd' else word for word in word.split())            \n",
    "                word = ''.join('velvet' if word == 'velvey' else word for word in word.split())\n",
    "                word = ''.join('daffodil' if word == 'dafodil' else word for word in word.split())\n",
    "                word = ''.join('halibut' if word == 'halibit' else word for word in word.split())\n",
    "                word = ''.join('book' if word == 'books' else word for word in word.split())\n",
    "                word = ''.join('hurricane' if word == 'hurricaine' else word for word in word.split())\n",
    "                # From experiment 2\n",
    "                word = ''.join('' if word == 'ado' else word for word in word.split()) # one row was ado, second tornao, third tornado               \n",
    "                word = ''.join('dandelion' if word == 'dandalione' else word for word in word.split())                \n",
    "                word = ''.join('dandelion' if word == 'dandeline' else word for word in word.split())                \n",
    "                word = ''.join('dandelion' if word == 'dandelion' else word for word in word.split())                \n",
    "                word = ''.join('dandelion' if word == 'dandelion' else word for word in word.split())                  \n",
    "                word = ''.join('dandelion' if word == 'dandalione' else word for word in word.split())                \n",
    "                word = ''.join('dandelion' if word == 'dandilion' else word for word in word.split())                  \n",
    "                #word = ''.join('finch' if word == 'flinch' else word for word in word.split()) as flinch has a different meaning, we didn't change it                   \n",
    "                word = ''.join('oregano' if word == 'gano' else word for word in word.split()) \n",
    "                word = ''.join('halibut' if word == 'halbait' else word for word in word.split())                \n",
    "                #word = ''.join('hail' if word == 'hale' else word for word in word.split()) as hale has a different meaning, we didn't change it                \n",
    "                #word = ''.join('linen' if word == 'lenin' else word for word in word.split()) as lenin has a different meaning, we didn't change it                 \n",
    "                word = ''.join('velvet' if word == 'lvet' else word for word in word.split())                  \n",
    "                word = ''.join('mountain' if word == 'ntain' else word for word in word.split())                \n",
    "                word = ''.join('nurse' if word == 'nurde' else word for word in word.split())                    \n",
    "                word = ''.join('' if word == 'ado' else word for word in word.split()) # one row was ado, second tornao, third tornado               \n",
    "                word = ''.join('blouse' if word == 'ouse' else word for word in word.split())                \n",
    "                word = ''.join('pants' if word == 'pant' else word for word in word.split())                \n",
    "                word = ''.join('piranha' if word == 'parhna' else word for word in word.split())\n",
    "                #word = ''.join('pear' if word == 'peat' else word for word in word.split()) as peat has a different meaning, we didn't change it\n",
    "                word = ''.join('oregano' if word == 'pregeno' else word for word in word.split())                  \n",
    "                word = ''.join('dresser' if word == 'sser' else word for word in word.split()) # difficult, but all other recalled items from that participant \\\n",
    "                # had a couple of letters missing in the front (see ntain or tton)\n",
    "                word = ''.join('cotton' if word == 'tton' else word for word in word.split())                  \n",
    "                word = ''.join('basil' if word == 'brasil' else word for word in word.split())                  \n",
    "                word = ''.join('tangerine' if word == 'tangarine' else word for word in word.split())                \n",
    "                word = ''.join('cloth' if word == 'clothes' else word for word in word.split())                \n",
    "                word = ''.join('cloth' if word == 'clothing' else word for word in word.split())                \n",
    "                word = ''.join('cloud' if word == 'clouds' else word for word in word.split())                  \n",
    "                word = ''.join('book' if word == 'books' else word for word in word.split())                \n",
    "                word = ''.join('lily' if word == 'lilly' else word for word in word.split())\n",
    "                word = ''.join('oregano' if word == 'oragano' else word for word in word.split())                \n",
    "                word = ''.join('tornado' if word == 'tronado' else word for word in word.split())                \n",
    "                word = ''.join('apple' if word == 'apll' else word for word in word.split())                \n",
    "                word = ''.join('comic' if word == 'comics' else word for word in word.split())                \n",
    "                word = ''.join('daffodil' if word == 'fodil' else word for word in word.split())                \n",
    "                word = ''.join('halibut' if word == 'habut' else word for word in word.split())                  \n",
    "                word = ''.join('lightning' if word == 'lighnting' else word for word in word.split())                \n",
    "                word = ''.join('lightning' if word == 'lightining' else word for word in word.split())             \n",
    "                word = ''.join('lightning' if word == 'lightning' else word for word in word.split())      \n",
    "                word = ''.join('organ' if word == 'organs' else word for word in word.split()) # Organ has multiple meanings by itself, hence potentially ok to change\n",
    "                word = ''.join('pigeon' if word == 'piogen' else word for word in word.split())      \n",
    "                word = ''.join('paprika' if word == 'pipraki' else word for word in word.split())\n",
    "                word = ''.join('piranha' if word == 'purrana' else word for word in word.split())\n",
    "                word = ''.join('drought' if word == 'rought' else word for word in word.split())\n",
    "                word = ''.join('trumpet' if word == 'trumphent' else word for word in word.split())\n",
    "                word = ''.join('trumpet' if word == 'trumpht' else word for word in word.split())      \n",
    "                word = ''.join('trumpet' if word == 'trumpt' else word for word in word.split())\n",
    "                word = ''.join('bookcase' if word == 'bookself' else word for word in word.split())                                   \n",
    "                word = ''.join('bookcase' if word == 'bookshlelf' else word for word in word.split())                \n",
    "                word = ''.join('bookcase' if word == 'boookcase' else word for word in word.split())                \n",
    "                word = ''.join('chlorine' if word == 'chloride' else word for word in word.split())\n",
    "                word = ''.join('chlorine' if word == 'chorline' else word for word in word.split())  \n",
    "                word = ''.join('chlorine' if word == 'cholrine' else word for word in word.split())                  \n",
    "                word = ''.join('chlorine' if word == 'chroine' else word for word in word.split())                \n",
    "                word = ''.join('cinnamon' if word == 'cinamon' else word for word in word.split())\n",
    "                word = ''.join('cinnamon' if word == 'cinnimin' else word for word in word.split())                \n",
    "                word = ''.join('cinnamon' if word == 'cinnoman' else word for word in word.split())                \n",
    "                word = ''.join('cinnamon' if word == 'cinnomin' else word for word in word.split())                \n",
    "                word = ''.join('cotton' if word == 'coten' else word for word in word.split())                  \n",
    "                word = ''.join('halibut' if word == 'halibat' else word for word in word.split())                \n",
    "                word = ''.join('halibut' if word == 'halibiut' else word for word in word.split())\n",
    "                word = ''.join('halibut' if word == 'hallibut' else word for word in word.split())                \n",
    "                word = ''.join('halibut' if word == 'halubit' else word for word in word.split())                \n",
    "                word = ''.join('helium' if word == 'heliu,' else word for word in word.split())\n",
    "                word = ''.join('helium' if word == 'elium' else word for word in word.split())\n",
    "                word = ''.join('helium' if word == 'heliu' else word for word in word.split())   \n",
    "                word = ''.join('hour' if word == 'hours' else word for word in word.split())                  \n",
    "                word = ''.join('paprika' if word == 'prika' else word for word in word.split())                \n",
    "                word = ''.join('rose' if word == 'roses' else word for word in word.split())\n",
    "                word = ''.join('oregano' if word == 'aregeno' else word for word in word.split())                \n",
    "                word = ''.join('blizzard' if word == 'blizzzard' else word for word in word.split())                \n",
    "                word = ''.join('blouse' if word == 'blousse' else word for word in word.split())                \n",
    "                word = ''.join('daffodil' if word == 'dafadil' else word for word in word.split())                  \n",
    "                word = ''.join('daffodil' if word == 'daffadil' else word for word in word.split())                \n",
    "                word = ''.join('daffodil' if word == 'daffildil' else word for word in word.split())                \n",
    "                word = ''.join('daffodil' if word == 'daffodils' else word for word in word.split())                \n",
    "                word = ''.join('daffodil' if word == 'daffoldil' else word for word in word.split())                \n",
    "                word = ''.join('daffodil' if word == 'dafidil' else word for word in word.split())                \n",
    "                word = ''.join('daffodil' if word == 'dafidill' else word for word in word.split())                  \n",
    "                word = ''.join('daffodil' if word == 'dafidill' else word for word in word.split())                \n",
    "                word = ''.join('daffodil' if word == 'daphadil' else word for word in word.split())\n",
    "                word = ''.join('daffodil' if word == 'daphadile' else word for word in word.split())                \n",
    "                word = ''.join('daffodil' if word == 'daphadile' else word for word in word.split())                \n",
    "                word = ''.join('daffodil' if word == 'daphadill' else word for word in word.split())                \n",
    "                word = ''.join('daffodil' if word == 'daphidile' else word for word in word.split())                  \n",
    "                word = ''.join('daffodil' if word == 'daphodil' else word for word in word.split())                \n",
    "                word = ''.join('daffodil' if word == 'daphodil' else word for word in word.split())                \n",
    "                word = ''.join('daffodil' if word == 'dapphodil' else word for word in word.split())                \n",
    "                word = ''.join('dentist' if word == 'dentis' else word for word in word.split())                \n",
    "                word = ''.join('dresser' if word == 'dreser' else word for word in word.split())                \n",
    "                word = ''.join('dress' if word == 'dresses' else word for word in word.split()) #very difficult, either dress or dresser, therefore we left it as is                \n",
    "                word = ''.join('drought' if word == 'drout' else word for word in word.split())                \n",
    "                word = ''.join('drum' if word == 'dum' else word for word in word.split())                   \n",
    "                word = ''.join('engineer' if word == 'engeneer' else word for word in word.split())                \n",
    "                word = ''.join('engineer' if word == 'enigineer' else word for word in word.split())                \n",
    "                word = ''.join('helium' if word == 'helim' else word for word in word.split())                \n",
    "                word = ''.join('linen' if word == 'linene' else word for word in word.split())\n",
    "                word = ''.join('lamp' if word == 'lamo' else word for word in word.split()) \n",
    "                word = ''.join('mountain' if word == 'mountian' else word for word in word.split())                \n",
    "                word = ''.join('nectarine' if word == 'necatrina' else word for word in word.split())                   \n",
    "                word = ''.join('nectarine' if word == 'necatrine' else word for word in word.split())                \n",
    "                word = ''.join('nectarine' if word == 'nectarine' else word for word in word.split())                \n",
    "                word = ''.join('nectarine' if word == 'nectatine' else word for word in word.split())                \n",
    "                word = ''.join('nectarine' if word == 'nectorin' else word for word in word.split())                  \n",
    "                word = ''.join('nectarine' if word == 'nectorine' else word for word in word.split())                \n",
    "                word = ''.join('nectarine' if word == 'nectrine' else word for word in word.split())                   \n",
    "                word = ''.join('nectarine' if word == 'necturine' else word for word in word.split())                \n",
    "                word = ''.join('nectarine' if word == 'nictarine' else word for word in word.split())                                             \n",
    "                word = ''.join('nitrogen' if word == 'nigrogen' else word for word in word.split())                  \n",
    "                word = ''.join('nitrogen' if word == 'nitrogren' else word for word in word.split())                \n",
    "                word = ''.join('oxygen' if word == 'ocygen' else word for word in word.split())                \n",
    "                word = ''.join('orchid' if word == 'orchird' else word for word in word.split())                \n",
    "                word = ''.join('oregano' if word == 'oreganno' else word for word in word.split())                \n",
    "                word = ''.join('oregano' if word == 'oregeno' else word for word in word.split())                \n",
    "                word = ''.join('oregano' if word == 'oregno' else word for word in word.split())                  \n",
    "                word = ''.join('oregano' if word == 'oregno' else word for word in word.split())                \n",
    "                word = ''.join('oregano' if word == 'oregono' else word for word in word.split())\n",
    "                word = ''.join('oxygen' if word == 'oxogen' else word for word in word.split())                \n",
    "                word = ''.join('paprika' if word == 'papricka' else word for word in word.split())                \n",
    "                word = ''.join('parrot' if word == 'parot' else word for word in word.split())                \n",
    "                word = ''.join('paprika' if word == 'peperika' else word for word in word.split())                  \n",
    "                word = ''.join('pamphlet' if word == 'phamplet' else word for word in word.split())                \n",
    "                word = ''.join('piranha' if word == 'phirana' else word for word in word.split())                \n",
    "                word = ''.join('piranha' if word == 'pirana' else word for word in word.split())                                \n",
    "                word = ''.join('piranha' if word == 'piranah' else word for word in word.split())                  \n",
    "                word = ''.join('piranha' if word == 'piranna' else word for word in word.split())                \n",
    "                word = ''.join('piranha' if word == 'pirhanna' else word for word in word.split())                \n",
    "                word = ''.join('piranha' if word == 'pirnaha' else word for word in word.split())                \n",
    "                word = ''.join('piranha' if word == 'pirrhana' else word for word in word.split())                \n",
    "                word = ''.join('plateau' if word == 'plaeatu' else word for word in word.split())                \n",
    "                word = ''.join('plateau' if word == 'plataue' else word for word in word.split())                  \n",
    "                word = ''.join('plateau' if word == 'plateua' else word for word in word.split())                \n",
    "                word = ''.join('plateau' if word == 'plateux' else word for word in word.split())\n",
    "                word = ''.join('plateau' if word == 'plaute' else word for word in word.split())                \n",
    "                word = ''.join('plateau' if word == 'pleatu' else word for word in word.split())                \n",
    "                word = ''.join('plateau' if word == 'pleteau' else word for word in word.split())                \n",
    "                word = ''.join('raspberry' if word == 'rasperry' else word for word in word.split())                  \n",
    "                word = ''.join('trumpet' if word == 'rumpet' else word for word in word.split())                \n",
    "                word = ''.join('secretary' if word == 'sactatary' else word for word in word.split())                \n",
    "                word = ''.join('saxophone' if word == 'saxphone' else word for word in word.split())\n",
    "                word = ''.join('tornado' if word == 'torando' else word for word in word.split())                  \n",
    "                word = ''.join('tornado' if word == 'tornadeo' else word for word in word.split())                \n",
    "                word = ''.join('tornado' if word == 'tornadoe' else word for word in word.split())                \n",
    "                word = ''.join('tornado' if word == 'tornadoe' else word for word in word.split())                \n",
    "                word = ''.join('tornado' if word == 'tornador' else word for word in word.split())                \n",
    "                word = ''.join('tornado' if word == 'tornando' else word for word in word.split())                \n",
    "                word = ''.join('tornado' if word == 'tornao' else word for word in word.split())                  \n",
    "                word = ''.join('tree' if word == 'trees' else word for word in word.split())                \n",
    "                word = ''.join('trumpet' if word == 'trmpet' else word for word in word.split())\n",
    "                word = ''.join('trumpet' if word == 'trmpet' else word for word in word.split())                \n",
    "                word = ''.join('trumpet' if word == 'trupmet' else word for word in word.split())                \n",
    "                word = ''.join('underwear' if word == 'undewear' else word for word in word.split())                \n",
    "                word = ''.join('velvet' if word == 'velvot' else word for word in word.split())                  \n",
    "                word = ''.join('velvet' if word == 'vevelt' else word for word in word.split())                \n",
    "                word = ''.join('velvet' if word == 'volvet' else word for word in word.split())                \n",
    "                word = ''.join('saxophone' if word == 'xophone' else word for word in word.split())                \n",
    "                word = ''.join('oxygen' if word == 'xygen' else word for word in word.split())                  \n",
    "                word = ''.join('daffodil' if word == 'dafodil' else word for word in word.split())\n",
    "                word = ''.join('halibut' if word == 'halibit' else word for word in word.split())\n",
    "                word = ''.join('raspberry, grape' if word == 'raspberrygrape' else word for word in word.split())\n",
    "                word = ''.join('desk, chair, lamp, hour' if word == 'deskchairlamphour' else word for word in word.split())\n",
    "                word = ''.join('tornado, hail' if word == 'tornadohail' else word for word in word.split())                \n",
    "\n",
    "                #Buffer\n",
    "                word = ''.join('arms' if word == 'arm' else word for word in word.split())\n",
    "                word = ''.join('green' if word == 'grreen' else word for word in word.split())\n",
    "            \n",
    "            # Biase comes from file name\n",
    "            if os.path.exists('new_encoding_' + str(subjectNumber) +'_0.pkl'):\n",
    "                biased = 0\n",
    "            else:\n",
    "                biased = 1\n",
    "\n",
    "            # Order comes from SN and impacts correct study list reference\n",
    "            if (subjectNumber%2 == 0) and (biased == 1):\n",
    "                order = 1\n",
    "                if word in studyList2:\n",
    "                    correct = 1\n",
    "                else:\n",
    "                    correct = 0\n",
    "            elif (subjectNumber%2 == 1) and (biased == 1):\n",
    "                order = 0     \n",
    "                if word in studyList1:\n",
    "                    correct = 1\n",
    "                else:\n",
    "                    correct = 0\n",
    "            if (subjectNumber%2 == 0) and (biased == 0):\n",
    "                order = 0\n",
    "                if word in studyList1:\n",
    "                    correct = 1\n",
    "                else:\n",
    "                    correct = 0\n",
    "            elif (subjectNumber%2 == 1) and (biased == 0):\n",
    "                order = 1     \n",
    "                if word in studyList2:\n",
    "                    correct = 1\n",
    "                else:\n",
    "                    correct = 0\n",
    "\n",
    "            # Indicate buffer\n",
    "            buffer = 0\n",
    "            for x in word:\n",
    "                if word in ['arms', 'green', 'uncle', 'hours']:\n",
    "                    buffer = 1\n",
    "                else:\n",
    "                    buffer = 0\n",
    "\n",
    "            # Collaboration comes from retrieval_1_1 files\n",
    "            if os.path.exists('new_retrieve_' + str(subjectNumber) +'_0_1.pkl'):\n",
    "                collaboration = 0\n",
    "            else:\n",
    "                collaboration = 1\n",
    "\n",
    "            #Problem: This is only the words from the 2. recall. And the column with who they collaborated\n",
    "            collaborator = 0\n",
    "            if os.path.exists('new_retrieve_' + str(subjectNumber) + str(subjectNumber+1) + '_1_1.pkl'):\n",
    "                collaborator = str(subjectNumber+1)\n",
    "            elif os.path.exists('new_retrieve_' + str(subjectNumber-1) + str(subjectNumber) + '_1_1.pkl'):\n",
    "                collaborator = str(subjectNumber-1)\n",
    "            else:\n",
    "                collaborator = 0\n",
    "            \n",
    "            trialDict = {'SN':subjectNumber, 'biased':biased, 'order':order, 'phase':phase, 'collaboration':collaboration, 'collaborator':collaborator, \n",
    "                         'word':word, 'correct':correct, 'buffer':buffer}\n",
    "            df = df.append(trialDict, ignore_index=True)\n",
    "\n",
    "df.to_csv('2022-07-28_Similarity_Exp2_AllWords_Clean1_test.csv', index=False)\n",
    "df = pd.read_csv('2022-07-28_Similarity_Exp2_AllWords_Clean1_test.csv')\n",
    "           \n",
    "# Splitting & classification of words that were written in one line\n",
    "df = (df.set_index(df.columns.drop('word',1).tolist()).word.str.split(',', expand=True).stack().reset_index().rename(columns={0:'word'}).loc[:, df.columns])\n",
    "df['correct'][df.word == 'grape'] = \"1\"\n",
    "df['correct'][df.word == 'raspberry'] = \"1\"\n",
    "df['correct'][df.word == 'chair'] = \"1\"\n",
    "df['correct'][df.word == 'tornado'] = \"1\"\n",
    "df['correct'][df.word == 'hail'] = \"1\"\n",
    "df['correct'][df.word == 'desk'] = \"1\"\n",
    "df['correct'][df.word == 'lamp'] = \"1\"\n",
    "df['correct'][df.word == 'hour'] = \"1\"\n",
    "df['buffer'][df.word == 'hour'] = \"1\"\n",
    "\n",
    "df = df.drop_duplicates(['SN','word']) # Delete duplicates\n",
    "df = df.dropna(axis=0, how='any', subset=['word'])\n",
    "\n",
    "df['number'] = df['word']\n",
    "# Includes misspellings and buffer words, same as in experiment 1 plus new intrusions and buffer\n",
    "\n",
    "translator = {'crow':1,'eagle':2,'finch':3,'parrot':4,'pigeon':5,'cardinal':6,'nitrogen':7,'helium':8,'chlorine':9,'calcium':10,'oxygen':11,\n",
    "          'mercury':12,'trout':13,'flounder':14,'halibut':15,'guppy':16,'piranha':17,'shark':18,'carnation':19,'orchid':20,'pansy':21,'daffodil':22,'violet':23,'rose':24,\n",
    "          'nectarine':25,'pear':26,'apple':27,'grape':28,'raspberry':29,'cherry':30,'tuba':31,'drum':32,'trumpet':33,'saxophone':34,'piano':35,'organ':36,'tree':37,'ocean':38,\n",
    "          'canyon':39,'mountain':40,'plateau':41,'cave':42,'cinnamon':43,'mustard':44,'basil':45,'oregano':46,'paprika':47,'salt':48,'cotton':49,'wool':50,'velvet':51,\n",
    "          'linen':52,'leather':53,'denim':54,'flyer':55,'newspaper':56,'comic':57,'essay':58,'pamphlet':59,'book':60,'tornado':61,'hail':62,'blizzard':63,'rain':64,'drought':65,\n",
    "          'lightning':66,'jacket':67,'dress':68,'blouse':69,'underwear':70,'shoes':71,'shirt':72,'lamp':73,'desk':74,'bookcase':75,'dresser':76,'chair':77,'recliner':78,\n",
    "          'banker':79,'dentist':80,'secretary':81,'engineer':82,'nurse':83,'doctor':84,'a':85,'architect':86,'avidafil?':87,'ballet':88,'baseball':89,'bed':90,\n",
    "          'bird':91,'blueberry':92,'cage':93,'canary':94,'carbon':95,'chemical':96,'clarinet':97,'coat':98,'concrete':99,'daisy':100,'dog':101,'experimenter':102,\n",
    "          'fish':103,'flamenco':104,'flower':105,'flute':106,'fruit':107,'give':108,'grass':109,'hydrogen':110,'instrument':111,'lawyer':112,'library':113,'lily':114,\n",
    "          'lithium':115,'lung':116,'melon':117,'music':118,'nickel':119,'nylon':120,'pants':121,'parsley':122,'pepper':123,'project':124,'red':125,'salmon':126,\n",
    "          'satin':127,'sea':128,'sky':129,'slave':130,'sofa':131,'storm':132,'table':133,'tango':134,'teacher':135,'the':136,'thunder':137,'thunderstorm':138,'tissue':139,\n",
    "          'to':140,'trombone':141,'trousers':142,'tulip':143,'tuna':144,'willing':145,'address':146,'cavern':147,'homework':148,'magazine':149,'are':150,\n",
    "          'dove':151,'sodium':152,'hurricaine':153,'hurricane':154,'violent':155,'violin':156,'green':157,'uncle':158,'hour':159,'arms':160,'orchard':161,\n",
    "          'paper':162,'sunflower':163,'dandelion':164,'snow':165,'green':166,'professor':167,'socks':168,'purse':169,'ar':170,'e':171,'mium':172,\n",
    "          't':173,'rk':174, 'hid':175,'breakfast':176,'uncle':177,'hat':178,'hale':179,'speacker':180,'dance':181,'baker':182,'couch':183,'tube':184,\n",
    "          'valley':185,'tangerine':186,'guitar':187,'orange':188,'skirt':189,'robin':190,'peach':191,'lavender':192,'thyme':193,'drawer':194,\n",
    "          'cloud':195,'word':196,'paper':197,'carnelian':198,'article':199,'daydream':200,'tectonicplate':201,'blossom':202,'cloth':203,'tsunami':204,\n",
    "          'legs':205,'keyboard':206,'fiction':207,'water':208,'earth':209,'light':210,'hair':211,'rocks':212,'moon':213,'weather':214,'wind':215,\n",
    "          'door':216,'letter':217,'time':218,'yellow':219,'river':220,'lake':221,'typhoon':222,'gloves':223,'magnesium':224,'ear':225,'camp':226,\n",
    "          'seed':227,'phone':228,'fresh':229,'lenin':230,'in':231,'halogen':232,'wager':233,'research':234,'positive':235,'success':236,'pelican':237,\n",
    "          'computer':238,'plum':239,'although':240,'brazil':241,'fly':242,'brochure':243,'flood':244,'perch':245,'year':246,'lilac':247,'silk':248,\n",
    "          'disaster':249,'bloom':250,'politician':251,'onion':252,'jazz':253,'peat':254,'flinch':255,'weatherphenomenon':256,'natural':257,\n",
    "          'flavoring':258,'food':259,'furniture':260,'formation':261,'papya':262,'snowstorm':263,'basketball':264, 'carnage':454, 'birch':455, 'pine':456}\n",
    "df.number = [translator[item] for item in df.number] \n",
    "\n",
    "df.to_csv('2022-07-28_Similarity_Exp2_AllWords_Clean1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SN  biased  order  collaboration collaborator      word  correct  buffer  \\\n",
      "0    25       0      1              0            0  chlorine        1       0   \n",
      "1    25       0      1              0            0     grape        1       0   \n",
      "2    25       0      1              0            0      pear        1       0   \n",
      "3    25       0      1              0            0      fish        0       0   \n",
      "4    25       0      1              0            0   piranha        1       0   \n",
      "..  ...     ...    ...            ...          ...       ...      ...     ...   \n",
      "91  147       1      0              1          146   plateau        1       0   \n",
      "92  147       1      0              1          146  mountain        1       0   \n",
      "93  147       1      0              1          146    cherry        1       0   \n",
      "94  147       1      0              1          146      wool        1       0   \n",
      "95  147       1      0              1          146      arms        1       1   \n",
      "\n",
      "    phase  number  \n",
      "0       2       9  \n",
      "1       2      28  \n",
      "2       2      26  \n",
      "3       2     103  \n",
      "4       2      17  \n",
      "..    ...     ...  \n",
      "91      2      41  \n",
      "92      2      40  \n",
      "93      2      30  \n",
      "94      2      50  \n",
      "95      2     160  \n",
      "\n",
      "[92 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------Input----------------------------------------------\n",
    "studyList1 = ['crow','eagle','finch','parrot','pigeon','cardinal','nitrogen','helium','chlorine','calcium','oxygen','mercury','trout',\\\n",
    "              'flounder','halibut','guppy','piranha','shark','carnation','orchid','pansy','daffodil','violet','rose','nectarine','pear',\\\n",
    "              'apple','grape','raspberry','cherry','tuba','drum','trumpet','saxophone','piano','organ','tree','ocean','canyon','mountain',\\\n",
    "              'plateau','cave','cinnamon','mustard','basil','oregano','paprika','cotton','wool', 'velvet','linen','leather','flyer',\\\n",
    "              'newspaper','comic','essay','pamphlet','tornado','hail','blizzard','rain','drought','jacket','dress','blouse','underwear',\\\n",
    "              'shoes','lamp','desk','bookcase','dresser','chair','banker','dentist','secretary','engineer','nurse','hour', 'arms', 'green',\\\n",
    "              'uncle']\n",
    "    \n",
    "studyList2 = ['crow','eagle','finch','parrot','pigeon','nitrogen','helium','chlorine','calcium','oxygen','trout','flounder','halibut',\\\n",
    "              'guppy','piranha','carnation','orchid','pansy','daffodil','violet','nectarine','pear','apple','grape','raspberry','tuba',\\\n",
    "              'drum','trumpet','saxophone','piano','tree','ocean','canyon','mountain','plateau','cinnamon','mustard','basil','oregano',\\\n",
    "              'paprika','salt','cotton','wool','velvet','linen','leather','denim','flyer','newspaper','comic','essay','pamphlet','book',\\\n",
    "              'tornado','hail','blizzard','rain','drought','lightning','jacket','dress','blouse','underwear','shoes','shirt','lamp','desk',\\\n",
    "              'bookcase','dresser','chair','recliner','banker','dentist','secretary','engineer','nurse','doctor','hour', 'arms', 'green', 'uncle']    \n",
    "    \n",
    "#-------------------------------------Create Data Frame----------------------------------------------\n",
    "\n",
    "df_dlm = pd.DataFrame(columns=['SN', 'biased','order', 'collaboration', 'collaborator', 'word', 'correct', 'buffer', 'phase'])\n",
    "\n",
    "dlmFiles = [25, 126, 147]\n",
    "\n",
    "# A: Any subjects we should exclude?\n",
    "for subjectNumber in dlmFiles:\n",
    "       \n",
    "    # A: Import all words from the 3 participants for recall phase 2\n",
    "    try:\n",
    "        exp_data = pd.read_table('retrieve_'+str(subjectNumber)+'_0_2.dlm', sep='\\t', names=[\"time-del\", \"time\", \"word-del\", \"word\"])\n",
    "        exp_data = exp_data.drop(['time-del', 'time', 'word-del'], axis=1)\n",
    "        exp_data = exp_data.drop([0])\n",
    "        words = exp_data['word'].to_list()\n",
    "        #type(word)\n",
    "        \n",
    "        #word2 = \" \".join([str(i) for i in word])\n",
    "\n",
    "        #print(word2)\n",
    "\n",
    "        for i in words:\n",
    "            word = str(i)\n",
    "            biased = 0\n",
    "            order = 0\n",
    "            collaboration = 0\n",
    "            phase = 2\n",
    "            correct = 0\n",
    "            buffer = 0\n",
    "            \n",
    "            # From experiment 1\n",
    "            word = word.replace(' ', '')\n",
    "            word = word.replace('\\\\', '')\n",
    "            word = word.lower()\n",
    "            word = re.sub(r'\\bcanyoan\\b', 'canyon', word)\n",
    "            word = re.sub(r'\\bcanyons\\b', 'canyon', word)\n",
    "            word = re.sub(r'\\bcherries\\b', 'cherry', word)\n",
    "            word = re.sub(r'\\bclorine\\b', 'chlorine', word)\n",
    "            word = re.sub(r'\\bcinammon\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinnaman\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinnimon\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinomman\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcotten\\b', 'cotton', word)\n",
    "            word = re.sub(r'\\bdaffidil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdafodill\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphodile\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdoffodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdenin\\b', 'denim', word)\n",
    "            word = re.sub(r'\\bdrums\\b', 'drum', word)\n",
    "            word = re.sub(r'\\benginner\\b', 'engineer', word)\n",
    "            word = re.sub(r'\\bflunder\\b', 'flounder', word)\n",
    "            word = re.sub(r'\\bflier\\b', 'flyer', word)\n",
    "            word = re.sub(r'\\bflyers\\b', 'flyer', word)\n",
    "            word = re.sub(r'\\bgrapes\\b', 'grape', word)\n",
    "            word = re.sub(r'\\bgupppy\\b', 'guppy', word)\n",
    "            word = re.sub(r'\\bhailbut\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhalibet\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\blinens\\b', 'linen', word)\n",
    "            word = re.sub(r'\\blinnen\\b', 'linen', word)\n",
    "            word = re.sub(r'\\bercury\\b', 'mercury', word)\n",
    "            word = re.sub(r'\\bmountains\\b', 'mountain', word)\n",
    "            word = re.sub(r'\\bmoutain\\b', 'mountain', word)\n",
    "            word = re.sub(r'\\bnecratine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnecterine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\borchad\\b', 'orchid', word)\n",
    "            word = re.sub(r'\\baregano\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\borageno\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\bregano\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\bamphlet\\b', 'pamphlet', word)\n",
    "            word = re.sub(r'\\bpamplet\\b', 'pamphlet', word)\n",
    "            word = re.sub(r'\\bpanplet\\b', 'pamphlet', word)\n",
    "            word = re.sub(r'\\bphamlet\\b', 'pamphlet', word)\n",
    "            word = re.sub(r'\\bansy\\b', 'pansy', word)\n",
    "            word = re.sub(r'\\bpaparika\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bpapprika\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bpaprica\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bpeprica\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bpegion\\b', 'pigeon', word)\n",
    "            word = re.sub(r'\\bpidgeon\\b', 'pigeon', word)\n",
    "            word = re.sub(r'\\bpidgeons\\b', 'pigeon', word)\n",
    "            word = re.sub(r'\\bpidgieon\\b', 'pigeon', word)\n",
    "            word = re.sub(r'\\bparanha\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirahana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirahna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirahnna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirannah\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirannha\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirhana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirranha\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bplateu\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bplatue\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\brasberries\\b', 'raspberry', word)\n",
    "            word = re.sub(r'\\brasberry\\b', 'raspberry', word)\n",
    "            word = re.sub(r'\\braspberries\\b', 'raspberry', word)\n",
    "            word = re.sub(r'\\brasphberry\\b', 'raspberry', word)\n",
    "            word = re.sub(r'\\bsaxaphone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsaxephone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsaxiphone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsecratary\\b', 'secretary', word)\n",
    "            word = re.sub(r'\\bsecrertary\\b', 'secretary', word)\n",
    "            word = re.sub(r'\\bshoe\\b', 'shoes', word)\n",
    "            word = re.sub(r'\\btrumphet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\btumpet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\bnderwear\\b', 'underwear', word)\n",
    "            word = re.sub(r'\\budnerwear\\b', 'underwear', word)\n",
    "            word = re.sub(r'\\bbookshelf\\b', 'bookcase', word)\n",
    "            word = re.sub(r'\\bchari\\b', 'chair', word)\n",
    "            word = re.sub(r'\\blightening\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blightenings\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blighting\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blightning\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blightnening\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\btress\\b', 'tree', word)\n",
    "            word = re.sub(r'\\bvoilet\\b', 'violet', word)\n",
    "            word = re.sub(r'\\bblizarrd\\b', 'blizzard', word)\n",
    "            word = re.sub(r'\\bvelvey\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bdafodil\\b', 'daffodil', word)            \n",
    "            word = re.sub(r'\\bhalibit\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bbooks\\b', 'book', word)\n",
    "            word = re.sub(r'\\bhurricaine\\b', 'hurricane', word)          \n",
    "            \n",
    "            #From experiment 2\n",
    "            #word = re.sub(r'\\bado\\b', '', word)\n",
    "            word = re.sub(r'\\bdandalione\\b', 'dandelion', word)\n",
    "            word = re.sub(r'\\bdandeline\\b', 'dandelion', word)\n",
    "            word = re.sub(r'\\bdandelion\\b', 'dandelion', word)\n",
    "            word = re.sub(r'\\bdandelion\\b', 'dandelion', word)\n",
    "            word = re.sub(r'\\bdandalione\\b', 'dandelion', word)\n",
    "            word = re.sub(r'\\bdandilion\\b', 'dandelion', word)\n",
    "            #word = re.sub(r'\\bflinch\\b', 'finch', word)\n",
    "            word = re.sub(r'\\bgano\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\bhalbait\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhale\\b', 'hail', word)\n",
    "            word = re.sub(r'\\blenin\\b', 'linen', word)\n",
    "            word = re.sub(r'\\blvet\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bntain\\b', 'mountain', word)\n",
    "            word = re.sub(r'\\bnurde\\b', 'nurse', word)\n",
    "            word = re.sub(r'\\bado\\b', '', word)\n",
    "            word = re.sub(r'\\bouse\\b', 'blouse', word)\n",
    "            word = re.sub(r'\\bpant\\b', 'pants', word)\n",
    "            word = re.sub(r'\\bparhna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpeat\\b', 'pear', word)\n",
    "            word = re.sub(r'\\bpregeno\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\bsser\\b', 'dresser', word)\n",
    "            word = re.sub(r'\\btton\\b', 'cotton', word)\n",
    "            word = re.sub(r'\\bbrasil\\b', 'basil', word)\n",
    "            word = re.sub(r'\\btangarine\\b', 'tangerine', word)\n",
    "            word = re.sub(r'\\bclothes\\b', 'cloth', word)\n",
    "            word = re.sub(r'\\bclothing\\b', 'cloth', word)\n",
    "            word = re.sub(r'\\bclouds\\b', 'cloud', word)\n",
    "            word = re.sub(r'\\bbooks\\b', 'book', word)\n",
    "            word = re.sub(r'\\blilly\\b', 'lily', word)\n",
    "            word = re.sub(r'\\boragano\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\btronado\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\bapll\\b', 'apple', word)\n",
    "            word = re.sub(r'\\bcomics\\b', 'comic', word)\n",
    "            word = re.sub(r'\\bfodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bhabut\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\blighnting\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blightining\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blightning\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\borgans\\b', 'organ', word)\n",
    "            word = re.sub(r'\\bpiogen\\b', 'pigeon', word)\n",
    "            word = re.sub(r'\\bpipraki\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bpurrana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\brought\\b', 'drought', word)\n",
    "            word = re.sub(r'\\btrumphent\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\btrumpht\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\btrumpt\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\bbookself\\b', 'bookcase', word)\n",
    "            word = re.sub(r'\\bbookshlelf\\b', 'bookcase', word)\n",
    "            word = re.sub(r'\\bboookcase\\b', 'bookcase', word)\n",
    "            word = re.sub(r'\\bchloride\\b', 'chlorine', word)\n",
    "            word = re.sub(r'\\bchorline\\b', 'chlorine', word)\n",
    "            word = re.sub(r'\\bcholrine\\b', 'chlorine', word)\n",
    "            word = re.sub(r'\\bchroine\\b', 'chlorine', word)\n",
    "            word = re.sub(r'\\bcinamon\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinnimin\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinnoman\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinnomin\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcoten\\b', 'cotton', word)\n",
    "            word = re.sub(r'\\bhalibat\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhalibiut\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhallibut\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhalubit\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bheliu,\\b', 'helium', word)\n",
    "            word = re.sub(r'\\belium\\b', 'helium', word)\n",
    "            word = re.sub(r'\\bheliu\\b', 'helium', word)\n",
    "            word = re.sub(r'\\bhours\\b', 'hour', word)\n",
    "            word = re.sub(r'\\bprika\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\broses\\b', 'rose', word)\n",
    "            word = re.sub(r'\\baregeno\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\bblizzzard\\b', 'blizzard', word)\n",
    "            word = re.sub(r'\\bblousse\\b', 'blouse', word)\n",
    "            word = re.sub(r'\\bdafadil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaffadil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaffildil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaffodils\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaffoldil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdafidil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdafidill\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdafidill\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphadil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphadile\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphadile\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphadill\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphidile\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdapphodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdentis\\b', 'dentist', word)\n",
    "            word = re.sub(r'\\bdreser\\b', 'dresser', word)\n",
    "            word = re.sub(r'\\bdresses\\b', 'dress', word)\n",
    "            word = re.sub(r'\\bdrout\\b', 'drought', word)\n",
    "            word = re.sub(r'\\bdum\\b', 'drum', word)\n",
    "            word = re.sub(r'\\bengeneer\\b', 'engineer', word)\n",
    "            word = re.sub(r'\\benigineer\\b', 'engineer', word)\n",
    "            word = re.sub(r'\\bhelim\\b', 'helium', word)\n",
    "            word = re.sub(r'\\blinene\\b', 'linen', word)\n",
    "            word = re.sub(r'\\blamo\\b', 'lamp', word)\n",
    "            word = re.sub(r'\\bmountian\\b', 'mountain', word)\n",
    "            word = re.sub(r'\\bnecatrina\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnecatrine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnectarine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnectatine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnectorin\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnectorine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnectrine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnecturine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnictarine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnigrogen\\b', 'nitrogen', word)\n",
    "            word = re.sub(r'\\bnitrogren\\b', 'nitrogen', word)\n",
    "            word = re.sub(r'\\bocygen\\b', 'oxygen', word)\n",
    "            word = re.sub(r'\\borchird\\b', 'orchid', word)\n",
    "            word = re.sub(r'\\boreganno\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\boregeno\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\boregno\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\boregno\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\boregono\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\boxogen\\b', 'oxygen', word)\n",
    "            word = re.sub(r'\\bpapricka\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bparot\\b', 'parrot', word)\n",
    "            word = re.sub(r'\\bpeperika\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bphamplet\\b', 'pamphlet', word)\n",
    "            word = re.sub(r'\\bphirana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpiranah\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpiranna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirhanna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirnaha\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirrhana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bplaeatu\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bplataue\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bplateua\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bplateux\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bplaute\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bpleatu\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bpleteau\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\brasperry\\b', 'raspberry', word)\n",
    "            word = re.sub(r'\\brumpet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\bsactatary\\b', 'secretary', word)\n",
    "            word = re.sub(r'\\bsaxphone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\btorando\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\btornadeo\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\btornadoe\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\btornadoe\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\btornador\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\btornando\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\btornao\\b', 'tornado', word)\n",
    "            word = re.sub(r'\\btrees\\b', 'tree', word)\n",
    "            word = re.sub(r'\\btrmpet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\btrmpet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\btrupmet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\bundewear\\b', 'underwear', word)\n",
    "            word = re.sub(r'\\bvelvot\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bvevelt\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bvolvet\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bxophone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bxygen\\b', 'oxygen', word)\n",
    "            word = re.sub(r'\\bdafodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bhalibit\\b', 'halibut', word)\n",
    "\n",
    "            # From experiment 3\n",
    "            word = re.sub(r'\\beather\\b', 'leather', word)\n",
    "            word = re.sub(r'\\bappl\\b', 'apple', word)\n",
    "            word = re.sub(r'\\barangano\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\barm\\b', 'arms', word)\n",
    "            word = re.sub(r'\\barregano\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\baxophpne\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bbizzard\\b', 'blizzard', word)\n",
    "            word = re.sub(r'\\bblizard\\b', 'blizzard', word)\n",
    "            #word = re.sub(r'\\bBlouse\\b', 'blouse', word)\n",
    "            word = re.sub(r'\\bblouser\\b', 'blouse', word)\n",
    "            word = re.sub(r'\\bbookshelf\\b', 'bookcase', word)\n",
    "            word = re.sub(r'\\bboooks\\b', 'book', word)\n",
    "            #word = re.sub(r'\\bCardinal\\b', 'cardinal', word)\n",
    "            word = re.sub(r'\\bcartigan\\b', 'cardigan', word)\n",
    "            word = re.sub(r'\\bchloride\\b', 'chlorine', word)\n",
    "            #word = re.sub(r'\\bComic\\b', 'comic', word)\n",
    "            word = re.sub(r'\\bcutton\\b', 'cotton', word)\n",
    "            word = re.sub(r'\\bdaffadil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaffildol\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaffodile\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaffodils\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdafidil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdafiodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdandelion\\b', 'dandelion', word)\n",
    "            word = re.sub(r'\\bdanelion\\b', 'dandelion', word)\n",
    "            word = re.sub(r'\\bdaphodil\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdaphodyl\\b', 'daffodil', word)\n",
    "            word = re.sub(r'\\bdoctore\\b', 'doctor', word)\n",
    "            #word = re.sub(r'\\bDress\\b', 'dress', word)\n",
    "            word = re.sub(r'\\bdresses\\b', 'dress', word)\n",
    "            word = re.sub(r'\\bdressor\\b', 'dresser', word)\n",
    "            word = re.sub(r'\\bdrough\\b', 'drought', word)\n",
    "            word = re.sub(r'\\bdrums\\b', 'drum', word)\n",
    "            #word = re.sub(r'\\bEagle\\b', 'eagle', word)\n",
    "            word = re.sub(r'\\begale\\b', 'eagle', word)\n",
    "            word = re.sub(r'\\beingineer\\b', 'engineer', word)\n",
    "            word = re.sub(r'\\bfinc\\b', 'finch', word)\n",
    "            word = re.sub(r'\\bFinch\\b', 'finch', word)\n",
    "            word = re.sub(r'\\bgraoe\\b', 'grape', word)\n",
    "            word = re.sub(r'\\bgrap\\b', 'grape', word)\n",
    "            #word = re.sub(r'\\bGrape\\b', 'grape', word)\n",
    "            word = re.sub(r'\\bgrapes\\b', 'grape', word)\n",
    "            #word = re.sub(r'\\bGreen\\b', 'green', word)\n",
    "            word = re.sub(r'\\bgruppy\\b', 'guppy', word)\n",
    "            word = re.sub(r'\\bgubby\\b', 'guppy', word)\n",
    "            #word = re.sub(r'\\bGuppy\\b', 'guppy', word)\n",
    "            word = re.sub(r'\\bhailbut\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhalbit\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhalibat\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bhelium\\'\\b', 'helium', word)\n",
    "            word = re.sub(r'\\bheluim\\b', 'helium', word)\n",
    "            word = re.sub(r'\\bhitrogen\\b', 'nitrogen', word)\n",
    "            #word = re.sub(r'\\bHour\\b', 'hour', word)\n",
    "            word = re.sub(r'\\binen\\b', 'linen', word)\n",
    "            word = re.sub(r'\\bjean\\b', 'jeans', word)\n",
    "            word = re.sub(r'\\bJesus\\b', 'jesus', word)\n",
    "            word = re.sub(r'\\bjupitor\\b', 'jupiter', word)\n",
    "            word = re.sub(r'\\blether\\b', 'leather', word)\n",
    "            word = re.sub(r'\\blightining\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blightning\\b', 'lightning', word)\n",
    "            word = re.sub(r'\\blinnen\\b', 'linen', word)\n",
    "            word = re.sub(r'\\bmountains\\b', 'mountain', word)\n",
    "            word = re.sub(r'\\bmpet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\bnectorine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnectraine\\b', 'nectarine', word)\n",
    "            word = re.sub(r'\\bnitrgoen\\b', 'nitrogen', word)\n",
    "            #word = re.sub(r'\\bNitrogen\\b', 'nitrogen', word)\n",
    "            word = re.sub(r'\\bnitrogen\\'\\b', 'nitrogen', word)\n",
    "            word = re.sub(r'\\bOcean\\b', 'ocean', word)\n",
    "            word = re.sub(r'\\bochid\\b', 'orchid', word)\n",
    "            word = re.sub(r'\\boctupus\\b', 'octopus', word)\n",
    "            word = re.sub(r'\\borchids\\b', 'orchid', word)\n",
    "            word = re.sub(r'\\borchird\\b', 'orchid', word)\n",
    "            word = re.sub(r'\\borcid\\b', 'orchid', word)\n",
    "            #word = re.sub(r'\\boregon\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\borgano\\b', 'oregano', word)\n",
    "            word = re.sub(r'\\bOxygen\\b', 'oxygen', word)\n",
    "            word = re.sub(r'\\bpaino\\b', 'piano', word)\n",
    "            word = re.sub(r'\\bpamphelt\\b', 'pamphlet', word)\n",
    "            word = re.sub(r'\\bpamplet\\b', 'pamphlet', word)\n",
    "            word = re.sub(r'\\bpaprikka\\b', 'paprika', word)\n",
    "            word = re.sub(r'\\bparahana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bparanha\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpasny\\b', 'pansy', word)\n",
    "            word = re.sub(r'\\bpianp\\b', 'piano', word)\n",
    "            word = re.sub(r'\\bpidgeon\\b', 'pigeon', word)\n",
    "            word = re.sub(r'\\bpiegon\\b', 'pigeon', word)\n",
    "            word = re.sub(r'\\bpihrana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirahna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirahnna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpiranah\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirannhea\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirhana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirhanna\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirranha\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirranhea\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bpirrhana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\bplataeu\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bplateu\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bpleasent\\b', 'pleasant', word)\n",
    "            word = re.sub(r'\\bpleatou\\b', 'plateau', word)\n",
    "            word = re.sub(r'\\bpoxygen\\b', 'oxygen', word)\n",
    "            word = re.sub(r'\\bprihana\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\brasberry\\b', 'raspberry', word)\n",
    "            word = re.sub(r'\\braseberry\\b', 'raspberry', word)\n",
    "            word = re.sub(r'\\brobbin\\b', 'robin', word)\n",
    "            word = re.sub(r'\\bsaxaphone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsaxapphone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsaxohpone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bSaxophone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsaxopon\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsaxphone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bsecatary\\b', 'secretary', word)\n",
    "            word = re.sub(r'\\bSecretary\\b', 'secretary', word)\n",
    "            word = re.sub(r'\\bshoe\\b', 'shoes', word)\n",
    "            word = re.sub(r'\\bsulfure\\b', 'sulfur', word)\n",
    "            word = re.sub(r'\\btangerene\\b', 'tangerine', word)\n",
    "            word = re.sub(r'\\bTangerine\\b', 'tangerine', word)\n",
    "            word = re.sub(r'\\btornadeo\\b', 'tornado', word)\n",
    "            #word = re.sub(r'\\bTree\\b', 'tree', word)\n",
    "            word = re.sub(r'\\btrees\\b', 'tree', word)\n",
    "            word = re.sub(r'\\btrought\\b', 'drought', word)\n",
    "            word = re.sub(r'\\btrumbone\\b', 'trombone', word)\n",
    "            word = re.sub(r'\\bTrumpet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\btrumphet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\btumeric\\b', 'turmeric', word)\n",
    "            word = re.sub(r'\\bumpet\\b', 'trumpet', word)\n",
    "            word = re.sub(r'\\bVelvet\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bvelvet3\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bvelviot\\b', 'velvet', word)\n",
    "            word = re.sub(r'\\bViolet\\b', 'violet', word)\n",
    "            word = re.sub(r'\\bwoman\\b', 'women', word)\n",
    "            word = re.sub(r'\\bwool\\\\b', 'wool', word)\n",
    "            word = re.sub(r'\\bxophone\\b', 'saxophone', word)\n",
    "            word = re.sub(r'\\bcinammon\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinamon\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinemmon\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinnamone\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bcinnemon\\b', 'cinnamon', word)\n",
    "            word = re.sub(r'\\bchior\\b', 'choir', word)\n",
    "            word = re.sub(r'\\bhaliberd\\b', 'halibut', word)\n",
    "            \n",
    "            #Buffer\n",
    "            word = re.sub(r'\\barm\\b', 'arms', word)\n",
    "            word = re.sub(r'\\bgrreen\\b', 'green', word)\n",
    "\n",
    "            #Experiment 3, recall 2\n",
    "            word = re.sub(r'\\bdree\\b', 'tree', word)\n",
    "            word = re.sub(r'\\bpiranhea\\b', 'piranha', word)\n",
    "            word = re.sub(r'\\boxigen\\b', 'oxygen', word)\n",
    "            word = re.sub(r'\\bogran\\b', 'organ', word)\n",
    "            word = re.sub(r'\\bchaor\\b', 'chair', word)\n",
    "            word = re.sub(r'\\brain\\'\\b', 'rain', word)\n",
    "            word = re.sub(r'\\bblosue\\b', 'blouse', word)\n",
    "            word = re.sub(r'\\boxegen\\b', 'oxygen', word)\n",
    "            word = re.sub(r'\\bparrot\\\\b', 'parrot', word)\n",
    "            word = re.sub(r'\\benigneer\\b', 'engineer', word)\n",
    "            word = re.sub(r'\\bhalabit\\b', 'halibut', word)\n",
    "            word = re.sub(r'\\bsecetary\\b', 'secretary', word)\n",
    "            word = re.sub(r'\\bdemtist\\b', 'dentist', word)\n",
    "            word = re.sub(r'\\bdenist\\b', 'dentist', word)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Biase comes from file name\n",
    "            if os.path.exists('new_encoding_' + str(subjectNumber) +'_0.pkl'):\n",
    "                biased = 0\n",
    "            else:\n",
    "                biased = 1\n",
    "\n",
    "            # Order comes from SN and impacts correct study list reference\n",
    "            if (subjectNumber%2 == 0) and (biased == 1):\n",
    "                order = 1\n",
    "                if word in studyList2:\n",
    "                    correct = 1\n",
    "                else:\n",
    "                    correct = 0\n",
    "            elif (subjectNumber%2 == 1) and (biased == 1):\n",
    "                order = 0     \n",
    "                if word in studyList1:\n",
    "                    correct = 1\n",
    "                else:\n",
    "                    correct = 0\n",
    "            if (subjectNumber%2 == 0) and (biased == 0):\n",
    "                order = 0\n",
    "                if word in studyList1:\n",
    "                    correct = 1\n",
    "                else:\n",
    "                    correct = 0\n",
    "            elif (subjectNumber%2 == 1) and (biased == 0):\n",
    "                order = 1     \n",
    "                if word in studyList2:\n",
    "                    correct = 1\n",
    "                else:\n",
    "                    correct = 0\n",
    "\n",
    "            # Indicate buffer\n",
    "            buffer = 0\n",
    "            for x in word:\n",
    "                if word in ['arms', 'green', 'uncle', 'hours']:\n",
    "                    buffer = 1\n",
    "                else:\n",
    "                    buffer = 0\n",
    "                    \n",
    "            # Collaboration comes from retrieval_1_1 files\n",
    "            if os.path.exists('new_retrieve_' + str(subjectNumber) +'_0_1.pkl'):\n",
    "                collaboration = 0\n",
    "            else:\n",
    "                collaboration = 1\n",
    "\n",
    "            #Problem: This is only the words from the 2. recall. And the column with who they collaborated\n",
    "            collaborator = 0\n",
    "            if os.path.exists('new_retrieve_' + str(subjectNumber) + str(subjectNumber+1) + '_1_1.pkl'):\n",
    "                collaborator = str(subjectNumber+1)\n",
    "            elif os.path.exists('new_retrieve_' + str(subjectNumber-1) + str(subjectNumber) + '_1_1.pkl'):\n",
    "                collaborator = str(subjectNumber-1)\n",
    "            else:\n",
    "                collaborator = 0\n",
    "\n",
    "            trialDict = {'SN':subjectNumber, 'biased':biased, 'order':order, 'phase':phase, 'collaboration':collaboration, 'collaborator':collaborator, \n",
    "                         'word':word, 'correct':correct, 'buffer':buffer}\n",
    "            df_dlm = df_dlm.append(trialDict, ignore_index=True)\n",
    "\n",
    "    except EOFError:\n",
    "        pass\n",
    "\n",
    "df_dlm = df_dlm.drop_duplicates(['SN','word', 'phase']) #Delete duplicates\n",
    "df_dlm = df_dlm.replace('', np.nan)\n",
    "df_dlm = df_dlm.dropna(axis=0, how='any', subset=['word'])\n",
    "\n",
    "df_dlm.to_csv('2022-07-28_Similarity_Exp2_AllWords_Clean2.csv', index=False)\n",
    "df_dlm['number'] = df_dlm['word']\n",
    "translator = {'crow':1,'eagle':2,'finch':3,'parrot':4,'pigeon':5,'cardinal':6,'nitrogen':7,'helium':8,'chlorine':9,'calcium':10,'oxygen':11,\n",
    "          'mercury':12,'trout':13,'flounder':14,'halibut':15,'guppy':16,'piranha':17,'shark':18,'carnation':19,'orchid':20,'pansy':21,'daffodil':22,'violet':23,'rose':24,\n",
    "          'nectarine':25,'pear':26,'apple':27,'grape':28,'raspberry':29,'cherry':30,'tuba':31,'drum':32,'trumpet':33,'saxophone':34,'piano':35,'organ':36,'tree':37,'ocean':38,\n",
    "          'canyon':39,'mountain':40,'plateau':41,'cave':42,'cinnamon':43,'mustard':44,'basil':45,'oregano':46,'paprika':47,'salt':48,'cotton':49,'wool':50,'velvet':51,\n",
    "          'linen':52,'leather':53,'denim':54,'flyer':55,'newspaper':56,'comic':57,'essay':58,'pamphlet':59,'book':60,'tornado':61,'hail':62,'blizzard':63,'rain':64,'drought':65,\n",
    "          'lightning':66,'jacket':67,'dress':68,'blouse':69,'underwear':70,'shoes':71,'shirt':72,'lamp':73,'desk':74,'bookcase':75,'dresser':76,'chair':77,'recliner':78,\n",
    "          'banker':79,'dentist':80,'secretary':81,'engineer':82,'nurse':83,'doctor':84,'a':85,'architect':86,'avidafil?':87,'ballet':88,'baseball':89,'bed':90,\n",
    "          'bird':91,'blueberry':92,'cage':93,'canary':94,'carbon':95,'chemical':96,'clarinet':97,'coat':98,'concrete':99,'daisy':100,'dog':101,'experimenter':102,\n",
    "          'fish':103,'flamenco':104,'flower':105,'flute':106,'fruit':107,'give':108,'grass':109,'hydrogen':110,'instrument':111,'lawyer':112,'library':113,'lily':114,\n",
    "          'lithium':115,'lung':116,'melon':117,'music':118,'nickel':119,'nylon':120,'pants':121,'parsley':122,'pepper':123,'project':124,'red':125,'salmon':126,\n",
    "          'satin':127,'sea':128,'sky':129,'slave':130,'sofa':131,'storm':132,'table':133,'tango':134,'teacher':135,'the':136,'thunder':137,'thunderstorm':138,'tissue':139,\n",
    "          'to':140,'trombone':141,'trousers':142,'tulip':143,'tuna':144,'willing':145,'address':146,'cavern':147,'homework':148,'magazine':149,'are':150,\n",
    "          'dove':151,'sodium':152,'hurricaine':153,'hurricane':154,'violent':155,'violin':156,'green':157,'uncle':158,'hour':159,'arms':160,'orchard':161,\n",
    "          'paper':162,'sunflower':163,'dandelion':164,'snow':165,'green':166,'professor':167,'socks':168,'purse':169,'ar':170,'e':171,'mium':172,\n",
    "          't':173,'rk':174, 'hid':175,'breakfast':176,'uncle':177,'hat':178,'hale':179,'speacker':180,'dance':181,'baker':182,'couch':183,'tube':184,\n",
    "          'valley':185,'tangerine':186,'guitar':187,'orange':188,'skirt':189,'robin':190,'peach':191,'lavender':192,'thyme':193,'drawer':194,\n",
    "          'cloud':195,'word':196,'paper':197,'carnelian':198,'article':199,'daydream':200,'tectonicplate':201,'blossom':202,'cloth':203,'tsunami':204,\n",
    "          'legs':205,'keyboard':206,'fiction':207,'water':208,'earth':209,'light':210,'hair':211,'rocks':212,'moon':213,'weather':214,'wind':215,\n",
    "          'door':216,'letter':217,'time':218,'yellow':219,'river':220,'lake':221,'typhoon':222,'gloves':223,'magnesium':224,'ear':225,'camp':226,\n",
    "          'seed':227,'phone':228,'fresh':229,'lenin':230,'in':231,'halogen':232,'wager':233,'research':234,'positive':235,'success':236,'pelican':237,\n",
    "          'computer':238,'plum':239,'although':240,'brazil':241,'fly':242,'brochure':243,'flood':244,'perch':245,'year':246,'lilac':247,'silk':248,\n",
    "          'disaster':249,'bloom':250,'politician':251,'onion':252,'jazz':253,'peat':254,'flinch':255,'weatherphenomenon':256,'natural':257,\n",
    "          'flavoring':258,'food':259,'furniture':260,'formation':261,'papya':262,'snowstorm':263,'basketball':264, 'carnage':454, 'birch':455, 'pine':456}\n",
    "df_dlm.number = [translator[item] for item in df_dlm.number] \n",
    "                          \n",
    "# Import files with pickle files                      \n",
    "df_dlm.to_csv('2022-07-28_Similarity_Exp2_AllWords_Clean2.csv', index=False)\n",
    "print(df_dlm)\n",
    "df = pd.read_csv('2022-07-28_Similarity_Exp2_AllWords_Clean1.csv')                        \n",
    "\n",
    "# Combine dataframe based on pickle and dlm files\n",
    "frames = [df, df_dlm]\n",
    "df_both = pd.concat(frames)\n",
    "df_both.to_csv('2022-07-28_Similarity_Exp2_AllWords_Clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       SN  biased  order  collaboration  collaborator      word  correct  \\\n",
      "0       1       1      0              1             2  nitrogen        1   \n",
      "28      2       0      0              1             1     apple        1   \n",
      "55      3       0      1              0             0   tornado        1   \n",
      "91      4       1      1              1             5      crow        1   \n",
      "117     5       0      1              1             4    oxygen        1   \n",
      "...   ...     ...    ...            ...           ...       ...      ...   \n",
      "4740  193       1      0              1           192     shark        1   \n",
      "4778  194       1      1              0             0     tango        0   \n",
      "4789   25       0      1              0             0  chlorine        1   \n",
      "4820  126       1      1              0             0    oxygen        1   \n",
      "4837  147       1      0              1           146     finch        1   \n",
      "\n",
      "      buffer  phase  number  \n",
      "0          0      2       7  \n",
      "28         0      2      27  \n",
      "55         0      2      61  \n",
      "91         0      2       1  \n",
      "117        0      2      11  \n",
      "...      ...    ...     ...  \n",
      "4740       0      2      18  \n",
      "4778       0      2     134  \n",
      "4789       0      2       9  \n",
      "4820       0      2      11  \n",
      "4837       0      2       3  \n",
      "\n",
      "[191 rows x 10 columns]\n",
      "000= [14, 34, 38, 42, 56, 66, 70, 74, 78, 100, 116, 120, 124, 128, 132, 150, 156, 160, 164, 180]\n",
      "001= [3, 13, 35, 39, 43, 45, 47, 51, 59, 75, 79, 89, 101, 107, 119, 123, 127, 131, 135, 149, 163, 167, 171, 175, 183, 189, 25]\n",
      "010= [15, 33, 37, 41, 57, 65, 69, 73, 77, 91, 93, 117, 121, 125, 129, 137, 155, 159, 161, 177]\n",
      "011= [12, 24, 28, 36, 40, 44, 46, 50, 58, 60, 76, 82, 90, 106, 114, 118, 122, 130, 148, 162, 168, 172, 176, 186, 194, 126]\n",
      "111= [4, 8, 16, 20, 30, 54, 64, 68, 72, 80, 86, 94, 98, 102, 110, 140, 144, 154, 158, 182, 188, 190]\n",
      "101= [5, 9, 17, 21, 29, 55, 63, 67, 71, 81, 85, 95, 99, 103, 111, 141, 145, 153, 157, 181, 187, 191]\n",
      "100= [2, 6, 10, 18, 22, 26, 32, 48, 52, 62, 84, 88, 96, 104, 108, 112, 134, 138, 142, 146, 152, 166, 170, 174, 178, 184, 192]\n",
      "110= [1, 7, 11, 19, 23, 27, 31, 49, 53, 61, 83, 87, 97, 105, 109, 113, 133, 139, 143, 151, 165, 169, 173, 179, 185, 193, 147]\n",
      "000= 20\n",
      "001= 27\n",
      "010= 20\n",
      "011= 26\n",
      "111= 22\n",
      "101= 22\n",
      "100= 27\n",
      "110= 27\n"
     ]
    }
   ],
   "source": [
    "# This is to get an overview how many participants were part of which condition. could have just done .tolist().count(), but wanted to be sure everything works properly\n",
    "# 47 biased=0, order=0\n",
    "# 47 biased=1, order=0\n",
    "# 48 biased=0, order=1\n",
    "# 46 biased=1, order=1\n",
    "#----------------------\n",
    "# 000 = 20\n",
    "# 001 = 26\n",
    "# 010 = 27\n",
    "# 011 = 22\n",
    "# 100 = 25\n",
    "# 101 = 20\n",
    "# 110 = 22\n",
    "# 111 = 26\n",
    "#df = pd.read_csv('2020-12-11_Similarity_Exp2_AllWords_Clean.csv')\n",
    "df = pd.read_csv('2022-07-28_Similarity_Exp2_AllWords_Clean.csv')\n",
    "# necessary because every row is a word recalled by that specific participant, but here we just want to know the total number of participants\n",
    "df = df.drop_duplicates(subset=['SN'], keep='first') \n",
    "print(df)\n",
    "# Identify participants per condition\n",
    "df_000 = df.SN[(df['collaboration']==0) & (df['biased']==0) & (df['order']==0)].tolist()\n",
    "df_001 = df.SN[(df['collaboration']==0) & (df['biased']==0) & (df['order']==1)].tolist()\n",
    "df_010 = df.SN[(df['collaboration']==0) & (df['biased']==1) & (df['order']==0)].tolist()\n",
    "df_011 = df.SN[(df['collaboration']==0) & (df['biased']==1) & (df['order']==1)].tolist()\n",
    "df_111 = df.SN[(df['collaboration']==1) & (df['biased']==1) & (df['order']==1)].tolist()\n",
    "df_101 = df.SN[(df['collaboration']==1) & (df['biased']==0) & (df['order']==1)].tolist()\n",
    "df_100 = df.SN[(df['collaboration']==1) & (df['biased']==0) & (df['order']==0)].tolist()\n",
    "df_110 = df.SN[(df['collaboration']==1) & (df['biased']==1) & (df['order']==0)].tolist()\n",
    "print('000=', df_000)\n",
    "print('001=', df_001)\n",
    "print('010=', df_010)\n",
    "print('011=', df_011)\n",
    "print('111=', df_111)\n",
    "print('101=', df_101)\n",
    "print('100=', df_100)\n",
    "print('110=', df_110)\n",
    "# Count how many participants per condition\n",
    "df_000 = df.SN[(df['collaboration']==0) & (df['biased']==0) & (df['order']==0)].count()\n",
    "df_001 = df.SN[(df['collaboration']==0) & (df['biased']==0) & (df['order']==1)].count()\n",
    "df_010 = df.SN[(df['collaboration']==0) & (df['biased']==1) & (df['order']==0)].count()\n",
    "df_011 = df.SN[(df['collaboration']==0) & (df['biased']==1) & (df['order']==1)].count()\n",
    "df_111 = df.SN[(df['collaboration']==1) & (df['biased']==1) & (df['order']==1)].count()\n",
    "df_101 = df.SN[(df['collaboration']==1) & (df['biased']==0) & (df['order']==1)].count()\n",
    "df_100 = df.SN[(df['collaboration']==1) & (df['biased']==0) & (df['order']==0)].count()\n",
    "df_110 = df.SN[(df['collaboration']==1) & (df['biased']==1) & (df['order']==0)].count()\n",
    "print('000=', df_000)\n",
    "print('001=', df_001)\n",
    "print('010=', df_010)\n",
    "print('011=', df_011)\n",
    "print('111=', df_111)\n",
    "print('101=', df_101)\n",
    "print('100=', df_100)\n",
    "print('110=', df_110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __B4. Similarity for dyads by sub-group__\n",
    "* Attention: These analyses only work if no participant recalled no words (all participants recalled at least 1 word)\n",
    "* The order of str1 and str2 is important, as str1 always needs to correspond to the first SN in the combo and str2 to the second (CHECK WHETHER THIS IS TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('2020-12-11_Similarity_Exp2_AllWords_Clean.csv')\n",
    "df = pd.read_csv('2022-07-28_Similarity_Exp2_AllWords_Clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL MEAN RECALLED\n",
      "mean_total biased           25.554974\n",
      "order            25.554974\n",
      "collaboration    25.554974\n",
      "collaborator     25.554974\n",
      "word             25.554974\n",
      "correct          25.554974\n",
      "buffer           25.554974\n",
      "phase            25.554974\n",
      "number           25.554974\n",
      "dtype: float64\n",
      "sd_total biased           10.645279\n",
      "order            10.645279\n",
      "collaboration    10.645279\n",
      "collaborator     10.645279\n",
      "word             10.645279\n",
      "correct          10.645279\n",
      "buffer           10.645279\n",
      "phase            10.645279\n",
      "number           10.645279\n",
      "dtype: float64\n",
      "TOTAL CORRECT MEAN RECALLED\n",
      "SN\n",
      "1      28\n",
      "2      24\n",
      "3      36\n",
      "4      24\n",
      "5      45\n",
      "       ..\n",
      "190    16\n",
      "191    29\n",
      "192    35\n",
      "193    37\n",
      "194    10\n",
      "Name: correct, Length: 191, dtype: int64 24.13612565445026 10.638178695767204\n",
      "DIFF INDIV/COLLAB RECALLED\n",
      "difference biased           4.66524\n",
      "order            4.66524\n",
      "collaboration    4.66524\n",
      "collaborator     4.66524\n",
      "word             4.66524\n",
      "correct          4.66524\n",
      "buffer           4.66524\n",
      "phase            4.66524\n",
      "number           4.66524\n",
      "dtype: float64\n",
      "diff_correct biased           4.493123\n",
      "order            4.493123\n",
      "collaboration    4.493123\n",
      "collaborator     4.493123\n",
      "word             4.493123\n",
      "correct          4.493123\n",
      "buffer           4.493123\n",
      "phase            4.493123\n",
      "number           4.493123\n",
      "dtype: float64\n",
      "diff_intrusion biased           0.190932\n",
      "order            0.190932\n",
      "collaboration    0.190932\n",
      "collaborator     0.190932\n",
      "word             0.190932\n",
      "correct          0.190932\n",
      "buffer           0.190932\n",
      "phase            0.190932\n",
      "number           0.190932\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean averages\n",
    "# Total word recalled\n",
    "print('TOTAL MEAN RECALLED')\n",
    "sum_pP_count = df.groupby('SN').count()\n",
    "mean_total = sum_pP_count.mean()\n",
    "sd_total = sum_pP_count.std()\n",
    "print('mean_total', mean_total)\n",
    "print('sd_total', sd_total)\n",
    "# Correct words recalled\n",
    "print('TOTAL CORRECT MEAN RECALLED')\n",
    "sum_pP_correct = df.groupby('SN')['correct'].sum()\n",
    "mean_correct = sum_pP_correct.mean()\n",
    "sd_correct = sum_pP_correct.std()\n",
    "print(sum_pP_correct, mean_correct, sd_correct)\n",
    "# Means for collaborative condition\n",
    "print('DIFF INDIV/COLLAB RECALLED')\n",
    "mean_total_collaboration = df[(df['collaboration'] == 1)].groupby('SN').count().mean()\n",
    "mean_total_noncollaboration = df[(df['collaboration'] == 0)].groupby('SN').count().mean()\n",
    "diff = mean_total_collaboration - mean_total_noncollaboration\n",
    "mean_total_collaboration_corr = df[(df['collaboration'] == 1) & (df['correct'] == 1)].groupby('SN').count().mean()\n",
    "mean_total_noncollaboration_corr = df[(df['collaboration'] == 0) & (df['correct'] == 1)].groupby('SN').count().mean()\n",
    "diff_correct = mean_total_collaboration_corr - mean_total_noncollaboration_corr\n",
    "mean_total_collaboration_int = df[(df['collaboration'] == 1) & (df['correct'] == 0)].groupby('SN').count().mean()\n",
    "mean_total_noncollaboration_int = df[(df['collaboration'] == 0) & (df['correct'] == 0)].groupby('SN').count().mean()\n",
    "diff_intrusion = mean_total_collaboration_int - mean_total_noncollaboration_int\n",
    "#print(diff, diff_correct, diff_intrusion)\n",
    "print('difference', diff)\n",
    "print('diff_correct', diff_correct)\n",
    "print('diff_intrusion',  diff_intrusion)\n",
    "#sum_pP_count_collab = df.groupby('SN').['collaboration'==1].count()\n",
    "#print(sum_pP_count_collab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(df, collab1, bias1, order1, phase1, collab2, bias2, order2, phase2, TrueCollab=True, Self=True):\n",
    "  \n",
    "    \"\"\"\n",
    "    USAGE:\n",
    "    all similarity measures = similarity(df, collab1, bias1, order1, phase1, collab2, bias2, order2, phase2, TrueCollab=True, Self=True)\n",
    "    Attention!!! If synthetic (nominal collaborative) groups are included in the comparison, they always have to be in the second position!!!\n",
    "\n",
    "    ARGUMENTS:\n",
    "    df          Dataframe to be used\n",
    "    \n",
    "    collab1     Collaborative condition for group 1: 1=yes, 0=no\n",
    "    bias1       Bias condition for group1: 0=no bias, 1=bias1\n",
    "    order1      If biased, then order 0 or 1\n",
    "    phase1      Which recall phase? Recall 2 for experiments 1(A) and 2(1B)\n",
    "    \n",
    "    collab2     Collaborative condition for group 2: 1=yes, 0=no\n",
    "    bias2       Bias condition for group 2: 0=no bias, 1=bias1\n",
    "    order2      If biased, then order 0 or 1\n",
    "    phase2      Which recall phase? Recall 2 for experiments 1(A) and 2(1B)\n",
    "    \n",
    "    TrueCollab  Did the participants on the two groups actually collaborate: Collaborated with each other=True, Nominal/Synthetic groups=False\n",
    "    Self        Only for experiment 3: Is this a pre-post collaboration comparison? (How about within-group?)\n",
    "\n",
    "    DESCRIPTION:\n",
    "    This helps to calculate the different specific group comparisons\n",
    "    \n",
    "    QUESTIONS:\n",
    "    why would bias be 2? --> for collaborative recall\n",
    "    where does df come from? it contains all? above. Make sure cells are run in correct order\n",
    "    \n",
    "    REMINDER:\n",
    "       \n",
    "    \"\"\"\n",
    "\n",
    "    # Identify all participants in the two groups\n",
    "    # Select the correct group 1 (identifies all participants in this group)\n",
    "    Group1 = df.SN[(df['collaboration'] == collab1) & (df['biased'] == bias1)  & (df['order'] == order1) & (df['phase'] == phase1)]\n",
    "    Group1 = Group1.unique().tolist()\n",
    "    print('normalGroup1')\n",
    "    print('SN1', Group1)\n",
    "           \n",
    "    # Select the correct group 2\n",
    "    Group2 = df.SN[(df['collaboration'] == collab2) & (df['biased'] == bias2)  & (df['order'] == order2) & (df['phase'] == phase2)]\n",
    "    Group2 = Group2.unique().tolist()\n",
    "    print('normalGroup2')\n",
    "    print('SN2', Group2)    \n",
    "    \n",
    "    # Merge all participants for both groups\n",
    "    groupSN = Group2 + Group1\n",
    "    print('groupSN', groupSN)\n",
    "    \n",
    "    # Calculate all potential permutations (aka combinations of participants) for ngram \"index\"\n",
    "    # For bigrams\n",
    "    maxi = max(df.number) \n",
    "    all = list(range(0,maxi))\n",
    "    p2 = list(itertools.permutations(all, 2))\n",
    "    # For trigrams\n",
    "    maxi = max(df.number) \n",
    "    all = list(range(0,maxi))\n",
    "    \n",
    "    # Calculate the average descriptive stats (# true, fals words) per participant in first group (1. set up list, 2. calculate mean per SN, 3. save mean per SN, 4. calculate mean across group)\n",
    "    average_recalled_t_Group1 = []\n",
    "    average_correct_recalled_t_Group1 = []\n",
    "    average_intrusion_recalled_t_Group1 = []\n",
    "    for i in Group1:            \n",
    "        average_recalled_Group1 = df.number[(df['SN'] == i) & (df['collaboration'] == collab1) & (df['biased'] == bias1)  & (df['order'] == order1) & (df['phase'] == phase1)].count()\n",
    "        average_correct_recalled_Group1 = df.number[(df['SN'] == i) & (df['correct'] == 1) & (df['collaboration'] == collab1) & (df['biased'] == bias1)  & (df['order'] == order1) & (df['phase'] == phase1)].count()\n",
    "        average_intrusion_recalled_Group1 = df.number[(df['SN'] == i) & (df['correct'] == 0) & (df['collaboration'] == collab1) & (df['biased'] == bias1)  & (df['order'] == order1) & (df['phase'] == phase1)].count()\n",
    "        average_recalled_t_Group1.append(average_recalled_Group1)\n",
    "        average_correct_recalled_t_Group1.append(average_correct_recalled_Group1)\n",
    "        average_intrusion_recalled_t_Group1.append(average_intrusion_recalled_Group1)\n",
    "    average_recalled_t_Group1 = np.mean(average_recalled_t_Group1)\n",
    "    average_correct_recalled_t_Group1 = np.mean(average_correct_recalled_t_Group1)\n",
    "    average_intrusion_recalled_t_Group1 = np.mean(average_intrusion_recalled_t_Group1)\n",
    "    print('average_recalled_t_Group1=', average_recalled_t_Group1)\n",
    "    print('average_correct_recalled_t_Group1=', average_correct_recalled_t_Group1)\n",
    "    print('average_intrusion_recalled_t_Group1=', average_intrusion_recalled_t_Group1)\n",
    "\n",
    "    # Calculate the average descriptive stats (# true, fals words) per participant in second group (1. set up list, 2. calculate mean per SN, 3. save mean per SN, 4. calculate mean across group)\n",
    "    average_recalled_t_Group2 = []\n",
    "    average_correct_recalled_t_Group2 = []\n",
    "    average_intrusion_recalled_t_Group2 = []\n",
    "    for i in Group2:\n",
    "        average_recalled_Group2 = df.number[(df['SN'] == i) & (df['collaboration'] == collab2) & (df['biased'] == bias2)  & (df['order'] == order2) & (df['phase'] == phase2)].count()\n",
    "        average_correct_recalled_Group2 = df.number[(df['SN'] == i) & (df['correct'] == 1) & (df['collaboration'] == collab2) & (df['biased'] == bias2)  & (df['order'] == order2) & (df['phase'] == phase2)].count()\n",
    "        average_intrusion_recalled_Group2 = df.number[(df['SN'] == i) & (df['correct'] == 0) & (df['collaboration'] == collab2) & (df['biased'] == bias2)  & (df['order'] == order2) & (df['phase'] == phase2)].count()\n",
    "        average_recalled_t_Group2.append(average_recalled_Group2)\n",
    "        average_correct_recalled_t_Group2.append(average_correct_recalled_Group2)\n",
    "        average_intrusion_recalled_t_Group2.append(average_intrusion_recalled_Group2)\n",
    "    average_recalled_t_Group2 = np.mean(average_recalled_t_Group2)\n",
    "    average_correct_recalled_t_Group2 = np.mean(average_correct_recalled_t_Group2)\n",
    "    average_intrusion_recalled_t_Group2 = np.mean(average_intrusion_recalled_t_Group2)\n",
    "    print('average_recalled_t_Group2=', average_recalled_t_Group2)\n",
    "    print('average_correct_recalled_t_Group2=', average_correct_recalled_t_Group2)\n",
    "    print('average_intrusion_recalled_t_Group2=', average_intrusion_recalled_t_Group2)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the dyad specific similarity measures \n",
    "\n",
    "    # Identify all potential/theoretical dyads across the two groups\n",
    "    # In theory I only need this for nominal and nominal collaborative groups, but this was the easiest (albeit not the fastest way)\n",
    "    allCombos = list(itertools.product(Group1, Group2))\n",
    "    allCombos2 = len(allCombos)\n",
    "    \n",
    "    # Set up data frame to save results per dyad in\n",
    "    dfresults = pd.DataFrame(columns=['Comparison','Dyad', 'intersection', 'overlap', 'Jaccard', 'SMC', 'lcs', \\\n",
    "                                      'OmEuni', 'OmEbi', 'OdEuni', 'OdEbi', 'OdMuni', 'OdMbi', 'OmEdMuni', 'OmEdMbi', \\\n",
    "                                      'OmEdMmEMuni', 'OmEdMmEMbi', 'pairedFreq', 'ITR2', 'ARC2', 'editdist','mod_editdist', \\\n",
    "                                      'editdist_IDST', 'editdist_IDS', 'editdist_ID', 'editdist_IDT'])\n",
    "\n",
    "    # Calculate similarity measures for every dyad\n",
    "    # for every dyad (in all combos)\n",
    "    for j in allCombos:\n",
    "        # Reset all values\n",
    "        if Self == False and TrueCollab == False:            \n",
    "            #str1 = df.number[(df['collaboration'] == collab1) & (df['biased'] == bias1) & (df['order'] == order1) & (df['SN'] == j[0]) & (df['phase'] == phase1)]\n",
    "            #str2 = df.number[(df['collaboration'] == collab2) & (df['biased'] == bias2) & (df['order'] == order2) & (df['SN'] == j[1]) & (df['phase'] == phase2)]    \n",
    "            str1 = df.number[(df['collaboration'] == collab1) & (df['biased'] == bias1) & (df['order'] == order1) & (df['SN'] == j[0]) & (df['collaborator'] != j[1]) & (df['phase'] == phase1)]\n",
    "            str2 = df.number[(df['collaboration'] == collab2) & (df['biased'] == bias2) & (df['order'] == order2) & (df['SN'] == j[1]) & (df['collaborator'] != j[0]) & (df['phase'] == phase2)] \n",
    "        # This would only need to be done for each SN and not all combos\n",
    "        elif Self == True and TrueCollab == False:\n",
    "            str1 = df.number[(df['collaboration'] == collab1) & (df['biased'] == bias1) & (df['order'] == order1) & (df['SN'] == j[0]) & (df['phase'] == phase1)]\n",
    "            str2 = df.number[(df['collaboration'] == collab2) & (df['biased'] == bias2) & (df['order'] == order2) & (df['SN'] == j[0]) & (df['phase'] == phase2)]           \n",
    "        elif Self == False and TrueCollab == True:\n",
    "            str1 = df.number[(df['collaboration'] == collab1) & (df['biased'] == bias1) & (df['order'] == order1) & (df['SN'] == j[0]) & (df['collaborator'] == j[1]) & (df['phase'] == phase1)]\n",
    "            str2 = df.number[(df['collaboration'] == collab2) & (df['biased'] == bias2) & (df['order'] == order2) & (df['SN'] == j[1]) & (df['collaborator'] == j[0]) & (df['phase'] == phase2)] \n",
    "        \n",
    "        num_comm = len(np.intersect1d(str1, str2))\n",
    "\n",
    "        if len(str1)==0 or len(str2)==0:\n",
    "            pass\n",
    "        else:\n",
    "            #num_pairs = ngram_abs(str1, str2, 2, p2, unidirectional=True)\n",
    "            #num_trip = ngram_abs(str1, str2, 3, p3, unidirectional=True) \n",
    "\n",
    "            a = intersection(str1, str2)\n",
    "            b = overlap(str1, str2)\n",
    "            c = Jaccard(str1, str2)\n",
    "            d = SMC(str1, str2)\n",
    "            f = OmE(str1, str2, 2, unidirectional=True)\n",
    "            g = OmE(str1, str2, 2, unidirectional=False)\n",
    "            h = OdE(str1, str2, 2, unidirectional=True)\n",
    "            z = OdE(str1, str2, 2, unidirectional=False)\n",
    "            k = OdM(str1, str2, 2, unidirectional=True)\n",
    "            l = OdM(str1, str2, 2, unidirectional=False)\n",
    "            m = OmEdM(str1, str2, 2, unidirectional=True)\n",
    "            n = OmEdM(str1, str2, 2, unidirectional=False)\n",
    "            o = OmEdMmE(str1, str2, 2, unidirectional=True)\n",
    "            p = OmEdMmE(str1, str2, 2, unidirectional=False)\n",
    "            q = pairedFreq(str1, str2)\n",
    "            r = ITR2(str1, str2)\n",
    "            s = ARC2(str1, str2)\n",
    "\n",
    "            str1 = str1.reset_index(drop=True)\n",
    "            str2 = str2.reset_index(drop=True)\n",
    "            str1 = np.array(str1)\n",
    "            str2 = np.array(str2)\n",
    "            e = lcs(str1, str2)\n",
    "            t = editdist(str1, str2, min_threshold = 0)\n",
    "            u = mod_editdist(str1, str2, min_threshold = 0)\n",
    "            v = edit_dists(str1, str2, insert=True, delete=True, substitute=True, transpose=True)\n",
    "            w = edit_dists(str1, str2, insert=True, delete=True, substitute=True, transpose=False)\n",
    "            x = edit_dists(str1, str2, insert=True, delete=True, substitute=False, transpose=False)\n",
    "            y = edit_dists(str1, str2, insert=True, delete=True, substitute=False, transpose=True)\n",
    "\n",
    "            # it saves all prior values for abc... if j exists but no update. so have to reset all to zero\n",
    "            trialDict = {'Comparison':(str(collab1) + str(bias1) + str(order1) + str(phase1) + str(int(TrueCollab)) + '_' \\\n",
    "                         + str(collab2) + str(bias2) + str(order2) + str(phase2) + str(int(TrueCollab))),'Dyad':j, \\\n",
    "                         'intersection':a, 'overlap':b, 'Jaccard':c, 'SMC':d, 'lcs':e,  \\\n",
    "                         'OmEuni':f, 'OmEbi':g, 'OdEuni':h, 'OdEbi':z, 'OdMuni':k, 'OdMbi':l, 'OmEdMuni':m, 'OmEdMbi':n, 'OmEdMmEMuni':o, \\\n",
    "                         'OmEdMmEMbi':p, 'pairedFreq':q, 'ITR2':r, 'ARC2':s, 'editdist':t,'mod_editdist':u, \\\n",
    "                         'editdist_IDST':v, 'editdist_IDS':w, 'editdist_ID':x, 'editdist_IDT':y}              \n",
    "                        # 'num_pairs':num_pairs, 'num_trip':num_trip,\n",
    "            dfresults = dfresults.append(trialDict, ignore_index=True)    \n",
    "        \n",
    "    # Calculate mean for descriptive statistics\n",
    "    mean = dfresults.mean()\n",
    "    #mode_0000_0001 = dfresults.mode()\n",
    "    #median_0000_0001 = dfresults.median()\n",
    "    print('Mean=', mean)\n",
    "    #print('Mode=',mode_0000_0001)\n",
    "    #print('Median=', median_0000_0001)\n",
    "    # If file name is changed, it has to also be changed below when merging the different files\n",
    "    dfresults.to_csv('2022-07-28_Similarity_Exp2_AllWords_Clean_'+ str(collab1) + str(bias1) + str(order1) + str(phase1) + str(int(TrueCollab)) + str(int(Self))+ '_' +  str(collab2) + str(bias2) + str(order2) + str(phase2) + str(int(TrueCollab))+ str(int(Self)) +'_Results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOMINAL POST\n",
      "normalGroup1\n",
      "SN1 [14, 34, 38, 42, 56, 66, 70, 74, 78, 100, 116, 120, 124, 128, 132, 150, 156, 160, 164, 180]\n",
      "normalGroup2\n",
      "SN2 [15, 33, 37, 41, 57, 65, 69, 73, 77, 91, 93, 117, 121, 125, 129, 137, 155, 159, 161, 177]\n",
      "groupSN [15, 33, 37, 41, 57, 65, 69, 73, 77, 91, 93, 117, 121, 125, 129, 137, 155, 159, 161, 177, 14, 34, 38, 42, 56, 66, 70, 74, 78, 100, 116, 120, 124, 128, 132, 150, 156, 160, 164, 180]\n",
      "average_recalled_t_Group1= 29.7\n",
      "average_correct_recalled_t_Group1= 28.05\n",
      "average_intrusion_recalled_t_Group1= 1.65\n",
      "average_recalled_t_Group2= 23.95\n",
      "average_correct_recalled_t_Group2= 22.15\n",
      "average_intrusion_recalled_t_Group2= 1.8\n",
      "Mean= Comparison            inf\n",
      "intersection     8.625000\n",
      "overlap          0.407749\n",
      "Jaccard          0.180868\n",
      "SMC              0.628239\n",
      "lcs              1.455000\n",
      "OmEuni           0.561202\n",
      "OmEbi            1.014903\n",
      "OdEuni           6.254373\n",
      "OdEbi            5.753817\n",
      "OdMuni           0.076238\n",
      "OdMbi            0.140859\n",
      "OmEdMuni         0.064128\n",
      "OmEdMbi          0.116640\n",
      "OmEdMmEMuni      0.064983\n",
      "OmEdMmEMbi       0.119765\n",
      "pairedFreq       1.014903\n",
      "ITR2             0.140859\n",
      "ARC2             0.119765\n",
      "editdist         0.061765\n",
      "mod_editdist     0.061907\n",
      "editdist_IDST    0.061907\n",
      "editdist_IDS     0.061765\n",
      "editdist_ID      0.143919\n",
      "editdist_IDT     0.149212\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [3, 13, 35, 39, 43, 45, 47, 51, 59, 75, 79, 89, 101, 107, 119, 123, 127, 131, 135, 149, 163, 167, 171, 175, 183, 189, 25]\n",
      "normalGroup2\n",
      "SN2 [12, 24, 28, 36, 40, 44, 46, 50, 58, 60, 76, 82, 90, 106, 114, 118, 122, 130, 148, 162, 168, 172, 176, 186, 194, 126]\n",
      "groupSN [12, 24, 28, 36, 40, 44, 46, 50, 58, 60, 76, 82, 90, 106, 114, 118, 122, 130, 148, 162, 168, 172, 176, 186, 194, 126, 3, 13, 35, 39, 43, 45, 47, 51, 59, 75, 79, 89, 101, 107, 119, 123, 127, 131, 135, 149, 163, 167, 171, 175, 183, 189, 25]\n",
      "average_recalled_t_Group1= 22.25925925925926\n",
      "average_correct_recalled_t_Group1= 20.925925925925927\n",
      "average_intrusion_recalled_t_Group1= 1.3333333333333333\n",
      "average_recalled_t_Group2= 18.46153846153846\n",
      "average_correct_recalled_t_Group2= 17.307692307692307\n",
      "average_intrusion_recalled_t_Group2= 1.1538461538461537\n",
      "Mean= Comparison            inf\n",
      "intersection     4.876068\n",
      "overlap          0.298547\n",
      "Jaccard          0.125689\n",
      "SMC              0.684894\n",
      "lcs              1.192308\n",
      "OmEuni           0.263332\n",
      "OmEbi            0.516693\n",
      "OdEuni           3.959505\n",
      "OdEbi            4.273886\n",
      "OdMuni           0.055541\n",
      "OdMbi            0.115863\n",
      "OmEdMuni         0.044996\n",
      "OmEdMbi          0.094773\n",
      "OmEdMmEMuni      0.045739\n",
      "OmEdMmEMbi       0.097816\n",
      "pairedFreq       0.516693\n",
      "ITR2             0.115863\n",
      "ARC2             0.097816\n",
      "editdist         0.050785\n",
      "mod_editdist     0.050857\n",
      "editdist_IDST    0.050857\n",
      "editdist_IDS     0.050785\n",
      "editdist_ID      0.118081\n",
      "editdist_IDT     0.122554\n",
      "dtype: float64\n",
      "NOMINAL COLLABORATIVE POST\n",
      "normalGroup1\n",
      "SN1 [4, 8, 16, 20, 30, 54, 64, 68, 72, 80, 86, 94, 98, 102, 110, 140, 144, 154, 158, 182, 188, 190]\n",
      "normalGroup2\n",
      "SN2 [5, 9, 17, 21, 29, 55, 63, 67, 71, 81, 85, 95, 99, 103, 111, 141, 145, 153, 157, 181, 187, 191]\n",
      "groupSN [5, 9, 17, 21, 29, 55, 63, 67, 71, 81, 85, 95, 99, 103, 111, 141, 145, 153, 157, 181, 187, 191, 4, 8, 16, 20, 30, 54, 64, 68, 72, 80, 86, 94, 98, 102, 110, 140, 144, 154, 158, 182, 188, 190]\n",
      "average_recalled_t_Group1= 24.045454545454547\n",
      "average_correct_recalled_t_Group1= 23.09090909090909\n",
      "average_intrusion_recalled_t_Group1= 0.9545454545454546\n",
      "average_recalled_t_Group2= 32.18181818181818\n",
      "average_correct_recalled_t_Group2= 30.90909090909091\n",
      "average_intrusion_recalled_t_Group2= 1.2727272727272727\n",
      "Mean= Comparison             inf\n",
      "intersection     10.391775\n",
      "overlap           0.466441\n",
      "Jaccard           0.222432\n",
      "SMC               0.633191\n",
      "lcs               1.551948\n",
      "OmEuni            0.630805\n",
      "OmEbi             1.203168\n",
      "OdEuni            5.700307\n",
      "OdEbi             5.343164\n",
      "OdMuni            0.074879\n",
      "OdMbi             0.142731\n",
      "OmEdMuni          0.060996\n",
      "OmEdMbi           0.114966\n",
      "OmEdMmEMuni       0.061837\n",
      "OmEdMmEMbi        0.118242\n",
      "pairedFreq        1.203168\n",
      "ITR2              0.142731\n",
      "ARC2              0.118242\n",
      "editdist          0.067013\n",
      "mod_editdist      0.067013\n",
      "editdist_IDST     0.067013\n",
      "editdist_IDS      0.067013\n",
      "editdist_ID       0.159423\n",
      "editdist_IDT      0.166353\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [1, 7, 11, 19, 23, 27, 31, 49, 53, 61, 83, 87, 97, 105, 109, 113, 133, 139, 143, 151, 165, 169, 173, 179, 185, 193, 147]\n",
      "normalGroup2\n",
      "SN2 [2, 6, 10, 18, 22, 26, 32, 48, 52, 62, 84, 88, 96, 104, 108, 112, 134, 138, 142, 146, 152, 166, 170, 174, 178, 184, 192]\n",
      "groupSN [2, 6, 10, 18, 22, 26, 32, 48, 52, 62, 84, 88, 96, 104, 108, 112, 134, 138, 142, 146, 152, 166, 170, 174, 178, 184, 192, 1, 7, 11, 19, 23, 27, 31, 49, 53, 61, 83, 87, 97, 105, 109, 113, 133, 139, 143, 151, 165, 169, 173, 179, 185, 193, 147]\n",
      "average_recalled_t_Group1= 24.333333333333332\n",
      "average_correct_recalled_t_Group1= 22.703703703703702\n",
      "average_intrusion_recalled_t_Group1= 1.6296296296296295\n",
      "average_recalled_t_Group2= 30.85185185185185\n",
      "average_correct_recalled_t_Group2= 29.25925925925926\n",
      "average_intrusion_recalled_t_Group2= 1.5925925925925926\n",
      "Mean= Comparison             inf\n",
      "intersection     10.058405\n",
      "overlap           0.463805\n",
      "Jaccard           0.213157\n",
      "SMC               0.647857\n",
      "lcs               1.551282\n",
      "OmEuni            0.635095\n",
      "OmEbi             1.156230\n",
      "OdEuni            5.327999\n",
      "OdEbi             5.018979\n",
      "OdMuni            0.076991\n",
      "OdMbi             0.141332\n",
      "OmEdMuni          0.063344\n",
      "OmEdMbi           0.114038\n",
      "OmEdMmEMuni       0.064324\n",
      "OmEdMmEMbi        0.117509\n",
      "pairedFreq        1.156230\n",
      "ITR2              0.141332\n",
      "ARC2              0.117509\n",
      "editdist          0.067475\n",
      "mod_editdist      0.067669\n",
      "editdist_IDST     0.067669\n",
      "editdist_IDS      0.067475\n",
      "editdist_ID       0.156165\n",
      "editdist_IDT      0.162020\n",
      "dtype: float64\n",
      "COLLABORATIVE POST\n",
      "normalGroup1\n",
      "SN1 [4, 8, 16, 20, 30, 54, 64, 68, 72, 80, 86, 94, 98, 102, 110, 140, 144, 154, 158, 182, 188, 190]\n",
      "normalGroup2\n",
      "SN2 [5, 9, 17, 21, 29, 55, 63, 67, 71, 81, 85, 95, 99, 103, 111, 141, 145, 153, 157, 181, 187, 191]\n",
      "groupSN [5, 9, 17, 21, 29, 55, 63, 67, 71, 81, 85, 95, 99, 103, 111, 141, 145, 153, 157, 181, 187, 191, 4, 8, 16, 20, 30, 54, 64, 68, 72, 80, 86, 94, 98, 102, 110, 140, 144, 154, 158, 182, 188, 190]\n",
      "average_recalled_t_Group1= 24.045454545454547\n",
      "average_correct_recalled_t_Group1= 23.09090909090909\n",
      "average_intrusion_recalled_t_Group1= 0.9545454545454546\n",
      "average_recalled_t_Group2= 32.18181818181818\n",
      "average_correct_recalled_t_Group2= 30.90909090909091\n",
      "average_intrusion_recalled_t_Group2= 1.2727272727272727\n",
      "Mean= Comparison       5.055046e+217\n",
      "intersection      1.586364e+01\n",
      "overlap           6.847645e-01\n",
      "Jaccard           3.962385e-01\n",
      "SMC               7.520661e-01\n",
      "lcs               2.681818e+00\n",
      "OmEuni            2.225290e+00\n",
      "OmEbi             3.359671e+00\n",
      "OdEuni            6.120381e+00\n",
      "OdEbi             5.547017e+00\n",
      "OdMuni            1.462400e-01\n",
      "OdMbi             2.424788e-01\n",
      "OmEdMuni          1.249377e-01\n",
      "OmEdMbi           1.998742e-01\n",
      "OmEdMmEMuni       1.289521e-01\n",
      "OmEdMmEMbi        2.111098e-01\n",
      "pairedFreq        3.359671e+00\n",
      "ITR2              2.424788e-01\n",
      "ARC2              2.111098e-01\n",
      "editdist          1.440093e-01\n",
      "mod_editdist      1.440093e-01\n",
      "editdist_IDST     1.440093e-01\n",
      "editdist_IDS      1.440093e-01\n",
      "editdist_ID       2.531376e-01\n",
      "editdist_IDT      2.641500e-01\n",
      "dtype: float64\n",
      "normalGroup1\n",
      "SN1 [1, 7, 11, 19, 23, 27, 31, 49, 53, 61, 83, 87, 97, 105, 109, 113, 133, 139, 143, 151, 165, 169, 173, 179, 185, 193, 147]\n",
      "normalGroup2\n",
      "SN2 [2, 6, 10, 18, 22, 26, 32, 48, 52, 62, 84, 88, 96, 104, 108, 112, 134, 138, 142, 146, 152, 166, 170, 174, 178, 184, 192]\n",
      "groupSN [2, 6, 10, 18, 22, 26, 32, 48, 52, 62, 84, 88, 96, 104, 108, 112, 134, 138, 142, 146, 152, 166, 170, 174, 178, 184, 192, 1, 7, 11, 19, 23, 27, 31, 49, 53, 61, 83, 87, 97, 105, 109, 113, 133, 139, 143, 151, 165, 169, 173, 179, 185, 193, 147]\n",
      "average_recalled_t_Group1= 24.333333333333332\n",
      "average_correct_recalled_t_Group1= 22.703703703703702\n",
      "average_intrusion_recalled_t_Group1= 1.6296296296296295\n",
      "average_recalled_t_Group2= 30.85185185185185\n",
      "average_correct_recalled_t_Group2= 29.25925925925926\n",
      "average_intrusion_recalled_t_Group2= 1.5925925925925926\n",
      "Mean= Comparison       4.081889e+267\n",
      "intersection      1.540741e+01\n",
      "overlap           6.307105e-01\n",
      "Jaccard           3.706632e-01\n",
      "SMC               7.668350e-01\n",
      "lcs               1.851852e+00\n",
      "OmEuni            1.382667e+00\n",
      "OmEbi             2.802371e+00\n",
      "OdEuni            4.909998e+00\n",
      "OdEbi             5.586092e+00\n",
      "OdMuni            9.044607e-02\n",
      "OdMbi             2.260177e-01\n",
      "OmEdMuni          6.866584e-02\n",
      "OmEdMbi           1.824572e-01\n",
      "OmEdMmEMuni       6.990234e-02\n",
      "OmEdMmEMbi        1.908280e-01\n",
      "pairedFreq        2.802371e+00\n",
      "ITR2              2.260177e-01\n",
      "ARC2              1.908280e-01\n",
      "editdist          9.633767e-02\n",
      "mod_editdist      9.749508e-02\n",
      "editdist_IDST     9.749508e-02\n",
      "editdist_IDS      9.633767e-02\n",
      "editdist_ID       2.043594e-01\n",
      "editdist_IDT      2.207138e-01\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Typical comparisons\n",
    "# similarity(df, collab1, bias1, order1, phase1, collab2, bias2, order2, phase2, TrueCollab=True, Self=True)\n",
    "# Reminder: Experiment 3 had half of the lists longer than the other half. Order in experiment 3 means which lists was longer. \n",
    "# Reminder cntd.: Always one biased and one unbiased person collaborated. Participants always collaborated with a person with the same order (two consecutive numbers).\n",
    "# Nominal Post\n",
    "print('NOMINAL POST')\n",
    "similarity(df, 0,0,0,2,0,1,0,2,False,False) #unbiased,order1 & biased,order1\n",
    "similarity(df, 0,0,1,2,0,1,1,2,False,False) #unbiased,order2 & biased,order2\n",
    "# Nominal collaborative Post\n",
    "print('NOMINAL COLLABORATIVE POST')\n",
    "similarity(df, 1,1,1,2,1,0,1,2,False,False) #biased,order2 & unbiased,order2\n",
    "similarity(df, 1,1,0,2,1,0,0,2,False,False) #biased,order1 & unbiased,order1\n",
    "# Collaborative Post\n",
    "print('COLLABORATIVE POST')\n",
    "similarity(df, 1,1,1,2,1,0,1,2,True,False) #biased,order2 & unbiased,order2\n",
    "similarity(df, 1,1,0,2,1,0,0,2,True,False) #biased,order1 & unbiased,order1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "000= 20\n",
    "001= 27\n",
    "010= 26\n",
    "011= 21\n",
    "111= 26\n",
    "101= 22\n",
    "100= 27\n",
    "110= 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B5. Group means (Results presented in Manuscript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022-07-28_Similarity_Exp2_AllWords_Clean_000200_010200_Results.csv', '2022-07-28_Similarity_Exp2_AllWords_Clean_001200_011200_Results.csv', '2022-07-28_Similarity_Exp2_AllWords_Clean_110200_100200_Results.csv', '2022-07-28_Similarity_Exp2_AllWords_Clean_110210_100210_Results.csv', '2022-07-28_Similarity_Exp2_AllWords_Clean_111200_101200_Results.csv', '2022-07-28_Similarity_Exp2_AllWords_Clean_111210_101210_Results.csv']\n"
     ]
    }
   ],
   "source": [
    "# Merge only the relevant results from the three groups in one big file\n",
    "#relevant_files = glob.glob(\"2020-12-11_Similarity_Exp2_AllWords_Clean_*_Results.csv\") \n",
    "relevant_files = glob.glob(\"2022-07-28_Similarity_Exp2_AllWords_Clean_*_Results.csv\") \n",
    "print(relevant_files)\n",
    "results = pd.concat((pd.read_csv(f) for f in relevant_files),sort=False)\n",
    "#results.to_csv('2020-12-11_Similarity_Exp2_AllWords_Clean-RelevantComparisons.csv', index=False)\n",
    "results.to_csv('2022-07-28_Similarity_Exp2_AllWords_Clean-RelevantComparisons.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEANS        intersection  overlap  Jaccard   SMC   lcs  OmEuni  OmEbi  OdEuni  \\\n",
      "Group                                                                      \n",
      "1              6.24     0.34     0.15  0.66  1.29    0.37   0.70    4.79   \n",
      "2             10.19     0.46     0.22  0.64  1.55    0.63   1.17    5.48   \n",
      "3             15.61     0.65     0.38  0.76  2.22    1.76   3.05    5.45   \n",
      "\n",
      "       OdEbi  OdMuni  ...  OmEdMmEMbi  pairedFreq  ITR2  ARC2  editdist  \\\n",
      "Group                 ...                                                 \n",
      "1       4.81    0.06  ...        0.11        0.70  0.12  0.11      0.05   \n",
      "2       5.15    0.08  ...        0.12        1.17  0.14  0.12      0.07   \n",
      "3       5.57    0.12  ...        0.20        3.05  0.23  0.20      0.12   \n",
      "\n",
      "       mod_editdist  editdist_IDST  editdist_IDS  editdist_ID  editdist_IDT  \n",
      "Group                                                                        \n",
      "1              0.05           0.05          0.05         0.13          0.13  \n",
      "2              0.07           0.07          0.07         0.16          0.16  \n",
      "3              0.12           0.12          0.12         0.23          0.24  \n",
      "\n",
      "[3 rows x 24 columns]\n",
      "SD        intersection  overlap  Jaccard   SMC   lcs  OmEuni  OmEbi  OdEuni  \\\n",
      "Group                                                                      \n",
      "1              4.82     0.18     0.09  0.09  1.01    1.07   1.29   11.53   \n",
      "2              5.21     0.15     0.09  0.07  0.64    0.98   1.36    7.96   \n",
      "3              7.82     0.15     0.15  0.08  2.54    2.91   3.08    5.30   \n",
      "\n",
      "       OdEbi  OdMuni  ...  OmEdMmEMbi  pairedFreq  ITR2  ARC2  editdist  \\\n",
      "Group                 ...                                                 \n",
      "1       7.97    0.14  ...        0.19        1.29  0.18  0.19      0.05   \n",
      "2       5.17    0.11  ...        0.14        1.36  0.13  0.14      0.04   \n",
      "3       3.32    0.16  ...        0.17        3.08  0.16  0.17      0.14   \n",
      "\n",
      "       mod_editdist  editdist_IDST  editdist_IDS  editdist_ID  editdist_IDT  \n",
      "Group                                                                        \n",
      "1              0.05           0.05          0.05         0.07          0.07  \n",
      "2              0.04           0.04          0.04         0.05          0.05  \n",
      "3              0.14           0.14          0.14         0.13          0.12  \n",
      "\n",
      "[3 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Update relevant group file to enable averaging across the two orders\n",
    "#AComp = pd.read_csv('2020-12-11_Similarity_Exp2_AllWords_Clean-RelevantComparisons.csv')\n",
    "AComp = pd.read_csv('2022-07-28_Similarity_Exp2_AllWords_Clean-RelevantComparisons.csv')\n",
    "#print(AComp)\n",
    "\n",
    "# str(collab1) + str(bias1) + str(order1) + str(phase1) + str(int(TrueCollab))\n",
    "# participants always have different bias, one 0 and the other 1\n",
    "# participants always have the same order\n",
    "AComp.loc[AComp['Comparison'] == '00020_01020', 'Group'] = '1' #Nominal_Post\n",
    "AComp.loc[AComp['Comparison'] == '00120_01120', 'Group'] = '1' #Nominal_Post\n",
    "AComp.loc[AComp['Comparison'] == '11020_10020', 'Group'] = '2' #NominalCollab_Post\n",
    "AComp.loc[AComp['Comparison'] == '11120_10120', 'Group'] = '2' #NominalCollab_Post\n",
    "AComp.loc[AComp['Comparison'] == '11021_10021', 'Group'] = '3' #Collab_Post\n",
    "AComp.loc[AComp['Comparison'] == '11121_10121', 'Group'] = '3' #Collab_Post\n",
    "\n",
    "#AComp.to_csv('2021-04-25_Similarity_Exp2_AllWords_Clean-RelevantComparisons2.csv', index=False)\n",
    "AComp.to_csv('2022-07-28_Similarity_Exp2_AllWords_Clean-RelevantComparisons2.csv', index=False)\n",
    "#print(AComp)\n",
    "GroupMeans = round(AComp.groupby('Group').mean(),2)\n",
    "print('MEANS', GroupMeans)\n",
    "GroupSD = round(AComp.groupby('Group').std(),2)\n",
    "print('SD', GroupSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __B6. Correlation Matrix (Results presented in Manuscript)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpos level high 4\n",
      "pos level high 0\n",
      "xpos level 0 0.11764705882352941\n",
      "rpos level high 10\n",
      "pos level high 4\n",
      "xpos level 0 0.5294117647058824\n",
      "rpos level high 3\n",
      "pos level high 14\n",
      "xpos level 0 0.9117647058823529\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# and add labels to bottom\n",
    "# July 28 2022\n",
    "# CHECK THAT THE LABELS ON THE LEFT ARE IN THE RIGHT ORDER! (I think so because of SMT but still)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import groupby\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "def test_table():\n",
    "    months = [datetime.date(2008, i+1, 1).strftime('%B') for i in range(12)]\n",
    "    seasons = ['Winter',]*3 + ['Spring',]*2 + ['Summer']*3 + ['Pre-Winter',]*4\n",
    "    tuples = list(zip(months, seasons))\n",
    "    index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])\n",
    "    d = {i: [np.random.randint(0,50) for _ in range(12)] for i in range(1950, 1960)}\n",
    "    #print('d test data', d)\n",
    "    #print('index', index)\n",
    "    df = pd.DataFrame(d, index=index)\n",
    "    #print('test df', df)\n",
    "    return df\n",
    "\n",
    "def add_line(ax, xpos, ypos, level):\n",
    "    #print('ypos', ypos)       \n",
    "    if level == 0:\n",
    "        line = plt.Line2D([ypos, ypos+ 1.2], [xpos, xpos], color='black', transform=ax.transAxes, linewidth=.25)\n",
    "    elif level ==1:\n",
    "        line = plt.Line2D([ypos+.05, ypos+ 1.35], [xpos, xpos], color='black', transform=ax.transAxes)\n",
    "    line.set_clip_on(False)\n",
    "    ax.add_line(line)\n",
    "    \n",
    "def add_line_specialLast(ax, xpos, ypos, level):\n",
    "    #print('ypos', ypos)       \n",
    "    if level == 0:\n",
    "        line = plt.Line2D([ypos-.085, ypos+ 1.2-.085], [xpos, xpos], color='black', transform=ax.transAxes, linewidth=.25)\n",
    "    elif level ==1:\n",
    "        line = plt.Line2D([ypos+.05, ypos+ 1.35], [xpos, xpos], color='black', transform=ax.transAxes)\n",
    "    line.set_clip_on(False)\n",
    "    ax.add_line(line)\n",
    "\n",
    "def add_line_x(ax, xpos, ypos, level):\n",
    "    #print('xpos', xpos)\n",
    "    #print('ypos', ypos)       \n",
    "    if level == 0:\n",
    "        line = plt.Line2D([xpos, xpos], [ypos, ypos], color='black', transform=ax.transAxes, linewidth=.25)\n",
    "    elif level ==1:\n",
    "        line = plt.Line2D([xpos, xpos], [ypos, ypos], color='black', transform=ax.transAxes)\n",
    "    line.set_clip_on(False)\n",
    "    ax.add_line(line)\n",
    "    \n",
    "def label_len(my_index,level):\n",
    "    labels = my_index.get_level_values(level)\n",
    "    return [(k, sum(1 for i in g)) for k,g in groupby(labels)]\n",
    "\n",
    "# This is for y-axis\n",
    "def label_group_bar_table(ax, df):\n",
    "    scale = 1./df.index.size \n",
    "    #print('scale', scale)\n",
    "    for level in range(df.index.nlevels):\n",
    "        pos = df.index.size #index counts how many labels there are per group\n",
    "        #print('pos', pos)\n",
    "        for label, rpos in label_len(df.index,level):\n",
    "            #print('label', label)\n",
    "            #print('rpos', rpos)\n",
    "\n",
    "            # this prints the text\n",
    "            if label in ['Intersection', 'Overlap', 'Jaccard', 'SMC', 'O-E (uni)', 'PF: O-E (bi)', 'O/E (uni)', 'O/E (bi)', 'O/M (uni)', 'O/M (bi)', '(O-E)/M (uni)', '(O-E)/M (bi)', '(O-E)/(M-E) (uni)', '(O-E)/(M-E) (bi)', 'ED (IDST)', 'ED (IDS)', 'ED (ID)', 'ED (IDT)']:\n",
    "                #xpos = -.2#\n",
    "                xpos = -.115\n",
    "                #add_line(ax, pos*scale, xpos, level)\n",
    "                add_line(ax, pos*scale, xpos-0.085, level)\n",
    "                pos -= rpos\n",
    "                lypos = (pos + .4 * rpos)*scale    \n",
    "                #lypos = (pos + .5 * rpos)*scale    \n",
    "                ax.text(xpos+.1, lypos, label, ha='right', transform=ax.transAxes, fontsize=11) #rotation='vertical' here rotates all labels, incl. sub labels \n",
    "                    \n",
    "            else:\n",
    "                # This adjusts the position of the category labels on the left (x-axis)\n",
    "                xpos = -.35 \n",
    "                add_line(ax, pos*scale, xpos, level)\n",
    "                # Adding something here seems to adjust the spacing/start location of the labels\n",
    "                pos -= rpos\n",
    "                # number changes position/start of category label text\n",
    "                # I would somehow need to get the sum of all the sub-labels of that category, but I didn't and just visually adjusted it\n",
    "                if label == 'ITR':\n",
    "                    lypos = (pos + .5 * rpos)*scale\n",
    "                    #print('lypos', lypos)\n",
    "                    ax.text(xpos+.1, lypos, label, ha='center', transform=ax.transAxes, fontsize=11, rotation='vertical')\n",
    "                elif label == 'Set-theoretic':\n",
    "                    lypos = (pos + .3 * rpos)*scale\n",
    "                    #print('lypos', lypos)\n",
    "                    ax.text(xpos+.1, lypos, label, ha='center', transform=ax.transAxes, fontsize=11, rotation='vertical')   \n",
    "                else:\n",
    "                    lypos = (pos + .2 * rpos)*scale #.2\n",
    "                    #print('lypos', lypos)\n",
    "                    ax.text(xpos+.1, lypos, label, ha='center', transform=ax.transAxes, fontsize=11, rotation='vertical')\n",
    "        \n",
    "        #add_line(ax, pos*scale , xpos, level) #this is the last solid black line\n",
    "        add_line_specialLast(ax, pos*scale , xpos, level)\n",
    "        xpos -= .2\n",
    "        \n",
    "def add_line_x(ax, xpos, ypos, level):\n",
    "    #print('ypos', ypos)       \n",
    "    if level == 0:\n",
    "        line = plt.Line2D([xpos, xpos], [ypos+.05, ypos+ 1.2], color='black', transform=ax.transAxes, linewidth=.25)\n",
    "    elif level ==1:\n",
    "        line = plt.Line2D([xpos, xpos], [ypos+.05, ypos+ 1.275], color='black', transform=ax.transAxes)\n",
    "    line.set_clip_on(False)\n",
    "    ax.add_line(line)\n",
    "\n",
    "def label_len_x(my_index,level):\n",
    "    labels = my_index.get_level_values(level)\n",
    "    return [(k, sum(1 for i in g)) for k,g in groupby(labels)]\n",
    "\n",
    "# This is for x-axis labels\n",
    "def label_group_bar_table_x(ax, df):\n",
    "    pos = 0\n",
    "    scale = 1./df.index.size #.055\n",
    "    #print('scale', scale)\n",
    "    for level in range(df.index.nlevels):\n",
    "        # level gives me 2 levels\n",
    "        #print('level', level)\n",
    "        pos = df.index.size #counts how many labels there are per group\n",
    "        pos = pos -17 #This needs to be adjusted depdendent on measures included\n",
    "        #print('pos1', pos)\n",
    "\n",
    "        for label, rpos in label_len_x(df.index,level):\n",
    "            #print('label', label)\n",
    "            #and then I need 2 different y-values for the two levels\n",
    "            #different x-values for each single label\n",
    "            \n",
    "            #xpos = 0 #(pos + .5 * rpos)*scale\n",
    "            #lypos = -2\n",
    "            \n",
    "            #add_line_x(ax, lypos, xpos, level)\n",
    "\n",
    "            #lypos = (pos + .5 * rpos)*scale\n",
    "            #print('lypos', lypos)\n",
    "            if label in ['Intersection', 'Overlap', 'Jaccard', 'SMC', 'O-E (uni)', 'PF: O-E (bi)', 'O/E (uni)', 'O/E (bi)', 'O/M (uni)', 'O/M (bi)', '(O-E)/M (uni)', '(O-E)/M (bi)', '(O-E)/(M-E) (uni)', '(O-E)/(M-E) (bi)', 'ED (IDST)', 'ED (IDS)', 'ED (ID)', 'ED (IDT)']:\n",
    "                #print('rpos', rpos)\n",
    "                #print('pos2', pos)\n",
    "                ypos = -.01\n",
    "                #xpos = 0.025 * pos + (.025 * rpos) #(pos + .5 * rpos)*scale\n",
    "                xpos = (pos + .5 * rpos)*scale\n",
    "                # rpos * x = 0.025\n",
    "                pos += rpos\n",
    "                #print('xpos level 1', xpos)\n",
    "            \n",
    "            #ax.text(lypos+.1, xpos, label, ha='center', transform=ax.transAxes, rotation='vertical') # This just adds the labels on the left again. the labels on the left have been created before with the other function similar name\n",
    "                ax.text(xpos, ypos, label, ha='center', va='top', transform=ax.transAxes, rotation='vertical') # This just adds the labels on the left again. the labels on the left have been created before with the other function similar name\n",
    "                #add_line_x(ax, xpos-.5*scale , -3.6*scale, level)\n",
    "                add_line_x(ax, xpos-.5*scale , ypos-(3.25*scale), level)\n",
    "            else:\n",
    "                print('rpos level high', rpos)\n",
    "                print('pos level high', pos)\n",
    "                ypos = -.2\n",
    "                #xpos = 0.025 * pos + (.025 * rpos) #(pos + .5 * rpos)*scale\n",
    "                xpos = (pos + .5 * rpos)*scale\n",
    "                # rpos * x = 0.025\n",
    "                pos += rpos\n",
    "                print('xpos level 0', xpos)\n",
    "            \n",
    "            #ax.text(lypos+.1, xpos, label, ha='center', transform=ax.transAxes, rotation='vertical') # This just adds the labels on the left again. the labels on the left have been created before with the other function similar name\n",
    "                ax.text(xpos, ypos, label, ha='center', transform=ax.transAxes, rotation='horizontal') # This just adds the labels on the left again. the labels on the left have been created before with the other function similar name\n",
    "                \n",
    "                x_for_thick_line = pos * scale #so correct\n",
    "                #add_line_x(ax, x_for_thick_line , ypos-.075, level) \n",
    "                add_line_x(ax, x_for_thick_line , ypos-1.275*scale, level) \n",
    "    #add_line_x(ax, 0 , -.275, 1) #manually insert y-axis bottom to add last sold think black line \n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "#corr = pd.read_csv('2021-04-25_Similarity_Exp2_AllWords_Clean-RelevantComparisons2.csv')\n",
    "corr = pd.read_csv('2022-07-28_Similarity_Exp2_AllWords_Clean-RelevantComparisons2.csv')\n",
    "#print('corr', corr)\n",
    "corr = corr.drop('lcs', axis=1)\n",
    "corr = corr.drop('Group', axis=1)\n",
    "corr = corr.drop('editdist', axis=1).drop('mod_editdist', axis=1).drop('pairedFreq', axis=1).drop('ITR2', axis=1).drop('ARC2', axis=1).drop('editdist_IDT', axis=1)\n",
    "corr = corr.rename(columns={'intersection':'Intersection', 'overlap':'Overlap', 'jaccard':'Jaccard', 'OmEuni':'O-E (uni)', 'OmEbi':'PF: O-E (bi)', 'OdEuni':'O/E (uni)', 'OdEbi':\"O/E (bi)\", 'OdMuni':'O/M (uni)', 'OdMbi':'O/M (bi)', 'OmEdMuni':'(O-E)/M (uni)', 'OmEdMbi':'(O-E)/M (bi)', 'OmEdMmEMuni':'(O-E)/(M-E) (uni)', 'OmEdMmEMbi':'(O-E)/(M-E) (bi)', 'editdist_IDST':'ED (IDST)', 'editdist_IDS':'ED (IDS)', 'editdist_ID':'ED (ID)'})\n",
    "corrMatrix = corr.corr()\n",
    "corrMatrix_y = corrMatrix.reset_index()\n",
    "corrMatrix_y = corrMatrix_y.drop(corrMatrix_y.columns[0], axis=1)\n",
    "#print('corrMatrix_y', corrMatrix_y)\n",
    "\n",
    "\n",
    "# I think this creates an index with two categories for each tuple\n",
    "idx = pd.MultiIndex.from_tuples([('Intersection','Set-theoretic'), ('Overlap','Set-theoretic'), ('Jaccard','Set-theoretic'), ('SMC','Set-theoretic'), ('O-E (uni)','ITR'), ('PF: O-E (bi)','ITR'), ('O/E (uni)','ITR'), (\"O/E (bi)\",'ITR'), ('O/M (uni)','ITR'), ('O/M (bi)','ITR'), ('(O-E)/M (uni)','ITR'), ('(O-E)/M (bi)','ITR'), ('(O-E)/(M-E) (uni)','ITR'), ('(O-E)/(M-E) (bi)','ITR'), ('ED (IDST)','Edit distance'), ('ED (IDS)','Edit distance'), ('ED (ID)','Edit distance')], names=['first', 'second'])  \n",
    "#print('idx', idx)\n",
    "\n",
    "# and this mask is a separate plot with the white space at on the other side; so that corrs are only shown 1x\n",
    "mask = np.triu(np.ones_like(corrMatrix_y, dtype=bool))\n",
    "#print('mask', mask)\n",
    "\n",
    "# forgot what this is but I assume the list of measures?\n",
    "df_corr2 = corrMatrix_y.to_dict('list')\n",
    "#print('df_corr2 list', df_corr2)\n",
    "df_corr3 = pd.DataFrame(df_corr2, index=idx)\n",
    "#print('df_corr2 DF (3)', df_corr3)\n",
    "\n",
    "#df = test_table()\n",
    "#print('df', df)\n",
    "\n",
    "# Finally create the figure\n",
    "fig = plt.figure(figsize = (12, 12))\n",
    "ax = fig.add_subplot(111)\n",
    "#ax2 = fig.add_subplot(111)\n",
    "\n",
    "sns.heatmap(df_corr3, mask=mask, cmap=\"RdYlBu\", vmin=-1, vmax=1) #\n",
    "ax.set_facecolor(\"white\") #to make the top right triangle a white mask\n",
    "\n",
    "#Below 3 lines remove default labels\n",
    "labels = ['' for item in ax.get_yticklabels()]\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_ylabel('')\n",
    "\n",
    "# This is responsible for horizontal lines and labels on the left\n",
    "label_group_bar_table(ax, df_corr3)\n",
    "\n",
    "# I think this adjusts the row height and also vertical line length \n",
    "fig.subplots_adjust(bottom=.1*df_corr3.index.nlevels)\n",
    "\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "# Delete the labels on the x axis to manually add them to also show category label\n",
    "labels = ['' for item in ax.get_xticklabels()]\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('')\n",
    "\n",
    "# This is responsible for vertical lines\n",
    "label_group_bar_table_x(ax, df_corr3)\n",
    "\n",
    "#fig.subplots_adjust(bottom=.1*df_corr3.index.nlevels)\n",
    "#plt.tight_layout()\n",
    "plt.savefig('Similarity_CorrMatrix_20220728', dpi='figure', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
